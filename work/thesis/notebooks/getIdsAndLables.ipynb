{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "This notebook is to train the encoder as a classifier with the idea of validate the encoder architecture first and then use this to train the VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on guanaco\n",
    "# ATENTION: if it is going to run on guanaco:\n",
    "# 1) comment the %matplotlib magic in next block and any magic (something like %code)\n",
    "# 2) Change to True the trainingOnGuanaco vairbale\n",
    "# 3) set epoch with an appropiate number\n",
    "# 4) add comment to experiemnts\n",
    "# 5) Add this file as python file \n",
    "# 6) Change launchJobOnGuanaco file to run this file but with python format\n",
    "trainingOnGuanaco = False\n",
    "\n",
    "# train without notebook\n",
    "trainWithJustPython = False\n",
    "\n",
    "# number_experiment (this is just a name)\n",
    "# priors:\n",
    "# 1\n",
    "number_experiment = 99\n",
    "number_experiment = str(number_experiment)\n",
    "\n",
    "# seed to generate same datasets\n",
    "seed = 0\n",
    "\n",
    "# training\n",
    "epochs = 100000\n",
    "\n",
    "# max elements by class\n",
    "max_elements_per_class = 15000\n",
    "\n",
    "# train with previous model\n",
    "trainWithPreviousModel = False\n",
    "\n",
    "# include delta errors\n",
    "includeDeltaErrors = False\n",
    "\n",
    "# band\n",
    "# passband = [5]\n",
    "passband = [0, 1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda device\n",
    "cuda_device = 0\n",
    "cuda_device = \"cuda:\" + str(cuda_device)\n",
    "\n",
    "# classes to analyze\n",
    "# 42,  90,  16,  67,  62, 993,  92,  52,  88,  65, 991, 992,  15,\n",
    "#        95,   6,  53, 994,  64\n",
    "\n",
    "# periodic\n",
    "# only_these_labels = [16, 92, 53]\n",
    "\n",
    "# periodic + variable\n",
    "only_these_labels = [16, 92, 53, 88, 65, 6]\n",
    "# 53 has 24 light curves\n",
    "\n",
    "# only_these_labels = [16, 92]\n",
    "# only_these_labels = [16, 92]\n",
    "# only_these_labels = [42,  90,  16,  67,  62, 993,  92,  52,  88,  65, 991, 992,  15,\n",
    "#         95,   6,  53, 994,  64]\n",
    "\n",
    "# VAE parameters\n",
    "latentDim = 100\n",
    "hiddenDim = 100\n",
    "inputDim = 72\n",
    "\n",
    "batch_training_size = 128\n",
    "\n",
    "# early stopping \n",
    "threshold_early_stop = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp 99 + encoder as clasifier with periodic + variable + class balancing + 1 conv layer more + 6 channels + seed 0 + without delta errors + max by class 15000\n"
     ]
    }
   ],
   "source": [
    "# add general comment about experiment \n",
    "# comment = \"encoder as clasifier with periodic + variable (with class balancing) + 1 conv layer more\"\n",
    "comment = \"exp \" + number_experiment + \" + encoder as clasifier with periodic + variable + class balancing + 1 conv layer more + \" + str(len(passband)) + \" channels + seed \" + str(seed) + \" + \" + (\"include delta errors\" if includeDeltaErrors else \"without delta errors\") + \" + max by class \" + str(max_elements_per_class)\n",
    "\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# from tqdm import tqdm_notebook\n",
    "\n",
    "# %matplotlib notebook\n",
    "\n",
    "# import functions to load dataset\n",
    "import sys\n",
    "sys.path.append(\"./codesToDatasets\")\n",
    "from plasticc_dataset_torch import get_plasticc_datasets\n",
    "# from plasticc_plotting import plot_light_curve\n",
    "\n",
    "import math\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "# local imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append('../models')\n",
    "# from classifier import EncoderClassifier, \n",
    "from classifierPrototype import EncoderClassifier\n",
    "\n",
    "sys.path.append(\"./aux/\")\n",
    "from auxFunctions import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the path to save model while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# create experiment's folder\n",
    "tmpGuanaco = \"/home/lbravo/thesis/thesis/work/thesis/\"\n",
    "tmpLocal = \"/home/leo/Desktop/thesis/work/thesis/\"\n",
    "\n",
    "expPath = \"experiments/\" + number_experiment + \"/seed\" + str(seed) + \"/maxClass\" + str(int(max_elements_per_class/1000)) + \"k\"\n",
    "\n",
    "folder_path = (tmpGuanaco + expPath) if trainingOnGuanaco else (tmpLocal + expPath)\n",
    "# !mkdir folder_path\n",
    "# os.makedirs(os.path.dirname(folder_path), exist_ok=True)\n",
    "\n",
    "# check if folder exists\n",
    "if not(os.path.isdir(folder_path)):\n",
    "        \n",
    "    # create folder\n",
    "    try:\n",
    "        os.makedirs(folder_path)\n",
    "        \n",
    "    except OSError as error:\n",
    "        print (\"Creation of the directory %s failed\" % folder_path)\n",
    "        print(error)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % folder_path)\n",
    "else:\n",
    "    print(\"folder already exists\")\n",
    "\n",
    "# define paht to save model while training\n",
    "pathToSaveModel = (tmpGuanaco + expPath + \"/model\") if trainingOnGuanaco else (tmpLocal + expPath + \"/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to dataset\n",
    "pathToFile = \"/home/shared/astro/PLAsTiCC/\" if trainingOnGuanaco else \"/home/leo/Downloads/plasticData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset with pytorch tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have selected lazy loading. Light curves will be loaded ondemand from the harddrive\n",
      "Found 2 csv files at given path\n",
      "Loading /home/leo/Downloads/plasticData/plasticc_train_lightcurves.csv\n",
      "Loading /home/leo/Downloads/plasticData/plasticc_test_set_batch1.csv\n"
     ]
    }
   ],
   "source": [
    "# torch_dataset_lazy = get_plasticc_datasets(pathToFile)\n",
    "\n",
    "# Light curves are tensors are now [bands, [mjd, flux, err, mask],\n",
    "# lc_data, lc_label, lc_plasticc_id                              \n",
    "torch_dataset_lazy = get_plasticc_datasets(pathToFile, only_these_labels=only_these_labels, max_elements_per_class = max_elements_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset test ok\n"
     ]
    }
   ],
   "source": [
    "assert torch_dataset_lazy.__len__() != 494096, \"dataset should be smaller\"\n",
    "print(\"dataset test ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting data (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# light curves ids: 3276\n"
     ]
    }
   ],
   "source": [
    "# splitting the data\n",
    "\n",
    "# get light curves ids, targets\n",
    "ids, targets, lightCurvesIds = getLightCurvesIds(torch_dataset_lazy)\n",
    "\n",
    "# test array shapes\n",
    "# assert len(targets) == torch_dataset_lazy.__len__()\n",
    "# print(ids, len(ids), targets, len(targets))\n",
    "# get light curves targets\n",
    "print(\"# light curves ids: \" + str(len(ids)))\n",
    "\n",
    "# split training\n",
    "trainIdx, tmpIdx = train_test_split(\n",
    "    ids,\n",
    "    test_size = 0.2,\n",
    "    shuffle = True,\n",
    "    stratify = targets,\n",
    "    random_state = seed\n",
    ")\n",
    "\n",
    "# float to int\n",
    "tmpIdx = tmpIdx.astype(int)\n",
    "\n",
    "# split val, test\n",
    "valIdx, testIdx = train_test_split(\n",
    "    tmpIdx,\n",
    "#     targets,\n",
    "    test_size = 0.5,\n",
    "    shuffle = True,\n",
    "    stratify = targets[tmpIdx],\n",
    "    random_state = seed\n",
    ")\n",
    "\n",
    "# float to int\n",
    "trainIdx = trainIdx.astype(int)\n",
    "valIdx = valIdx.astype(int)\n",
    "testIdx = testIdx.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "assert targets.shape == lightCurvesIds.shape \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "light curves ids saved on a file\n"
     ]
    }
   ],
   "source": [
    "# saving ids\n",
    "saveLightCurvesIdsBeforeBalancing(trainIdx, valIdx, testIdx, folder_path, lightCurvesIds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load ids dictionary\n",
    "# a_file = open(folder_path + \"/dataset_ids_before_balancing.pkl\", \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 16.,  93.,   0.,   0.,   0.,   4., 104.,   0.,   0., 111.]),\n",
       " array([ 6. , 14.6, 23.2, 31.8, 40.4, 49. , 57.6, 66.2, 74.8, 83.4, 92. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # analize classes distributino\n",
    "fig, ax = plt.subplots(3, 1)\n",
    "\n",
    "ax[0].hist(targets[trainIdx])\n",
    "ax[1].hist(targets[valIdx])\n",
    "ax[2].hist(targets[testIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 2620\n",
      "validation size:  328\n",
      "test size: 328\n",
      "sum:  3276\n"
     ]
    }
   ],
   "source": [
    "# # Spliting the data\n",
    "\n",
    "# # print(torch_dataset_lazy.__len__())\n",
    "\n",
    "totalSize = torch_dataset_lazy.__len__()\n",
    "\n",
    "# totalSize = totalSize\n",
    "# # print(totalSize)\n",
    "\n",
    "# selecting train splitting\n",
    "# train_size = int(0.8 * totalSize)\n",
    "train_size = trainIdx.shape[0]\n",
    "#print(train_size)\n",
    "\n",
    "# # getting test splitting\n",
    "# validation_size = math.floor((totalSize - train_size)/3)\n",
    "validation_size = valIdx.shape[0]\n",
    "# #print(validation_size)\n",
    "\n",
    "# # getting test splitting\n",
    "# test_size = totalSize - train_size - validation_size\n",
    "test_size = testIdx.shape[0]\n",
    "# #print(test_size)\n",
    "\n",
    "# # spliting the torch dataset\n",
    "# trainDataset, validationDataset,  testDataset = torch.utils.data.random_split(\n",
    "#     torch_dataset_lazy, \n",
    "#     [train_size, validation_size, test_size],\n",
    "    \n",
    "#     # set seed\n",
    "#     generator = torch.Generator().manual_seed(seed)\n",
    "# )\n",
    "\n",
    "print(\"train size:\", train_size)\n",
    "print(\"validation size: \", validation_size)\n",
    "print(\"test size:\", test_size)\n",
    "totTmp = train_size+ validation_size + test_size\n",
    "print(\"sum: \", totTmp)\n",
    "assert torch_dataset_lazy.__len__() == totTmp, \"dataset partition should be the same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initila distribution\n",
      "[ 157  927   36 1042  733  381]\n"
     ]
    }
   ],
   "source": [
    "print(\"initila distribution\")\n",
    "# initialClassesDistribution = countClasses(trainDataset, only_these_labels)\n",
    "initialClassesDistribution = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "print(initialClassesDistribution)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.bar(x = np.arange(len(only_these_labels)), height = initialClassesDistribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data loader (minibatches)\n",
    "\n",
    "# training loader\n",
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    torch_dataset_lazy, \n",
    "    batch_size = batch_training_size, \n",
    "    # to balance classes\n",
    "    sampler=ImbalancedDatasetSampler(\n",
    "        torch_dataset_lazy, \n",
    "        indices = trainIdx,\n",
    "        seed = seed\n",
    "#         indices = [0, 1, 2]\n",
    "    ),\n",
    "    # each worker retrieve data from disk, so the data will be ready to be processed by main process. The main process should get the data from disk, so if workers > 0, the workers will get the data (not the main process)\n",
    "    num_workers = 4,\n",
    "    \n",
    "    # https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/\n",
    "    # the dataloader loads the data in pinned memory (instead of pageable memory), avoiding one process (to transfer data from pageable memory to pinned memory, work done by CUDA driver)\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "\n",
    "# validation loader\n",
    "validationLoader = torch.utils.data.DataLoader(\n",
    "#     validationDataset, \n",
    "    torch_dataset_lazy,\n",
    "    batch_size= batch_training_size,  \n",
    "    num_workers = 4,\n",
    "    pin_memory = True,\n",
    "    sampler = torch.utils.data.SubsetRandomSampler(\n",
    "        valIdx,\n",
    "        generator = torch.Generator().manual_seed(seed)\n",
    "    ),\n",
    ")\n",
    "\n",
    "# # test loader\n",
    "# testLoader = torch.utils.data.DataLoader(testDataset)\n",
    "testLoader = torch.utils.data.DataLoader(\n",
    "#     validationDataset, \n",
    "    torch_dataset_lazy,\n",
    "#     batch_size= batch_training_size,  \n",
    "    num_workers = 4,\n",
    "    pin_memory = True,\n",
    "    sampler = torch.utils.data.SubsetRandomSampler(\n",
    "        testIdx,\n",
    "        generator = torch.Generator().manual_seed(seed)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced distribution\n",
      "[444. 420. 439. 474. 400. 443.]\n"
     ]
    }
   ],
   "source": [
    "print(\"balanced distribution\")\n",
    "balancedClassesDistribution = countClasses(trainLoader, only_these_labels)\n",
    "\n",
    "print(balancedClassesDistribution)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.bar(x = np.ar# return 0# return 0ange(6), height = balancedClassesDistribution)\n",
    "# ax.bar(x = only_these_labels, height = temp2, width = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "light curves ids saved on a file\n"
     ]
    }
   ],
   "source": [
    "# save ids of dataset to use (train, test and validation)\n",
    "saveLightCurvesIdsAfterBalancing(trainLoader, train_size, testLoader, test_size, validationLoader, validation_size, path = folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# load ids dictionary\n",
    "# a_file = open(folder_path + \"/dataset_ids_after_balancing.pkl\", \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# # print(output[\"labels_train\"].shape)\n",
    "# # print(output[\"train\"].shape)\n",
    "\n",
    "# assert output[\"labels_test\"].shape == output[\"test\"].shape\n",
    "# print(\"ok\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
