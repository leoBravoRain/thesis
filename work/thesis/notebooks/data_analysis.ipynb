{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "This notebook is to train the encoder as a classifier with the idea of validate the encoder architecture first and then use this to train the VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on guanaco\n",
    "# ATENTION: if it is going to run on guanaco:\n",
    "# 1) comment the %matplotlib magic in next block and any magic (something like %code)\n",
    "# 2) Change to True the trainingOnGuanaco vairbale\n",
    "# 3) set epoch with an appropiate number\n",
    "# 4) add comment to experiemnts\n",
    "# 5) Add this file as python file \n",
    "# 6) Change launchJobOnGuanaco file to run this file but with python format\n",
    "trainingOnGuanaco = False\n",
    "# trainingOnGuanaco = True\n",
    "\n",
    "# train without notebook\n",
    "trainWithJustPython = False\n",
    "\n",
    "# number_experiment (this is just a name)\n",
    "# priors:\n",
    "# 1\n",
    "number_experiment = 27\n",
    "number_experiment = str(number_experiment)\n",
    "\n",
    "# seed to generate same datasets\n",
    "seed = 0\n",
    "\n",
    "# training\n",
    "epochs = 100000\n",
    "\n",
    "# max elements by class\n",
    "max_elements_per_class = 15000\n",
    "\n",
    "# train with previous model\n",
    "trainWithPreviousModel = False\n",
    "\n",
    "# include delta errors\n",
    "includeDeltaErrors = True\n",
    "\n",
    "# band\n",
    "passband = [0]\n",
    "# passband = [1]\n",
    "# passband = [2]\n",
    "# passband = [3]\n",
    "# passband = [4]\n",
    "# passband = [5]\n",
    "# passband = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "\n",
    "# include ohter feautures\n",
    "includeOtherFeatures = False\n",
    "\n",
    "# num of features to add\n",
    "# ṕvar by channel\n",
    "otherFeaturesDim = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda device\n",
    "cuda_device = 0\n",
    "cuda_device = \"cuda:\" + str(cuda_device)\n",
    "\n",
    "# classes to analyze\n",
    "# 42,  90,  16,  67,  62, 993,  92,  52,  88,  65, 991, 992,  15,\n",
    "#        95,   6,  53, 994,  64\n",
    "\n",
    "# periodic\n",
    "# only_these_labels = [16, 92, 53]\n",
    "\n",
    "# periodic + variable\n",
    "only_these_labels = [16, 92, 53, 88, 65, 6]\n",
    "# 53 has 24 light curves\n",
    "\n",
    "# only_these_labels = [16, 92]\n",
    "# only_these_labels = [16, 92]\n",
    "# only_these_labels = [42,  90,  16,  67,  62, 993,  92,  52,  88,  65, 991, 992,  15,\n",
    "#         95,   6,  53, 994,  64]\n",
    "\n",
    "# VAE parameters\n",
    "latentDim = 100\n",
    "hiddenDim = 100\n",
    "inputDim = 72\n",
    "\n",
    "batch_training_size = 128\n",
    "\n",
    "# early stopping \n",
    "threshold_early_stop = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp 27 + encoder as clasifier with periodic + variable + class balancing + 1 conv layer more + 1 channels + seed 0 + include delta errors + max by class 15000 + not other features\n"
     ]
    }
   ],
   "source": [
    "# add general comment about experiment \n",
    "# comment = \"encoder as clasifier with periodic + variable (with class balancing) + 1 conv layer more\"\n",
    "comment = \"exp \" + number_experiment + \" + encoder as clasifier with periodic + variable + class balancing + 1 conv layer more + \" + str(len(passband)) + \" channels + seed \" + str(seed) + \" + \" + (\"include delta errors\" if includeDeltaErrors else \"without delta errors\") + \" + max by class \" + str(max_elements_per_class) + \" + \" + (\"\" if includeOtherFeatures else \"not\") + \" other features\"\n",
    "\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# from tqdm import tqdm_notebook\n",
    "\n",
    "if not trainingOnGuanaco:\n",
    "    \n",
    "    %matplotlib notebook\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "else:\n",
    "    print(\"not load magics\")\n",
    "    \n",
    "# import functions to load dataset\n",
    "import sys\n",
    "sys.path.append(\"./codesToDatasets\")\n",
    "from plasticc_dataset_torch import get_plasticc_datasets\n",
    "from plasticc_plotting import plot_light_curve\n",
    "\n",
    "import math\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "# local imports\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "sys.path.append('../models')\n",
    "# from classifier import EncoderClassifier, \n",
    "from classifierPrototype import EncoderClassifier\n",
    "\n",
    "sys.path.append(\"./aux/\")\n",
    "from auxFunctions import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the path to save model while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create experiment's folder\n",
    "tmpGuanaco = \"/home/lbravo/thesis/thesis/work/thesis/\"\n",
    "tmpLocal = \"/home/leo/Desktop/thesis/work/thesis/\"\n",
    "\n",
    "expPath = \"experiments/\" + number_experiment + \"/seed\" + str(seed) + \"/maxClass\" + str(int(max_elements_per_class/1000)) + \"k\"\n",
    "\n",
    "folder_path = (tmpGuanaco + expPath) if trainingOnGuanaco else (tmpLocal + expPath)\n",
    "# !mkdir folder_path\n",
    "# os.makedirs(os.path.dirname(folder_path), exist_ok=True)\n",
    "\n",
    "# check if folder exists\n",
    "# if not(os.path.isdir(folder_path)):\n",
    "        \n",
    "#     # create folder\n",
    "#     try:\n",
    "#         os.makedirs(folder_path)\n",
    "        \n",
    "#     except OSError as error:\n",
    "#         print (\"Creation of the directory %s failed\" % folder_path)\n",
    "#         print(error)\n",
    "#     else:\n",
    "#         print (\"Successfully created the directory %s \" % folder_path)\n",
    "# else:\n",
    "#     print(\"folder already exists\")\n",
    "\n",
    "# # define paht to save model while training\n",
    "# pathToSaveModel = (tmpGuanaco + expPath + \"/model\") if trainingOnGuanaco else (tmpLocal + expPath + \"/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to dataset\n",
    "pathToFile = \"/home/shared/astro/PLAsTiCC/\" if trainingOnGuanaco else \"/home/leo/Downloads/plasticData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset with pytorch tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have selected lazy loading. Light curves will be loaded ondemand from the harddrive\n",
      "Found 2 csv files at given path\n",
      "Loading /home/leo/Downloads/plasticData/plasticc_train_lightcurves.csv\n",
      "Loading /home/leo/Downloads/plasticData/plasticc_test_set_batch1.csv\n"
     ]
    }
   ],
   "source": [
    "# torch_dataset_lazy = get_plasticc_datasets(pathToFile)\n",
    "\n",
    "# Light curves are tensors are now [bands, [mjd, flux, err, mask],\n",
    "# lc_data, lc_label, lc_plasticc_id                              \n",
    "torch_dataset_lazy = get_plasticc_datasets(pathToFile, only_these_labels=only_these_labels, max_elements_per_class = max_elements_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset test ok\n"
     ]
    }
   ],
   "source": [
    "assert torch_dataset_lazy.__len__() != 494096, \"dataset should be smaller\"\n",
    "print(\"dataset test ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lc_data, label, lc_id = torch_dataset_lazy.__getitem__(1300)\n",
    "\n",
    "# # 11 was good\n",
    "\n",
    "# fig, ax = plt.subplots(figsize= (8,6), tight_layout=True)\n",
    "# for band, band_name in enumerate('ugrizY'):\n",
    "#     mask = lc_data[band, 3, :] == 1\n",
    "#     mjd, flux, flux_err = lc_data[band, :3, mask]\n",
    "#     ax.errorbar(mjd, flux, flux_err, fmt='.', label=band_name)\n",
    "# ax.legend()\n",
    "# ax.set_ylabel('Flux', fontsize = 20)\n",
    "# ax.set_xlabel('Modified Julian Data', fontsize = 20)\n",
    "# ax.set_title(f'PLAsTiCC ID: {lc_id} Label: {label}', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig(\"lightCurve.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataInformation(dataset):\n",
    "    \n",
    "    totalSize = dataset.__len__()\n",
    "    \n",
    "    print(\"total light curves: \", totalSize)\n",
    "    \n",
    "    # samples by filter\n",
    "    samplesByFilter = np.zeros(shape = (totalSize, 6))\n",
    "    \n",
    "    # time length mean\n",
    "    timeLength = np.zeros(shape = (totalSize, 6))\n",
    "    \n",
    "    # mean error\n",
    "    errorMeanByFilter = np.zeros(shape = (totalSize, 6))\n",
    "    \n",
    "    # flux \n",
    "    fluxMeanByFilter = np.zeros(shape = (totalSize, 6))\n",
    "    fluxStdByFilter = np.zeros(shape = (totalSize, 6))\n",
    "    \n",
    "    # delta time mean\n",
    "    deltaTimeMean = np.zeros(shape= (totalSize, 6))\n",
    "    \n",
    "    targets = np.zeros(shape = (totalSize, 6))\n",
    "    \n",
    "    # to filter after\n",
    "    arrays = [samplesByFilter, timeLength, errorMeanByFilter, fluxMeanByFilter, fluxStdByFilter, deltaTimeMean, targets]\n",
    "    \n",
    "    for idx, data_ in enumerate(dataset):\n",
    "        \n",
    "        data = data_[0]\n",
    "        \n",
    "        # get samples by filter\n",
    "        samplesByFilter[idx, :] = torch.count_nonzero(data[:, 3, :], dim = 1)\n",
    "        \n",
    "        # get targets\n",
    "        targets[idx, :] = data_[1]\n",
    "        \n",
    "        # iterating by each filter\n",
    "        for i in range(6):\n",
    "            \n",
    "#             data = data_[0]\n",
    "                \n",
    "            \n",
    "            # taking last valid index\n",
    "            # mask= [1,1,1,1,1, 0,0,0,0]\n",
    "            # so it is taking the last non zero -1 (last item index)\n",
    "            lastIndex = samplesByFilter[idx, i].astype(int)-1\n",
    "            \n",
    "            # test\n",
    "#             assert data[0][i, 3, lastIndex] == 1 and data[0][i, 3, lastIndex+1] == 1\n",
    "            \n",
    "            if data[i, 0, 0:lastIndex].shape[0] <= 1:\n",
    "                \n",
    "                timeLength[idx, i] = float(\"NaN\")\n",
    "                errorMeanByFilter[idx, i]= float(\"NaN\")\n",
    "                fluxMeanByFilter[idx, i]= float(\"NaN\")\n",
    "                fluxStdByFilter[idx, i]= float(\"NaN\")\n",
    "                deltaTimeMean[idx, i] = float(\"NaN\")\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # diff between last and first time\n",
    "                timeLength[idx, i] = data[i, 0, lastIndex]  - data[i, 0, 0]\n",
    "\n",
    "                # mean error\n",
    "    #             print(torch.mean(data[0][i, 2, 0:lastIndex], dim = 0).shape)\n",
    "    #             errorMeanByFilter[idx, i] = torch.mean(data[i, 2, 0:lastIndex], dim = 0)\n",
    "                errorMeanByFilter[idx, i] = torch.median(data[i, 2, 0:lastIndex])\n",
    "\n",
    "                # flux mean\n",
    "    #             fluxMeanByFilter[idx, i] = torch.mean(data[i, 1, 0:lastIndex], dim = 0)\n",
    "                fluxMeanByFilter[idx, i] = torch.median(data[i, 1, 0:lastIndex])\n",
    "    #             unbiased = True give nan for channle 0\n",
    "    #             fluxStdByFilter[idx, i] = torch.std(data[i, 1, 0:lastIndex], dim = 0)\n",
    "                fluxStdByFilter[idx, i] = torch.quantile(data[i, 1, 0:lastIndex], 0.75, dim = 0) - torch.quantile(data[i, 1, 0:lastIndex], 0.25, dim = 0)\n",
    "\n",
    "                # delta time\n",
    "                originalTime = data[i, 0, 0:lastIndex]\n",
    "    #             print(originalTime.shape)\n",
    "#                 if originalTime.shape[0] <= 1 :\n",
    "#     #                 print(\"shape short\")\n",
    "#                     deltaTimeMean[idx, i] = float(\"NaN\")\n",
    "    #                 print(deltaTimeMean[idx, i])\n",
    "    #                 print(np.isnan(deltaTimeMean[idx, i]))\n",
    "#                 else:\n",
    "\n",
    "        #             deltaTimeMean[idx, i] = torch.mean(originalTime[1:] - originalTime[:-1], dim = 0)\n",
    "                deltaTimeMean[idx, i] = torch.median(originalTime[1:] - originalTime[:-1])\n",
    "#             print(torch.median(originalTime[1:] - originalTime[:-1]))\n",
    "#             print(torch.mean(originalTime[1:] - originalTime[:-1], dim = 0))\n",
    "\n",
    "    # remove nanas\n",
    "    mask = ~np.isnan(deltaTimeMean).any(axis = 1)\n",
    "#     print(np.any(mask))\n",
    "    for idx, array in enumerate(arrays):\n",
    "            \n",
    "        arrays[idx] = array[mask, :]\n",
    "        \n",
    "    print(arrays[idx].shape)\n",
    "    \n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total light curves:  3276\n",
      "(3275, 6)\n"
     ]
    }
   ],
   "source": [
    "samplesByFilter, timeLength, errorMeanByFilter, fluxMeanByFilter, fluxStdByFilter, deltaTimeMean, targets = getDataInformation(torch_dataset_lazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_ = np.unique(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStats(arrays, targets):\n",
    "    \n",
    "    arrayNames = [\"samples\", \"time length\", \"error mean by filter\", \"flux mean by filter\", \"flux Std By Filter\", \"delta Time Mean\"]\n",
    "    \n",
    "    targets_ = np.unique(targets)\n",
    "    \n",
    "    statsMeans = np.zeros(shape = (len(arrayNames), targets_.shape[0], 6) )\n",
    "    \n",
    "    statsStd = np.zeros(shape = (len(arrayNames), targets_.shape[0], 6) )\n",
    "    \n",
    "    \n",
    "    for idx, array in enumerate(arrays):\n",
    "            \n",
    "        print(arrayNames[idx] + \"\\n\")\n",
    "        \n",
    "        for idxClass, class_ in enumerate(targets_):\n",
    "\n",
    "            print(class_)\n",
    "            \n",
    "    #         print(targets.shape)\n",
    "            maskClass = (targets[:, 0] == class_)\n",
    "            \n",
    "#             print(array[maskClass,:].shape[0])\n",
    "            print(arrayNames[idx])\n",
    "            print(np.mean(array[maskClass,:], 0))\n",
    "            print(np.std(array[maskClass,:], 0))\n",
    "            print(\"\\n\\n\")\n",
    "            \n",
    "            \n",
    "            statsMeans[idx, idxClass, :] = np.median(array[maskClass,:], 0)\n",
    "            statsStd[idx, idxClass, :] = np.percentile(array[maskClass,:], 0.75) - np.percentile(array[maskClass,:], 0.25)\n",
    "            \n",
    "#             dict_[arrayNames[idx]][targets[0, 0]][\"mean\"] =  np.mean(array[maskClass,:], 0)\n",
    "#             tmpDict[targets_] = np.mean(array[maskClass,:], 0)\n",
    "#             print(np.mean(array[maskClass,:], 0))\n",
    "#             dict_[arrayNames[idx]][targets_]=  np.mean(array[maskClass,:], 0)\n",
    "\n",
    "#         print(tmpDict)\n",
    "    \n",
    "    return statsMeans, statsStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples\n",
      "\n",
      "6.0\n",
      "samples\n",
      "[15.8089172  14.43312102 24.12738854 22.99363057 32.         35.50318471]\n",
      "[14.14940018 12.4600119   9.87419342 10.23948696  8.36431608 10.34908175]\n",
      "\n",
      "\n",
      "\n",
      "16.0\n",
      "samples\n",
      "[19.6429342  18.24271845 26.71305286 25.99568501 33.1639698  36.46494067]\n",
      "[18.55359917 16.12707053 12.35704325 12.85459227 10.21954663 10.89097736]\n",
      "\n",
      "\n",
      "\n",
      "53.0\n",
      "samples\n",
      "[23.38888889 22.22222222 29.         28.47222222 35.11111111 39.        ]\n",
      "[21.65932924 18.11707468 14.18723996 14.4865572  10.68950102 10.24966124]\n",
      "\n",
      "\n",
      "\n",
      "65.0\n",
      "samples\n",
      "[28.7840691  26.11516315 33.04318618 32.40978887 37.45201536 39.86948177]\n",
      "[24.38062727 20.40309196 15.46654667 15.96276919 12.4560687  11.75078098]\n",
      "\n",
      "\n",
      "\n",
      "88.0\n",
      "samples\n",
      "[43.71311475 38.85655738 42.6420765  42.46994536 44.71584699 45.76502732]\n",
      "[26.32981631 20.58077312 15.53421326 15.79990743 12.69422313 11.36200075]\n",
      "\n",
      "\n",
      "\n",
      "92.0\n",
      "samples\n",
      "[36.0839895  33.14173228 38.17585302 37.68766404 41.73490814 43.80839895]\n",
      "[26.17633081 21.61390789 16.51208242 17.03307205 13.10789325 12.15462639]\n",
      "\n",
      "\n",
      "\n",
      "time length\n",
      "\n",
      "6.0\n",
      "time length\n",
      "[924.71317675 843.25569765 815.08250398 931.25490147 968.85972333\n",
      " 973.17702528]\n",
      "[163.75988975 111.40687448  95.36244925 113.22280878 105.56050063\n",
      " 100.92798599]\n",
      "\n",
      "\n",
      "\n",
      "16.0\n",
      "time length\n",
      "[895.65531452 843.65355313 805.1663127  913.67010518 951.46821905\n",
      " 955.34273025]\n",
      "[157.2215561  116.28450199  90.03248087 108.34587236 106.65966626\n",
      " 104.44095913]\n",
      "\n",
      "\n",
      "\n",
      "53.0\n",
      "time length\n",
      "[854.55349392 864.26475694 818.76768663 937.75108507 959.69259983\n",
      " 956.05772569]\n",
      "[179.29276213 125.00683064  65.47804634 115.28634322 105.59289007\n",
      " 101.13457039]\n",
      "\n",
      "\n",
      "\n",
      "65.0\n",
      "time length\n",
      "[868.06044941 843.52606166 823.27519194 891.54816834 923.87069263\n",
      " 926.62459888]\n",
      "[125.44304212  89.35557528  88.96872504  90.06643512  99.00015138\n",
      "  97.94451625]\n",
      "\n",
      "\n",
      "\n",
      "88.0\n",
      "time length\n",
      "[832.98541026 843.40541752 836.88764195 865.2201428  881.01052873\n",
      " 883.70044612]\n",
      "[92.77341603 64.28083811 62.53722421 64.18221813 69.44679663 70.72925592]\n",
      "\n",
      "\n",
      "\n",
      "92.0\n",
      "time length\n",
      "[870.55440043 847.86911499 835.74086491 896.05716864 915.40152354\n",
      " 915.16288345]\n",
      "[130.90067057  88.91405632  64.48568532  93.20378425  97.20526004\n",
      "  93.42128782]\n",
      "\n",
      "\n",
      "\n",
      "error mean by filter\n",
      "\n",
      "6.0\n",
      "error mean by filter\n",
      "[252.3703696   30.22645431  27.68230122  29.14230892  37.27060354\n",
      "  59.36603644]\n",
      "[1489.53714536   84.77125187   72.15945894   69.55956504   71.35697741\n",
      "   80.31102336]\n",
      "\n",
      "\n",
      "\n",
      "16.0\n",
      "error mean by filter\n",
      "[278.16844395  27.66878417  18.75394084  16.73235118  19.84352458\n",
      "  33.43770697]\n",
      "[5173.91311516  154.65886564   46.21061618   28.7293942    19.46533896\n",
      "   24.28063682]\n",
      "\n",
      "\n",
      "\n",
      "53.0\n",
      "error mean by filter\n",
      "[ 39.81433392  30.34320475  33.88194888 176.12756195 270.55663384\n",
      " 334.98827129]\n",
      "[113.90575335 106.87339443 100.68711333 613.09477192 806.54735591\n",
      " 899.0996957 ]\n",
      "\n",
      "\n",
      "\n",
      "65.0\n",
      "error mean by filter\n",
      "[11.89225439  2.93340537  3.69463675  6.43890762 12.21166568 26.25198307]\n",
      "[46.54217322  4.62830575  2.91942871  4.28800459  7.86371957 16.40163436]\n",
      "\n",
      "\n",
      "\n",
      "88.0\n",
      "error mean by filter\n",
      "[12.11987738  3.10298914  2.92769113  3.99257772  6.69828625 14.26179387]\n",
      "[71.3093357  11.35626154  6.83069811  6.65135326  7.47568003 13.90212949]\n",
      "\n",
      "\n",
      "\n",
      "92.0\n",
      "error mean by filter\n",
      "[2897.50934457  133.56214953   48.52473253   32.49632683   28.40373107\n",
      "   39.23750501]\n",
      "[5.53781239e+04 1.08338454e+03 1.26891660e+02 6.32062034e+01\n",
      " 4.46625041e+01 5.24887981e+01]\n",
      "\n",
      "\n",
      "\n",
      "flux mean by filter\n",
      "\n",
      "6.0\n",
      "flux mean by filter\n",
      "[ -16.24322501   36.58205755   88.93482216  -46.05439018 -171.26442625\n",
      " -235.19864206]\n",
      "[ 366.03205287 1369.63423616 3683.0101163  4445.94045571 5732.79885233\n",
      " 7189.90213676]\n",
      "\n",
      "\n",
      "\n",
      "16.0\n",
      "flux mean by filter\n",
      "[31.66237159 58.93160517 87.93509905 67.67581427 41.82563058 16.84661168]\n",
      "[772.21321263 445.8208655  505.73267503 363.6621066  243.43751171\n",
      "  96.36063525]\n",
      "\n",
      "\n",
      "\n",
      "53.0\n",
      "flux mean by filter\n",
      "[   13.35972334  -306.71454671  -564.10740343 -3710.02848159\n",
      " -6214.18094512 -7658.56103473]\n",
      "[   78.4242494    852.31454699  1472.10428462  9332.84847456\n",
      " 16881.55224049 22394.22094792]\n",
      "\n",
      "\n",
      "\n",
      "65.0\n",
      "flux mean by filter\n",
      "[ -7.78144562 -10.04048665 -10.28403668  -9.48043129  -8.49958175\n",
      "  -8.23714815]\n",
      "[41.02491698 50.22796869 52.11039462 48.23747411 43.97696281 40.78046175]\n",
      "\n",
      "\n",
      "\n",
      "88.0\n",
      "flux mean by filter\n",
      "[ 9.88917498  7.72973898  8.27412309  8.38365101  9.38845451 15.56499015]\n",
      "[179.46361231 157.14503252 149.18379951 152.36694849 166.55317775\n",
      " 236.15768908]\n",
      "\n",
      "\n",
      "\n",
      "92.0\n",
      "flux mean by filter\n",
      "[ -41.28982011 -947.8322811  -728.32663757 -303.74518873 -222.83688774\n",
      " -286.27496602]\n",
      "[1628.34554883 2444.26723437 1700.75888722 1235.35861124 1107.68419749\n",
      " 1076.63308965]\n",
      "\n",
      "\n",
      "\n",
      "flux Std By Filter\n",
      "\n",
      "6.0\n",
      "flux Std By Filter\n",
      "[ 417.45068189  978.99921771 2353.01138248 2577.09597652 2375.42144886\n",
      " 3059.77680198]\n",
      "[ 2091.51196528  9571.92499721 24595.50537378 26085.88483464\n",
      " 23020.62887944 29622.8553221 ]\n",
      "\n",
      "\n",
      "\n",
      "16.0\n",
      "flux Std By Filter\n",
      "[279.96924811 199.43496535 235.63199159 184.47012836 134.42742456\n",
      "  90.13232588]\n",
      "[4204.44875053  708.46140262  703.2976315   565.55033883  388.55253402\n",
      "  192.76059873]\n",
      "\n",
      "\n",
      "\n",
      "53.0\n",
      "flux Std By Filter\n",
      "[   46.17523124   628.29529148   892.29167011  5881.23520999\n",
      " 12189.17492252 16622.94223701]\n",
      "[  123.69905472  1167.43840361  1222.96270403  7220.48294524\n",
      " 10995.69448391 14829.70562488]\n",
      "\n",
      "\n",
      "\n",
      "65.0\n",
      "flux Std By Filter\n",
      "[14.81899968  4.37642178  5.35030095  9.09782687 16.28773221 33.2825527 ]\n",
      "[60.45993677  7.80145903  4.73652947  6.94904883 11.23710898 21.60793421]\n",
      "\n",
      "\n",
      "\n",
      "88.0\n",
      "flux Std By Filter\n",
      "[51.62923264 48.76056068 48.19885879 50.30997997 58.91335259 71.540983  ]\n",
      "[146.82429497 130.00948084 119.91127496 126.95078301 156.19513988\n",
      " 235.05990709]\n",
      "\n",
      "\n",
      "\n",
      "92.0\n",
      "flux Std By Filter\n",
      "[3497.66221719 4499.97220249 3804.13777629 3193.68726785 2787.35661578\n",
      " 2763.72054221]\n",
      "[55219.24537474  3618.16503837  2548.91825438  2133.54815561\n",
      "  1857.76692983  1851.6356376 ]\n",
      "\n",
      "\n",
      "\n",
      "delta Time Mean\n",
      "\n",
      "6.0\n",
      "delta Time Mean\n",
      "[44.49201334 46.91784435 17.55961385 19.38990346 16.4738754  16.42428842]\n",
      "[30.36622856 29.29545222  6.2198625   8.28529888  6.42092394  7.87936524]\n",
      "\n",
      "\n",
      "\n",
      "16.0\n",
      "delta Time Mean\n",
      "[37.20173864 40.85974582 16.36973267 18.97003944 16.1149078  15.86267445]\n",
      "[27.09823241 27.19889854  7.49829851  9.18472779  7.4991939   8.44653925]\n",
      "\n",
      "\n",
      "\n",
      "53.0\n",
      "delta Time Mean\n",
      "[32.12087674 40.95952691 15.17860243 19.11903212 15.05750868 12.76421441]\n",
      "[27.22632609 31.18463218  7.4449151  10.23081326  7.73143476  7.29790999]\n",
      "\n",
      "\n",
      "\n",
      "65.0\n",
      "delta Time Mean\n",
      "[25.58110155 30.29220774 13.44998351 14.74395318 13.71931607 14.0528206 ]\n",
      "[22.817272   25.96783224  7.6451841   8.92308565  8.1921339   8.75930948]\n",
      "\n",
      "\n",
      "\n",
      "88.0\n",
      "delta Time Mean\n",
      "[13.7021431  19.19794869  9.57715911 10.03923327  9.55094134  9.97415578]\n",
      "[22.44229153 24.93599956  7.16606659  8.11320527  7.20071745  7.82327803]\n",
      "\n",
      "\n",
      "\n",
      "92.0\n",
      "delta Time Mean\n",
      "[23.09931718 25.73275509 11.57942708 12.73234498 11.19702469 10.7537012 ]\n",
      "[31.03763957 26.84491013  8.60182451  9.59304612  7.9421522   8.8613352 ]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statsMeans, statsStd = getStats([samplesByFilter, timeLength, errorMeanByFilter, fluxMeanByFilter, fluxStdByFilter, deltaTimeMean], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getErrorComplicated(err, flux, targets):\n",
    "    \n",
    "#     print(np.median(err, 0))\n",
    "# #     print(np.std(flux, 0))\n",
    "    \n",
    "#     stats = np.zeros(shape = (2, 6))\n",
    "    \n",
    "#     stats[0, :] = np.median(err, 0)/np.std(flux, 0)\n",
    "# #     \n",
    "# #     print(\"error normalizado promedio\")\n",
    "# #     print(np.median(err, 0)/np.std(flux, 0))\n",
    "# #     print(\"\\n\")\n",
    "    \n",
    "#     stats[1, :] = np.std(err,0)/np.std(flux,0)\n",
    "    \n",
    "# #     print(\"errores fotométricos normalizados\")\n",
    "# #     print(np.std(err,0)/np.std(flux,0))\n",
    "    \n",
    "#     return stats\n",
    "\n",
    "\n",
    "\n",
    "    targets_ = np.unique(targets)\n",
    "    \n",
    "#     statsMeans = np.zeros(shape = (len(arrayNames), targets_.shape[0], 6) )\n",
    "    stats = np.zeros(shape = (2, 6, 6))\n",
    "    \n",
    "    \n",
    "    for idxClass, class_ in enumerate(targets_):\n",
    "\n",
    "        print(class_)\n",
    "\n",
    "#         print(targets.shape)\n",
    "        maskClass = (targets[:, 0] == class_)\n",
    "\n",
    "#             print(array[maskClass,:].shape[0])\n",
    "#         print(arrayNames[idx])\n",
    "#         print(np.mean(array[maskClass,:], 0))\n",
    "#         print(np.std(array[maskClass,:], 0))\n",
    "#         print(\"\\n\\n\")\n",
    "\n",
    "    \n",
    "#         print(\"error normalizado promedio\")\n",
    "#         print(np.median(err, 0)/np.std(flux, 0))\n",
    "#         print(\"\\n\")\n",
    "\n",
    "        stats[0, idxClass, :] = np.median(err[maskClass, :], 0)/np.std(flux[maskClass, :], 0)\n",
    "        \n",
    "        \n",
    "#         print(\"errores fotométricos normalizados\")\n",
    "#         print(np.std(err,0)/np.std(flux,0))\n",
    "\n",
    "        stats[1, idxClass, :] = np.std(err[maskClass, :],0)/np.std(flux[maskClass, :],0)\n",
    "    \n",
    "\n",
    "#             dict_[arrayNames[idx]][targets[0, 0]][\"mean\"] =  np.mean(array[maskClass,:], 0)\n",
    "#             tmpDict[targets_] = np.mean(array[maskClass,:], 0)\n",
    "#             print(np.mean(array[maskClass,:], 0))\n",
    "#             dict_[arrayNames[idx]][targets_]=  np.mean(array[maskClass,:], 0)\n",
    "\n",
    "#         print(tmpDict)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "16.0\n",
      "53.0\n",
      "65.0\n",
      "88.0\n",
      "92.0\n"
     ]
    }
   ],
   "source": [
    "errorStats = getErrorComplicated(errorMeanByFilter, fluxMeanByFilter, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errorStats[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"stats\": {\"means\": statsMeans, \"std\": statsStd}, \"errorStats\": errorStats}\n",
    "\n",
    "# # save stats\n",
    "a_file = open(\"./datasetStats.pkl\", \"wb\")\n",
    "pickle.dump(results, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Exit from code, because we are in cluster or running locally. Training has finished.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Exit from code, because we are in cluster or running locally. Training has finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "sys.exit(\"Exit from code, because we are in cluster or running locally. Training has finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load ids dictionary\n",
    "a_file = open(\"./datasetStats.pkl\", \"rb\")\n",
    "output = pickle.load(a_file)\n",
    "print(output[\"errorStats\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "targets_ = [6, 16, 53, 65, 88, 92]\n",
    "\n",
    "arrayNames = [\"samples\", \"time length\", \"error mean by filter\", \"flux mean by filter\", \"flux Std By Filter\", \"delta Time Mean\"]\n",
    "\n",
    "fig, ax = plt.subplots(len(arrayNames), 6, tight_layout = True, figsize = (15,10))\n",
    "\n",
    "for i in np.arange(len(arrayNames)):\n",
    "    \n",
    "    for c in np.arange(6):\n",
    "        \n",
    "        ax[i, c].scatter(np.arange(0, 6), output[\"stats\"][\"means\"][i, c, :])\n",
    "        ax[i, c].errorbar(np.arange(0, 6), output[\"stats\"][\"means\"][i, c, :], yerr=output[\"stats\"][\"std\"][i, c, :], fmt='o')\n",
    "        ax[i, c].set_title(arrayNames[i] + \" class \" + str(targets_[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file as pdf\n",
    "fig.savefig(\"./dataSetAnalysis/statsByClassAndFilterWithErrorBar.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arrayNames = [\"er norm\", \"er fot norm\"]\n",
    "\n",
    "fig, ax = plt.subplots(len(arrayNames), 6, tight_layout = True, figsize = (15,4))\n",
    "\n",
    "for i in np.arange(len(arrayNames)):\n",
    "    \n",
    "    for c in np.arange(6):\n",
    "        \n",
    "        ax[i, c].scatter(np.arange(0, 6), output[\"errorStats\"][i, c, :])\n",
    "#         ax[i, c].errorbar(np.arange(0, 6), output[\"errorStats\"][\"means\"][i, c], yerr=output[\"errorStats\"][\"std\"][i, c, :], fmt='o')\n",
    "        ax[i, c].set_title(arrayNames[i] + \" class \" + str(targets_[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as figure\n",
    "fig.savefig(\"./dataSetAnalysis/errorStatsByClassAndFilterWithErrorBar.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "\n",
    "# get light curves ids, targets\n",
    "# ids, targets, lightCurvesIds = getLightCurvesIds(torch_dataset_lazy)\n",
    "\n",
    "\n",
    "# test array shapes\n",
    "# assert len(targets) == torch_dataset_lazy.__len__()\n",
    "# print(ids, len(ids), targets, len(targets))\n",
    "# get light curves targets\n",
    "# print(\"# light curves ids: \" + str(len(ids)))\n",
    "\n",
    "# # split training\n",
    "# trainIdx, tmpIdx = train_test_split(\n",
    "#     ids,\n",
    "#     test_size = 0.2,\n",
    "#     shuffle = True,\n",
    "#     stratify = targets,\n",
    "#     random_state = seed\n",
    "# )\n",
    "\n",
    "# # float to int\n",
    "# tmpIdx = tmpIdx.astype(int)\n",
    "\n",
    "# # split val, test\n",
    "# valIdx, testIdx = train_test_split(\n",
    "#     tmpIdx,\n",
    "# #     targets,\n",
    "#     test_size = 0.5,\n",
    "#     shuffle = True,\n",
    "#     stratify = targets[tmpIdx],\n",
    "#     random_state = seed\n",
    "# )\n",
    "\n",
    "# # float to int\n",
    "# trainIdx = trainIdx.astype(int)\n",
    "# valIdx = valIdx.astype(int)\n",
    "# testIdx = testIdx.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving ids\n",
    "# saveLightCurvesIdsBeforeBalancing(trainIdx, valIdx, testIdx, folder_path, lightCurvesIds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load ids dictionary\n",
    "# a_file = open(folder_path + \"/dataset_ids_before_balancing.pkl\", \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # analize classes distributino\n",
    "# fig, ax = plt.subplots(3, 1)\n",
    "\n",
    "# ax[0].hist(targets[trainIdx])\n",
    "# ax[1].hist(targets[valIdx])\n",
    "# ax[2].hist(targets[testIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spliting the data\n",
    "\n",
    "# # print(torch_dataset_lazy.__len__())\n",
    "\n",
    "totalSize = torch_dataset_lazy.__len__()\n",
    "\n",
    "# totalSize = totalSize\n",
    "# # print(totalSize)\n",
    "\n",
    "# selecting train splitting\n",
    "# train_size = int(0.8 * totalSize)\n",
    "train_size = trainIdx.shape[0]\n",
    "#print(train_size)\n",
    "\n",
    "# # getting test splitting\n",
    "# validation_size = math.floor((totalSize - train_size)/3)\n",
    "validation_size = valIdx.shape[0]\n",
    "# #print(validation_size)\n",
    "\n",
    "# # getting test splitting\n",
    "# test_size = totalSize - train_size - validation_size\n",
    "test_size = testIdx.shape[0]\n",
    "# #print(test_size)\n",
    "\n",
    "# # spliting the torch dataset\n",
    "# trainDataset, validationDataset,  testDataset = torch.utils.data.random_split(\n",
    "#     torch_dataset_lazy, \n",
    "#     [train_size, validation_size, test_size],\n",
    "    \n",
    "#     # set seed\n",
    "#     generator = torch.Generator().manual_seed(seed)\n",
    "# )\n",
    "\n",
    "print(\"train size:\", train_size)\n",
    "print(\"validation size: \", validation_size)\n",
    "print(\"test size:\", test_size)\n",
    "totTmp = train_size+ validation_size + test_size\n",
    "print(\"sum: \", totTmp)\n",
    "assert torch_dataset_lazy.__len__() == totTmp, \"dataset partition should be the same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"initila distribution\")\n",
    "# initialClassesDistribution = countClasses(trainDataset, only_these_labels)\n",
    "initialClassesDistribution = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "print(initialClassesDistribution)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.bar(x = np.arange(len(only_these_labels)), height = initialClassesDistribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data loader (minibatches)\n",
    "\n",
    "# training loader\n",
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    torch_dataset_lazy, \n",
    "    batch_size = batch_training_size, \n",
    "    # to balance classes\n",
    "    sampler=ImbalancedDatasetSampler(\n",
    "        torch_dataset_lazy, \n",
    "        indices = trainIdx,\n",
    "        seed = seed\n",
    "#         indices = [0, 1, 2]\n",
    "    ),\n",
    "#     sampler = torch.utils.data.SubsetRandomSampler(\n",
    "#         trainIdx,\n",
    "#         generator = torch.Generator().manual_seed(seed)\n",
    "#     ),\n",
    "    # each worker retrieve data from disk, so the data will be ready to be processed by main process. The main process should get the data from disk, so if workers > 0, the workers will get the data (not the main process)\n",
    "    num_workers = 4,\n",
    "    \n",
    "    # https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/\n",
    "    # the dataloader loads the data in pinned memory (instead of pageable memory), avoiding one process (to transfer data from pageable memory to pinned memory, work done by CUDA driver)\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "\n",
    "# validation loader\n",
    "validationLoader = torch.utils.data.DataLoader(\n",
    "#     validationDataset, \n",
    "    torch_dataset_lazy,\n",
    "    batch_size= batch_training_size,  \n",
    "    num_workers = 4,\n",
    "    pin_memory = True,\n",
    "    sampler = valIdx,\n",
    "#     sampler = torch.utils.data.SubsetRandomSampler(\n",
    "#         valIdx,\n",
    "#         generator = torch.Generator().manual_seed(seed)\n",
    "#     ),\n",
    "#     sampler=ImbalancedDatasetSampler(\n",
    "#         torch_dataset_lazy, \n",
    "#         indices = valIdx,\n",
    "#         seed = seed\n",
    "# #         indices = [0, 1, 2]\n",
    "#     ),\n",
    ")\n",
    "\n",
    "# # test loader\n",
    "# testLoader = torch.utils.data.DataLoader(testDataset)\n",
    "testLoader = torch.utils.data.DataLoader(\n",
    "#     validationDataset, \n",
    "    torch_dataset_lazy,\n",
    "#     batch_size= batch_training_size,  \n",
    "    num_workers = 4,\n",
    "    pin_memory = True,\n",
    "    sampler = testIdx,\n",
    "#     sampler = torch.utils.data.SubsetRandomSampler(\n",
    "#         testIdx,\n",
    "#         generator = torch.Generator().manual_seed(seed)\n",
    "#     ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced distribution\")\n",
    "balancedClassesDistribution = countClasses(trainLoader, only_these_labels)\n",
    "\n",
    "print(balancedClassesDistribution)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.bar(x = np.ar# return 0# return 0ange(6), height = balancedClassesDistribution)\n",
    "# ax.bar(x = only_these_labels, height = temp2, width = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save ids of dataset to use (train, test and validation)\n",
    "saveLightCurvesIdsAfterBalancing(trainLoader, train_size, testLoader, test_size, validationLoader, validation_size, path = folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load ids dictionary\n",
    "# a_file = open(folder_path + \"/dataset_ids_after_balancing.pkl\", \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# print(output[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if includeOtherFeatures:\n",
    "    \n",
    "    # save features\n",
    "    trainOtherFeaturesArray = np.zeros(shape = (train_size, otherFeaturesDim))\n",
    "    validOtherFeaturesArray = np.zeros(shape = (validation_size, otherFeaturesDim))\n",
    "\n",
    "    print(\"starting to get the other features\")\n",
    "\n",
    "    trainLastIndex = 0\n",
    "    validLastIndex = 0\n",
    "    \n",
    "    for trainData_ in trainLoader:\n",
    "        \n",
    "        # get other features by batch\n",
    "        # [batch size, features]\n",
    "        trainOtherFeatures = getOtherFeatures(trainData_[0]).to(device = cuda_device)\n",
    "\n",
    "        # indexation\n",
    "        trainLastIndex_ = trainLastIndex + trainData_[0].shape[0]\n",
    "        \n",
    "        # save features in array indexing them\n",
    "        trainOtherFeaturesArray[trainLastIndex : trainLastIndex_] = trainOtherFeatures.cpu().numpy()\n",
    "            \n",
    "        # update indexs\n",
    "        trainLastIndex = trainLastIndex_\n",
    "    \n",
    "    # test size\n",
    "    assert trainLastIndex == train_size\n",
    "    \n",
    "    for validData_ in validationLoader:\n",
    "        \n",
    "        # get other features by batch\n",
    "        # [batch size, features]\n",
    "        validOtherFeatures = getOtherFeatures(validData_[0]).to(device = cuda_device)\n",
    "\n",
    "        # indexation\n",
    "        validLastIndex_ = validLastIndex + validData_[0].shape[0]\n",
    "        \n",
    "        # save features in array indexing them\n",
    "        validOtherFeaturesArray[validLastIndex : validLastIndex_] = validOtherFeatures.cpu().numpy()\n",
    "            \n",
    "        # update indexs\n",
    "        validLastIndex = validLastIndex_\n",
    "    \n",
    "    # test size\n",
    "    assert validLastIndex == validation_size\n",
    "    \n",
    "    print(\"finish to get other features\")\n",
    "    \n",
    "    print(\"normalize features\")\n",
    "    # normalize features\n",
    "    trainNormalizedFeatures = torch.from_numpy(normalizeOtherFeatures(trainOtherFeaturesArray)).type(torch.FloatTensor)\n",
    "    validNormalizedFeatures = torch.from_numpy(normalizeOtherFeatures(validOtherFeaturesArray)).type(torch.FloatTensor)\n",
    "    \n",
    "    # check nan values\n",
    "    print(f\"nan values train: {np.any(torch.isnan(trainNormalizedFeatures).cpu().numpy())}\")\n",
    "    print(f\"nan values valid: {np.any(torch.isnan(validNormalizedFeatures).cpu().numpy())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test shape\n",
    "# assert (trainNormalizedFeatures.shape == trainOtherFeaturesArray.shape)\n",
    "# assert (validNormalizedFeatures.shape == validOtherFeaturesArray.shape)\n",
    "# print(trainNormalizedFeatures.shape)\n",
    "# print(validNormalizedFeatures.shape)\n",
    "\n",
    "# # test man values on normalized\n",
    "# print(torch.mean(torch.from_numpy(trainOtherFeaturesArray), dim = 0))\n",
    "# print(torch.mean(trainNormalizedFeatures, dim = 0))\n",
    "\n",
    "# print(torch.mean(torch.from_numpy(validOtherFeaturesArray), dim = 0))\n",
    "# print(torch.mean(validNormalizedFeatures, dim = 0))\n",
    "\n",
    "# # test max values on normalized\n",
    "# print(torch.max(torch.from_numpy(trainOtherFeaturesArray), dim = 0))\n",
    "# print(torch.max(trainNormalizedFeatures, dim = 0))\n",
    "\n",
    "# print(torch.max(torch.from_numpy(validOtherFeaturesArray), dim = 0))\n",
    "# print(torch.max(validNormalizedFeatures, dim = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create experiment parameters file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store varibales on file\n",
    "if trainingOnGuanaco or trainWithJustPython:\n",
    "    text_file = open(\"../\" + expPath + \"/experimentParameters.txt\" , \"w\")\n",
    "    text = \"N° experiment: {7}\\n General comment: {13}\\n Classes: {0}\\n train_size: {9}\\n validation_size: {10}\\n test_size: {11}\\n total dataset size: {12}\\n Epochs: {8}\\n Latent dimension: {1}\\n Hidden dimension: {2}\\n Input dimension: {3}\\n Passband: {4}\\n Learning rate: {5}\\n Batch training size: {6}\\n initial train classes distribution: {14}\\nbalanced train class distribution: {15}\".format(only_these_labels, latentDim, hiddenDim, inputDim, passband, learning_rate, batch_training_size, number_experiment, epochs, train_size, validation_size, test_size, train_size + validation_size + test_size, comment, initialClassesDistribution, balancedClassesDistribution)\n",
    "    text_file.write(text)\n",
    "    text_file.close()\n",
    "    print(\"experiment parameters file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining parameters to Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of parameters\n",
    "# latentDim = 5\n",
    "# hiddenDim = 10\n",
    "# inputDim = 72\n",
    "\n",
    "latentDim = latentDim\n",
    "hiddenDim = hiddenDim\n",
    "inputDim = inputDim\n",
    "\n",
    "# passband = passband\n",
    "\n",
    "num_classes = len(only_these_labels)\n",
    "\n",
    "if trainWithPreviousModel:\n",
    "    \n",
    "    # loadgin model\n",
    "    model = torch.load(pathToSaveModel + \".txt\").to(device = cuda_device)\n",
    "    \n",
    "    print(\"loading saved model\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # defining model\n",
    "    model = EncoderClassifier(\n",
    "        latent_dim = latentDim, \n",
    "        hidden_dim = hiddenDim, \n",
    "        input_dim = inputDim, \n",
    "        num_classes = num_classes, \n",
    "        passband = passband, \n",
    "        includeDeltaErrors = includeDeltaErrors,\n",
    "        includeOtherFeatures = includeOtherFeatures,\n",
    "        otherFeaturesDim = otherFeaturesDim,\n",
    "    )\n",
    "\n",
    "    # mdel to GPU\n",
    "    model = model.to(device = cuda_device)\n",
    "    \n",
    "    print(\"creating model with default parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# optimizeraa\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# loss function\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss\n",
    "train_loss = np.zeros((epochs,))\n",
    "test_loss = np.zeros((epochs,))\n",
    "\n",
    "# f1 scores\n",
    "f1Scores = np.zeros((epochs, ))\n",
    "\n",
    "# min global test loss \n",
    "minTestLossGlobalSoFar = float(\"inf\")\n",
    "\n",
    "# # # loss plot\n",
    "# if it is not cluster\n",
    "if (not trainingOnGuanaco) or (not trainWithJustPython):\n",
    "    \n",
    "    # add f1 and loss plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (7, 3), tight_layout = True)\n",
    "    \n",
    "    # error\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[0].set_ylabel(\"Error\")\n",
    "    \n",
    "    \n",
    "    # f1 score\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].set_ylabel(\"F1 score\")\n",
    "    \n",
    "\n",
    "# early stopping\n",
    "count_early_stop = 0\n",
    "\n",
    "\n",
    "print(\"starting the training\")\n",
    "\n",
    "\n",
    "# epoch\n",
    "for nepoch in range(epochs):\n",
    "        \n",
    "    print(\"epoch:    {0} / {1}\".format(nepoch, epochs))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    ######## Train ###########\n",
    "    \n",
    "    \n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    # this is for getting the other features\n",
    "    trainLastIndex = 0\n",
    "    \n",
    "    for data_ in trainLoader:\n",
    "        \n",
    "        # get raw data and labels\n",
    "        labels = data_[1].to(device = cuda_device)\n",
    "        \n",
    "        # optimzer to zero\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # this returns deltas\n",
    "        data = generateDeltas(data_[0], passband, includeDeltaErrors).type(torch.FloatTensor).to(device = cuda_device)\n",
    "            \n",
    "        # add other features\n",
    "        # [batch size, features dim]\n",
    "        if includeOtherFeatures:\n",
    "            \n",
    "            # index to include batch data\n",
    "            trainLastIndex_ = trainLastIndex + data_[0].shape[0]\n",
    "\n",
    "            # get only the normalized data from the batch (by indexation)\n",
    "            otherFeatures = trainNormalizedFeatures[trainLastIndex : trainLastIndex_, :].to(device = cuda_device)\n",
    "            \n",
    "            # update index \n",
    "            trainLastIndex = trainLastIndex_\n",
    "            \n",
    "            # validate data\n",
    "            if np.any(torch.isnan(otherFeatures).cpu().numpy()):\n",
    "                \n",
    "                print(f\"other features with nan values in epoch {nepoch}\")\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors, otherFeatures)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors)\n",
    "\n",
    "        # validate nana values on output model\n",
    "        if np.any(torch.isnan(outputs).cpu().numpy()):\n",
    "                \n",
    "                print(f\"outpues with nan values in epoch {nepoch}\")\n",
    "                \n",
    "\n",
    "        # loss function\n",
    "        loss = lossFunction(outputs, mapLabels(labels, only_these_labels).to(device = cuda_device))\n",
    "            \n",
    "        # validate nan values on loss\n",
    "        if np.any(torch.isnan(loss).cpu().numpy()):\n",
    "                \n",
    "                print(f\"loss with nan values in epoch {nepoch}\")\n",
    "                \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add loss value (of the currrent minibatch)\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "    # get epoch loss value\n",
    "    train_loss[nepoch] = epoch_train_loss / train_size\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### Validation ########\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    epoch_test_loss = 0\n",
    "    \n",
    "    # check f1 score in each minibatch\n",
    "    f1Score = 0\n",
    "    \n",
    "    batchCounter = 0\n",
    "    \n",
    "    # this is to get other features\n",
    "    validLastIndex = 0\n",
    "    \n",
    "    # minibatches\n",
    "    for data_ in validationLoader:\n",
    "        \n",
    "        labels = data_[1].to(device = cuda_device)\n",
    "            \n",
    "        # get deltas\n",
    "        data = generateDeltas(data_[0], passband, includeDeltaErrors).type(torch.FloatTensor).to(device = cuda_device)\n",
    "    \n",
    "        \n",
    "        # inlcude other features\n",
    "        if includeOtherFeatures:\n",
    "            \n",
    "            validLastIndex_ = validLastIndex + data_[0].shape[0]\n",
    "\n",
    "            otherFeatures = validNormalizedFeatures[validLastIndex : validLastIndex_, :].to(device = cuda_device)\n",
    "        \n",
    "            validLastIndex = validLastIndex_\n",
    "            \n",
    "            # validate data\n",
    "            if np.any(torch.isnan(otherFeatures).cpu().numpy()):\n",
    "                \n",
    "                print(f\"other features with nan values in epoch {nepoch}\")\n",
    "                \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors, otherFeatures)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors)\n",
    "        \n",
    "        if np.any(torch.isnan(outputs).cpu().numpy()):\n",
    "                \n",
    "                print(f\"outputs with nan values in epoch {nepoch}\")\n",
    "                \n",
    "        # loss function\n",
    "        loss = lossFunction(outputs, mapLabels(labels, only_these_labels).to(device = cuda_device))\n",
    "        \n",
    "        #  store minibatch loss value\n",
    "        epoch_test_loss += loss.item()\n",
    "\n",
    "        # f1 score\n",
    "        f1Score += f1_score(\n",
    "            mapLabels(labels, only_these_labels).cpu().numpy(), \n",
    "            torch.argmax(outputs, 1).cpu().numpy(), \n",
    "            average = \"weighted\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # batch counter\n",
    "        batchCounter += 1\n",
    "    \n",
    "    \n",
    "    # get epoch test loss value\n",
    "    test_loss[nepoch] = epoch_test_loss / validation_size\n",
    "    \n",
    "    # get epoch f1 score\n",
    "    f1Scores[nepoch] = f1Score / batchCounter\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # plot values\n",
    "    \n",
    "    \n",
    "    # plot loss values\n",
    "    # if it's not cluster\n",
    "    if (not trainingOnGuanaco) or (not trainWithJustPython):\n",
    "\n",
    "        # loss values\n",
    "        ax[0].plot(train_loss[0: nepoch], label = \"train\", linewidth = 3, c = \"red\") \n",
    "        ax[0].plot(test_loss[0: nepoch], label = \"test\", linestyle = \"--\", linewidth = 3, c = \"green\")\n",
    "        \n",
    "        # f1 score values\n",
    "        ax[1].plot(f1Scores[0: nepoch], linewidth = 3, c = \"green\")\n",
    "        \n",
    "        # plot\n",
    "        fig.canvas.draw()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Saving best model ####\n",
    "    \n",
    "    # if epoch test loss is smaller than global min\n",
    "    if test_loss[nepoch] < minTestLossGlobalSoFar:\n",
    "        \n",
    "        # update global min\n",
    "        minTestLossGlobalSoFar = test_loss[nepoch]\n",
    "        \n",
    "        # save model\n",
    "        saveBestModel(model, pathToSaveModel, number_experiment, nepoch, minTestLossGlobalSoFar, expPath)\n",
    "                \n",
    "   \n",
    "\n",
    "\n",
    "    #### save losses ####\n",
    "    print(\"saving losses\")\n",
    "    losses = np.asarray([train_loss, test_loss]).T\n",
    "    np.savetxt(\"../\" + expPath + \"/training_losses.csv\", losses, delimiter=\",\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ### save f1 scores ####\n",
    "    print(\"saving f1 scores\")\n",
    "    np.savetxt(\"../\" + expPath + \"/f1Scores.csv\", f1Scores, delimiter=\",\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Early stopping #####\n",
    "    # If minimum global validation error does not decrease in X epochs, so stop training\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if new test loss is greater than the min valid error\n",
    "    if test_loss[nepoch] > minTestLossGlobalSoFar:\n",
    "        count_early_stop += 1\n",
    "        print(\"early stopping counter: \", count_early_stop)\n",
    "        \n",
    "    # if it is smaller\n",
    "    else: \n",
    "        count_early_stop = 0\n",
    "    \n",
    "    # analyze early stopping\n",
    "    if count_early_stop >= threshold_early_stop:\n",
    "        \n",
    "        print(\"Early stopping in epoch: \", nepoch)\n",
    "        text_file = open(\"../\" + expPath + \"/earlyStopping.txt\", \"w\")\n",
    "        metricsText = \"Epoch: {0}\\n ES counter: {1}\\n\".format(nepoch, count_early_stop)\n",
    "        text_file.write(metricsText)\n",
    "        text_file.close()\n",
    "        break\n",
    "        \n",
    "    \n",
    "    \n",
    "# final message\n",
    "print(\"training has finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # get metrics on trainig dataset\n",
    "\n",
    "# if not includeOtherFeatures:\n",
    "#     trainNormalizedFeatures = None\n",
    "#     validNormalizedFeatures = None\n",
    "    \n",
    "# getConfusionAndClassificationReport(\n",
    "#     trainLoader, \n",
    "#     nameLabel = \"Train\", \n",
    "#     passband = passband, \n",
    "#     model = model, \n",
    "#     staticLabels = only_these_labels, \n",
    "#     number_experiment = number_experiment, \n",
    "#     expPath = expPath, \n",
    "#     includeDeltaErrors = includeDeltaErrors, \n",
    "#     includeOtherFeatures = includeOtherFeatures,\n",
    "#     normalizedFeatures = trainNormalizedFeatures,\n",
    "# )\n",
    "\n",
    "\n",
    "# # get metrics on validation dataset\n",
    "# getConfusionAndClassificationReport(\n",
    "#     validationLoader, \n",
    "#     nameLabel = \"Validation\", \n",
    "#     passband = passband, \n",
    "#     model = model, \n",
    "#     staticLabels = only_these_labels, \n",
    "#     number_experiment = number_experiment, \n",
    "#     expPath = expPath, \n",
    "#     includeDeltaErrors = includeDeltaErrors, \n",
    "#     includeOtherFeatures = includeOtherFeatures,\n",
    "#     normalizedFeatures = validNormalizedFeatures,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get own model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create new dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data loader (minibatches)\n",
    "\n",
    "# training loader\n",
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    torch_dataset_lazy, \n",
    "#     batch_size = batch_training_size, \n",
    "    # to balance classes\n",
    "    sampler=ImbalancedDatasetSampler(\n",
    "        torch_dataset_lazy, \n",
    "        indices = trainIdx,\n",
    "        seed = seed\n",
    "#         indices = [0, 1, 2]\n",
    "    ),\n",
    "#     sampler = torch.utils.data.SubsetRandomSampler(\n",
    "#         trainIdx,\n",
    "#         generator = torch.Generator().manual_seed(seed)\n",
    "#     ),\n",
    "    # each worker retrieve data from disk, so the data will be ready to be processed by main process. The main process should get the data from disk, so if workers > 0, the workers will get the data (not the main process)\n",
    "    num_workers = 4,\n",
    "    \n",
    "    # https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/\n",
    "    # the dataloader loads the data in pinned memory (instead of pageable memory), avoiding one process (to transfer data from pageable memory to pinned memory, work done by CUDA driver)\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "\n",
    "# validation loader\n",
    "validationLoader = torch.utils.data.DataLoader(\n",
    "#     validationDataset, \n",
    "    torch_dataset_lazy,\n",
    "#     batch_size= batch_training_size,  \n",
    "    num_workers = 4,\n",
    "    pin_memory = True,\n",
    "    sampler = valIdx,\n",
    "#     sampler = torch.utils.data.SubsetRandomSampler(\n",
    "#         valIdx,\n",
    "#         generator = torch.Generator().manual_seed(seed)\n",
    "#     ),\n",
    "#     sampler=ImbalancedDatasetSampler(\n",
    "#         torch_dataset_lazy, \n",
    "#         indices = valIdx,\n",
    "#         seed = seed\n",
    "# #         indices = [0, 1, 2]\n",
    "#     ),\n",
    ")\n",
    "\n",
    "# # test loader\n",
    "# testLoader = torch.utils.data.DataLoader(testDataset)\n",
    "testLoader = torch.utils.data.DataLoader(\n",
    "#     validationDataset, \n",
    "    torch_dataset_lazy,\n",
    "#     batch_size= batch_training_size,  \n",
    "    num_workers = 4,\n",
    "    pin_memory = True,\n",
    "    sampler = testIdx,\n",
    "#     sampler = torch.utils.data.SubsetRandomSampler(\n",
    "#         testIdx,\n",
    "#         generator = torch.Generator().manual_seed(seed)\n",
    "#     ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadgin model\n",
    "model = torch.load(pathToSaveModel + \".txt\").to(device = cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class predictions\n",
    "trainModelPredictions = np.zeros(shape = (train_size,))\n",
    "\n",
    "# lc ids\n",
    "trainIds = np.zeros(shape = (train_size,))\n",
    "\n",
    "# test labels\n",
    "trainLabels = np.zeros(shape = (train_size,))\n",
    "\n",
    "print(\"getting predictions on train\")\n",
    "\n",
    "# index = 0\n",
    "    \n",
    "# this is for getting the other features\n",
    "trainLastIndex = 0\n",
    "    \n",
    "# iterate on test dataset\n",
    "# for data_ in trainLoader:\n",
    "for idx, data_ in enumerate(trainLoader):\n",
    "        \n",
    "#         # index to include batch data\n",
    "#         index_ = index + data_[0].shape[0]\n",
    "\n",
    "        data = data_[0]\n",
    "\n",
    "        # this take the deltas (time and magnitude)\n",
    "        data = generateDeltas(data, passband, includeDeltaErrors).type(torch.FloatTensor).to(device = cuda_device)\n",
    "            \n",
    "#         # get model output\n",
    "#         outputs = model.forward(data, includeDeltaErrors)\n",
    "        \n",
    "        # add other features\n",
    "        # [batch size, features dim]\n",
    "        if includeOtherFeatures:\n",
    "            \n",
    "            # index to include batch data\n",
    "            trainLastIndex_ = trainLastIndex + data_[0].shape[0]\n",
    "\n",
    "            # get only the normalized data from the batch (by indexation)\n",
    "            otherFeatures = trainNormalizedFeatures[trainLastIndex : trainLastIndex_, :].to(device = cuda_device)\n",
    "            \n",
    "            # update index \n",
    "            trainLastIndex = trainLastIndex_\n",
    "            \n",
    "            # validate data\n",
    "            if np.any(torch.isnan(otherFeatures).cpu().numpy()):\n",
    "                \n",
    "                print(f\"other features with nan values in epoch {nepoch}\")\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors, otherFeatures)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors)\n",
    "        \n",
    "#         print(trainLastIndex_)\n",
    "        \n",
    "#         # get model predictions\n",
    "#         trainModelPredictions[index : index_] = only_these_labels[torch.argmax(outputs, 1).cpu().numpy()[0]]\n",
    "        \n",
    "#         # get lc ids\n",
    "#         trainIds[index : index_] = data_[2]\n",
    "        \n",
    "#         # save labels\n",
    "#         trainLabels[index : index_] = data_[1]\n",
    "        \n",
    "#         # update index \n",
    "#         index = index_\n",
    "        \n",
    "        # get model predictions\n",
    "        trainModelPredictions[idx] = only_these_labels[torch.argmax(outputs, 1).cpu().numpy()[0]]\n",
    "        \n",
    "        # get lc ids\n",
    "        trainIds[idx] = data_[2]\n",
    "        \n",
    "        # save labels\n",
    "        trainLabels[idx] = data_[1]\n",
    "        \n",
    "        \n",
    "print(\"predictions ready\")# Get own model predictions\n",
    "\n",
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class predictions\n",
    "validModelPredictions = np.zeros(shape = (validation_size,))\n",
    "\n",
    "# lc ids\n",
    "validIds = np.zeros(shape = (validation_size,))\n",
    "\n",
    "# test labels\n",
    "validLabels = np.zeros(shape = (validation_size,))\n",
    "\n",
    "print(\"getting predictions on validtion\")\n",
    "\n",
    "# index = 0\n",
    "trainLastIndex = 0\n",
    "\n",
    "# iterate on test dataset\n",
    "# for data_ in (validationLoader):\n",
    "for idx, data_ in enumerate(validationLoader):\n",
    "    \n",
    "#         # index to include batch data\n",
    "#         index_ = index + data_[0].shape[0]\n",
    "        \n",
    "        data = data_[0]\n",
    "\n",
    "        # this take the deltas (time and magnitude)\n",
    "        data = generateDeltas(data, passband, includeDeltaErrors).type(torch.FloatTensor).to(device = cuda_device)\n",
    "            \n",
    "#         # get model output\n",
    "#         outputs = model.forward(data, includeDeltaErrors)\n",
    "        \n",
    "        # add other features\n",
    "        # [batch size, features dim]\n",
    "        if includeOtherFeatures:\n",
    "            \n",
    "            # index to include batch data\n",
    "            trainLastIndex_ = trainLastIndex + data_[0].shape[0]\n",
    "\n",
    "            # get only the normalized data from the batch (by indexation)\n",
    "            otherFeatures = validNormalizedFeatures[trainLastIndex : trainLastIndex_, :].to(device = cuda_device)\n",
    "            \n",
    "            # update index \n",
    "            trainLastIndex = trainLastIndex_\n",
    "            \n",
    "            # validate data\n",
    "            if np.any(torch.isnan(otherFeatures).cpu().numpy()):\n",
    "                \n",
    "                print(f\"other features with nan values in epoch {nepoch}\")\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors, otherFeatures)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors)\n",
    "            \n",
    "#         # get model predictions\n",
    "#         validModelPredictions[index : index_] = only_these_labels[torch.argmax(outputs, 1).cpu().numpy()[0]]\n",
    "        \n",
    "#         # get lc ids\n",
    "#         validIds[index : index_] = data_[2]\n",
    "        \n",
    "#         # save labels\n",
    "#         validLabels[index : index_] = data_[1]\n",
    "        \n",
    "#         # update index \n",
    "#         index = index_\n",
    "\n",
    "        # get model predictions\n",
    "        validModelPredictions[idx] = only_these_labels[torch.argmax(outputs, 1).cpu().numpy()[0]]\n",
    "        \n",
    "        # get lc ids\n",
    "        validIds[idx] = data_[2]\n",
    "        \n",
    "        # save labels\n",
    "        validLabels[idx] = data_[1]\n",
    "        \n",
    "        \n",
    "print(\"predictions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class predictions\n",
    "testModelPredictions = np.zeros(shape = (test_size,))\n",
    "\n",
    "# lc ids\n",
    "testIds = np.zeros(shape = (test_size,))\n",
    "\n",
    "# test labels\n",
    "testLabels = np.zeros(shape = (test_size,))\n",
    "\n",
    "print(\"getting predictions on test\")\n",
    "\n",
    "trainLastIndex = 0\n",
    "\n",
    "# iterate on test dataset\n",
    "for idx, data_ in enumerate(testLoader):\n",
    "        \n",
    "        data = data_[0]\n",
    "\n",
    "        # this take the deltas (time and magnitude)\n",
    "        data = generateDeltas(data, passband, includeDeltaErrors).type(torch.FloatTensor).to(device = cuda_device)\n",
    "            \n",
    "#         # get model output\n",
    "#         outputs = model.forward(data, includeDeltaErrors)\n",
    "        \n",
    "        # add other features\n",
    "        # [batch size, features dim]\n",
    "        if includeOtherFeatures:\n",
    "            \n",
    "            # index to include batch data\n",
    "            trainLastIndex_ = trainLastIndex + data_[0].shape[0]\n",
    "\n",
    "            # get only the normalized data from the batch (by indexation)\n",
    "            otherFeatures = testNormalizedFeatures[trainLastIndex : trainLastIndex_, :].to(device = cuda_device)\n",
    "            \n",
    "            # update index \n",
    "            trainLastIndex = trainLastIndex_\n",
    "            \n",
    "            # validate data\n",
    "            if np.any(torch.isnan(otherFeatures).cpu().numpy()):\n",
    "                \n",
    "                print(f\"other features with nan values in epoch {nepoch}\")\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors, otherFeatures)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors)\n",
    "            \n",
    "        # get model predictions\n",
    "        testModelPredictions[idx] = only_these_labels[torch.argmax(outputs, 1).cpu().numpy()[0]]\n",
    "        \n",
    "        # get lc ids\n",
    "        testIds[idx] = data_[2]\n",
    "        \n",
    "        # save labels\n",
    "        testLabels[idx] = data_[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "print(\"predictions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "results = {\n",
    "    \n",
    "    # train\n",
    "    \"trainIds\": trainIds,\n",
    "    \"trainLabels\": trainLabels,\n",
    "    \"trainPredictions\": trainModelPredictions,\n",
    "    \n",
    "     # validation\n",
    "    \"validIds\": validIds,\n",
    "    \"validLabels\": validLabels,\n",
    "    \"validPredictions\": validModelPredictions,\n",
    "    \n",
    "    \n",
    "    # test\n",
    "    \"testIds\": testIds,\n",
    "    \"testLabels\": testLabels,\n",
    "    \"testPredictions\": testModelPredictions,\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# save object\n",
    "\n",
    "if trainingOnGuanaco or trainWithJustPython:\n",
    "\n",
    "    a_file = open(\"../experiments/comparingModels/seed\" + str(seed) + \"/ownModel/OwnModel\" + number_experiment + \"Predictions.pkl\", \"wb\")\n",
    "    pickle.dump(results, a_file)\n",
    "    a_file.close()\n",
    "\n",
    "    print(\"model predictions saved on a file\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(\"not save metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop execution if it's on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if  trainingOnGuanaco or trainWithJustPython:\n",
    "\n",
    "    sys.exit(\"Exit from code, because we are in cluster or running locally. Training has finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
