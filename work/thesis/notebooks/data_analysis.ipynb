{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "This notebook is to train the encoder as a classifier with the idea of validate the encoder architecture first and then use this to train the VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on guanaco\n",
    "# ATENTION: if it is going to run on guanaco:\n",
    "# 1) comment the %matplotlib magic in next block and any magic (something like %code)\n",
    "# 2) Change to True the trainingOnGuanaco vairbale\n",
    "# 3) set epoch with an appropiate number\n",
    "# 4) add comment to experiemnts\n",
    "# 5) Add this file as python file \n",
    "# 6) Change launchJobOnGuanaco file to run this file but with python format\n",
    "trainingOnGuanaco = False\n",
    "\n",
    "# train without notebook\n",
    "trainWithJustPython = False\n",
    "\n",
    "# number_experiment (this is just a name)\n",
    "# priors:\n",
    "# 1\n",
    "number_experiment = 27\n",
    "number_experiment = str(number_experiment)\n",
    "\n",
    "# seed to generate same datasets\n",
    "seed = 1\n",
    "\n",
    "# training\n",
    "epochs = 100000\n",
    "\n",
    "# max elements by class\n",
    "max_elements_per_class = 15000\n",
    "\n",
    "# train with previous model\n",
    "trainWithPreviousModel = False\n",
    "\n",
    "# include delta errors\n",
    "includeDeltaErrors = True\n",
    "\n",
    "# band\n",
    "passband = [0]\n",
    "# passband = [1]\n",
    "# passband = [2]\n",
    "# passband = [3]\n",
    "# passband = [4]\n",
    "# passband = [5]\n",
    "# passband = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "\n",
    "# include ohter feautures\n",
    "includeOtherFeatures = False\n",
    "\n",
    "# num of features to add\n",
    "# ṕvar by channel\n",
    "otherFeaturesDim = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda device\n",
    "cuda_device = 0\n",
    "cuda_device = \"cuda:\" + str(cuda_device)\n",
    "\n",
    "# classes to analyze\n",
    "# 42,  90,  16,  67,  62, 993,  92,  52,  88,  65, 991, 992,  15,\n",
    "#        95,   6,  53, 994,  64\n",
    "\n",
    "# periodic\n",
    "# only_these_labels = [16, 92, 53]\n",
    "\n",
    "# periodic + variable\n",
    "only_these_labels = [16, 92, 53, 88, 65, 6]\n",
    "# 53 has 24 light curves\n",
    "\n",
    "# only_these_labels = [16, 92]\n",
    "# only_these_labels = [16, 92]\n",
    "# only_these_labels = [42,  90,  16,  67,  62, 993,  92,  52,  88,  65, 991, 992,  15,\n",
    "#         95,   6,  53, 994,  64]\n",
    "\n",
    "# VAE parameters\n",
    "latentDim = 100\n",
    "hiddenDim = 100\n",
    "inputDim = 72\n",
    "\n",
    "batch_training_size = 128\n",
    "\n",
    "# early stopping \n",
    "threshold_early_stop = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp 27 + encoder as clasifier with periodic + variable + class balancing + 1 conv layer more + 1 channels + seed 1 + include delta errors + max by class 15000 + not other features\n"
     ]
    }
   ],
   "source": [
    "# add general comment about experiment \n",
    "# comment = \"encoder as clasifier with periodic + variable (with class balancing) + 1 conv layer more\"\n",
    "comment = \"exp \" + number_experiment + \" + encoder as clasifier with periodic + variable + class balancing + 1 conv layer more + \" + str(len(passband)) + \" channels + seed \" + str(seed) + \" + \" + (\"include delta errors\" if includeDeltaErrors else \"without delta errors\") + \" + max by class \" + str(max_elements_per_class) + \" + \" + (\"\" if includeOtherFeatures else \"not\") + \" other features\"\n",
    "\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# from tqdm import tqdm_notebook\n",
    "\n",
    "if not trainingOnGuanaco:\n",
    "    \n",
    "    %matplotlib notebook\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "else:\n",
    "    print(\"not load magics\")\n",
    "    \n",
    "# import functions to load dataset\n",
    "import sys\n",
    "sys.path.append(\"./codesToDatasets\")\n",
    "from plasticc_dataset_torch import get_plasticc_datasets\n",
    "from plasticc_plotting import plot_light_curve\n",
    "\n",
    "import math\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "# local imports\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "sys.path.append('../models')\n",
    "# from classifier import EncoderClassifier, \n",
    "from classifierPrototype import EncoderClassifier\n",
    "\n",
    "sys.path.append(\"./aux/\")\n",
    "from auxFunctions import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the path to save model while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create experiment's folder\n",
    "tmpGuanaco = \"/home/lbravo/thesis/thesis/work/thesis/\"\n",
    "tmpLocal = \"/home/leo/Desktop/thesis/work/thesis/\"\n",
    "\n",
    "expPath = \"experiments/\" + number_experiment + \"/seed\" + str(seed) + \"/maxClass\" + str(int(max_elements_per_class/1000)) + \"k\"\n",
    "\n",
    "folder_path = (tmpGuanaco + expPath) if trainingOnGuanaco else (tmpLocal + expPath)\n",
    "# !mkdir folder_path\n",
    "# os.makedirs(os.path.dirname(folder_path), exist_ok=True)\n",
    "\n",
    "# check if folder exists\n",
    "# if not(os.path.isdir(folder_path)):\n",
    "        \n",
    "#     # create folder\n",
    "#     try:\n",
    "#         os.makedirs(folder_path)\n",
    "        \n",
    "#     except OSError as error:\n",
    "#         print (\"Creation of the directory %s failed\" % folder_path)\n",
    "#         print(error)\n",
    "#     else:\n",
    "#         print (\"Successfully created the directory %s \" % folder_path)\n",
    "# else:\n",
    "#     print(\"folder already exists\")\n",
    "\n",
    "# # define paht to save model while training\n",
    "# pathToSaveModel = (tmpGuanaco + expPath + \"/model\") if trainingOnGuanaco else (tmpLocal + expPath + \"/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to dataset\n",
    "pathToFile = \"/home/shared/astro/PLAsTiCC/\" if trainingOnGuanaco else \"/home/leo/Downloads/plasticData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset with pytorch tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have selected lazy loading. Light curves will be loaded ondemand from the harddrive\n",
      "Found 2 csv files at given path\n",
      "Loading /home/leo/Downloads/plasticData/plasticc_train_lightcurves.csv\n",
      "Loading /home/leo/Downloads/plasticData/plasticc_test_set_batch1.csv\n"
     ]
    }
   ],
   "source": [
    "# torch_dataset_lazy = get_plasticc_datasets(pathToFile)\n",
    "\n",
    "# Light curves are tensors are now [bands, [mjd, flux, err, mask],\n",
    "# lc_data, lc_label, lc_plasticc_id                              \n",
    "torch_dataset_lazy = get_plasticc_datasets(pathToFile, only_these_labels=only_these_labels, max_elements_per_class = max_elements_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset test ok\n"
     ]
    }
   ],
   "source": [
    "assert torch_dataset_lazy.__len__() != 494096, \"dataset should be smaller\"\n",
    "print(\"dataset test ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lc_data, label, lc_id = torch_dataset_lazy.__getitem__(1300)\n",
    "\n",
    "# # 11 was good\n",
    "\n",
    "# fig, ax = plt.subplots(figsize= (8,6), tight_layout=True)\n",
    "# for band, band_name in enumerate('ugrizY'):\n",
    "#     mask = lc_data[band, 3, :] == 1\n",
    "#     mjd, flux, flux_err = lc_data[band, :3, mask]\n",
    "#     ax.errorbar(mjd, flux, flux_err, fmt='.', label=band_name)\n",
    "# ax.legend()\n",
    "# ax.set_ylabel('Flux', fontsize = 20)\n",
    "# ax.set_xlabel('Modified Julian Data', fontsize = 20)\n",
    "# ax.set_title(f'PLAsTiCC ID: {lc_id} Label: {label}', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig(\"lightCurve.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting data (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataInformation(dataset):\n",
    "    \n",
    "    totalSize = dataset.__len__()\n",
    "    \n",
    "    print(\"total light curves: \", totalSize)\n",
    "#     ids = np.zeros(shape = (totalSize,))\n",
    "#     targets = np.zeros(shape = (totalSize,))\n",
    "    \n",
    "    # samples by filter\n",
    "    samplesByFilter = np.zeros(shape = (totalSize, 6))\n",
    "    \n",
    "    # time length mean\n",
    "    timeLength = np.zeros(shape = (totalSize, 6))\n",
    "    \n",
    "    # mean error\n",
    "    errorMeanByFilter = np.zeros(shape = (totalSize, 6))\n",
    "    \n",
    "    # flux \n",
    "    fluxMeanByFilter = np.zeros(shape = (totalSize, 6))\n",
    "    fluxStdByFilter = np.zeros(shape = (totalSize, 6))\n",
    "    \n",
    "    # delta time mean\n",
    "    deltaTimeMean = np.zeros(shape= (totalSize, 6))\n",
    "    \n",
    "    targets = np.zeros(shape = (totalSize, 6))\n",
    "    \n",
    "    # to filter after\n",
    "    arrays = [samplesByFilter, timeLength, errorMeanByFilter, fluxMeanByFilter, fluxStdByFilter, deltaTimeMean, targets]\n",
    "    \n",
    "    for idx, data_ in enumerate(dataset):\n",
    "        \n",
    "#         print(data[0][0, 2, :])\n",
    "        \n",
    "        data = data_[0]\n",
    "        \n",
    "        # get samples by filter\n",
    "        samplesByFilter[idx, :] = torch.count_nonzero(data[:, 3, :], dim = 1)\n",
    "        \n",
    "        # get targets\n",
    "        targets[idx, :] = data_[1]\n",
    "        \n",
    "        # iterating by each filter\n",
    "        for i in range(6):\n",
    "            \n",
    "#             data = data_[0]\n",
    "            \n",
    "            # taking last valid index\n",
    "            # mask= [1,1,1,1,1, 0,0,0,0]\n",
    "            # so it is taking the last non zero -1 (last item index)\n",
    "            lastIndex = samplesByFilter[idx, i].astype(int)-1\n",
    "            \n",
    "            # test\n",
    "#             assert data[0][i, 3, lastIndex] == 1 and data[0][i, 3, lastIndex+1] == 1\n",
    "            \n",
    "            # diff between last and first time\n",
    "            timeLength[idx, i] = data[i, 0, lastIndex]  - data[i, 0, 0]\n",
    "            \n",
    "            # mean error\n",
    "#             print(torch.mean(data[0][i, 2, 0:lastIndex], dim = 0).shape)\n",
    "            errorMeanByFilter[idx, i] = torch.mean(data[i, 2, 0:lastIndex], dim = 0)\n",
    "            \n",
    "            # flux mean\n",
    "            fluxMeanByFilter[idx, i] = torch.mean(data[i, 1, 0:lastIndex], dim = 0)\n",
    "#             unbiased = True give nan for channle 0\n",
    "            fluxStdByFilter[idx, i] = torch.std(data[i, 1, 0:lastIndex], dim = 0)\n",
    "            \n",
    "        \n",
    "            # delta time\n",
    "            originalTime = data[i, 0, 0:lastIndex]\n",
    "#             print(originalTime.shape)\n",
    "#             if originalTime.shape > :\n",
    "        \n",
    "            deltaTimeMean[idx, i] = torch.mean(originalTime[1:] - originalTime[:-1], dim = 0)\n",
    "    \n",
    "# #             if np.isnan(deltaTimeMean[idx, i]):\n",
    "#             if np.isnan(fluxStdByFilter[idx, i]):\n",
    "#                 print(\"nan value\")\n",
    "#                 print(lastIndex)\n",
    "#                 print(originalTime)\n",
    "#                 print(torch.mean(originalTime[1:] - originalTime[:-1], dim = 0))\n",
    "#                 print(fluxMeanByFilter[idx, i])\n",
    "#                 print(data[i, 1, 0:lastIndex])\n",
    "#             print(deltaTimeMean[idx, i])\n",
    "        \n",
    "#     applyMask = False\n",
    "    \n",
    "    # remove nan values\n",
    "#     for idx, array in enumerate(arrays):\n",
    "            \n",
    "#         if np.any(np.isnan(array)):\n",
    "            \n",
    "            \n",
    "    mask = ~np.isnan(fluxStdByFilter).any(axis = 1)\n",
    "    for idx, array in enumerate(arrays):\n",
    "            \n",
    "        arrays[idx] = array[mask, :]\n",
    "            \n",
    "#         if np.any(np.isnan(array)):\n",
    "            \n",
    "#         array = array[mask, :]\n",
    "#         arrays[idx] = array\n",
    "    \n",
    "    \n",
    "#             print(tmpDeltaTime)\n",
    "# #     print(samplesByFilter.mean(dim = 0))\n",
    "#     print(timeLength.shape)\n",
    "#     print(errorByFilter)\n",
    "    \n",
    "#     return samplesByFilter, timeLength, errorMeanByFilter, fluxMeanByFilter, fluxStdByFilter,deltaTimeMean\n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total light curves:  3276\n"
     ]
    }
   ],
   "source": [
    "samplesByFilter, timeLength, errorMeanByFilter, fluxMeanByFilter, fluxStdByFilter, deltaTimeMean, targets = getDataInformation(torch_dataset_lazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3275, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStats(arrays, targets):\n",
    "    \n",
    "    arrayNames = [\"samples\", \"time length\", \"error mean by filter\", \"flux mean by filter\", \"flux Std By Filter\", \"delta Time Mean\"]\n",
    "    \n",
    "    targets_ = np.unique(targets)\n",
    "    \n",
    "    statsMeans = np.zeros(shape = (len(arrayNames), targets_.shape[0], 6) )\n",
    "    \n",
    "    statsStd = np.zeros(shape = (len(arrayNames), targets_.shape[0], 6) )\n",
    "    \n",
    "    \n",
    "    for idx, array in enumerate(arrays):\n",
    "            \n",
    "        print(arrayNames[idx] + \"\\n\")\n",
    "        \n",
    "        for idxClass, class_ in enumerate(targets_):\n",
    "\n",
    "            print(class_)\n",
    "            \n",
    "    #         print(targets.shape)\n",
    "            maskClass = (targets[:, 0] == class_)\n",
    "            \n",
    "#             print(array[maskClass,:].shape[0])\n",
    "            print(arrayNames[idx])\n",
    "            print(np.mean(array[maskClass,:], 0))\n",
    "            print(np.std(array[maskClass,:], 0))\n",
    "            print(\"\\n\\n\")\n",
    "            \n",
    "            \n",
    "            statsMeans[idx, idxClass, :] = np.mean(array[maskClass,:], 0)\n",
    "            statsStd[idx, idxClass, :] = np.std(array[maskClass,:], 0)\n",
    "            \n",
    "#             dict_[arrayNames[idx]][targets[0, 0]][\"mean\"] =  np.mean(array[maskClass,:], 0)\n",
    "#             tmpDict[targets_] = np.mean(array[maskClass,:], 0)\n",
    "#             print(np.mean(array[maskClass,:], 0))\n",
    "#             dict_[arrayNames[idx]][targets_]=  np.mean(array[maskClass,:], 0)\n",
    "\n",
    "#         print(tmpDict)\n",
    "    \n",
    "    return statsMeans, statsStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples\n",
      "\n",
      "6.0\n",
      "samples\n",
      "[15.8089172  14.43312102 24.12738854 22.99363057 32.         35.50318471]\n",
      "[14.14940018 12.4600119   9.87419342 10.23948696  8.36431608 10.34908175]\n",
      "\n",
      "\n",
      "\n",
      "16.0\n",
      "samples\n",
      "[19.6429342  18.24271845 26.71305286 25.99568501 33.1639698  36.46494067]\n",
      "[18.55359917 16.12707053 12.35704325 12.85459227 10.21954663 10.89097736]\n",
      "\n",
      "\n",
      "\n",
      "53.0\n",
      "samples\n",
      "[23.38888889 22.22222222 29.         28.47222222 35.11111111 39.        ]\n",
      "[21.65932924 18.11707468 14.18723996 14.4865572  10.68950102 10.24966124]\n",
      "\n",
      "\n",
      "\n",
      "65.0\n",
      "samples\n",
      "[28.7840691  26.11516315 33.04318618 32.40978887 37.45201536 39.86948177]\n",
      "[24.38062727 20.40309196 15.46654667 15.96276919 12.4560687  11.75078098]\n",
      "\n",
      "\n",
      "\n",
      "88.0\n",
      "samples\n",
      "[43.71311475 38.85655738 42.6420765  42.46994536 44.71584699 45.76502732]\n",
      "[26.32981631 20.58077312 15.53421326 15.79990743 12.69422313 11.36200075]\n",
      "\n",
      "\n",
      "\n",
      "92.0\n",
      "samples\n",
      "[36.0839895  33.14173228 38.17585302 37.68766404 41.73490814 43.80839895]\n",
      "[26.17633081 21.61390789 16.51208242 17.03307205 13.10789325 12.15462639]\n",
      "\n",
      "\n",
      "\n",
      "time length\n",
      "\n",
      "6.0\n",
      "time length\n",
      "[924.71317675 843.25569765 815.08250398 931.25490147 968.85972333\n",
      " 973.17702528]\n",
      "[163.75988975 111.40687448  95.36244925 113.22280878 105.56050063\n",
      " 100.92798599]\n",
      "\n",
      "\n",
      "\n",
      "16.0\n",
      "time length\n",
      "[895.65531452 843.65355313 805.1663127  913.67010518 951.46821905\n",
      " 955.34273025]\n",
      "[157.2215561  116.28450199  90.03248087 108.34587236 106.65966626\n",
      " 104.44095913]\n",
      "\n",
      "\n",
      "\n",
      "53.0\n",
      "time length\n",
      "[854.55349392 864.26475694 818.76768663 937.75108507 959.69259983\n",
      " 956.05772569]\n",
      "[179.29276213 125.00683064  65.47804634 115.28634322 105.59289007\n",
      " 101.13457039]\n",
      "\n",
      "\n",
      "\n",
      "65.0\n",
      "time length\n",
      "[868.06044941 843.52606166 823.27519194 891.54816834 923.87069263\n",
      " 926.62459888]\n",
      "[125.44304212  89.35557528  88.96872504  90.06643512  99.00015138\n",
      "  97.94451625]\n",
      "\n",
      "\n",
      "\n",
      "88.0\n",
      "time length\n",
      "[832.98541026 843.40541752 836.88764195 865.2201428  881.01052873\n",
      " 883.70044612]\n",
      "[92.77341603 64.28083811 62.53722421 64.18221813 69.44679663 70.72925592]\n",
      "\n",
      "\n",
      "\n",
      "92.0\n",
      "time length\n",
      "[870.55440043 847.86911499 835.74086491 896.05716864 915.40152354\n",
      " 915.16288345]\n",
      "[130.90067057  88.91405632  64.48568532  93.20378425  97.20526004\n",
      "  93.42128782]\n",
      "\n",
      "\n",
      "\n",
      "error mean by filter\n",
      "\n",
      "6.0\n",
      "error mean by filter\n",
      "[248.71728018  43.67462461  47.7701952   45.4186888   48.84556349\n",
      "  71.23092687]\n",
      "[1381.4752834   149.67940282  206.0200217   190.65917015  128.14814804\n",
      "  151.486364  ]\n",
      "\n",
      "\n",
      "\n",
      "16.0\n",
      "error mean by filter\n",
      "[302.58537407  30.57093407  20.78259024  17.88154881  21.02434027\n",
      "  34.82194951]\n",
      "[5641.30199131  181.55216744   50.65348686   29.86088781   21.05370321\n",
      "   24.88724257]\n",
      "\n",
      "\n",
      "\n",
      "53.0\n",
      "error mean by filter\n",
      "[ 39.80622052  37.37959937  41.09208677 196.73652502 276.2882185\n",
      " 347.2250501 ]\n",
      "[108.37312965 104.22445478  98.0974089  586.74665246 742.34365514\n",
      " 899.86482409]\n",
      "\n",
      "\n",
      "\n",
      "65.0\n",
      "error mean by filter\n",
      "[12.30017067  3.33707365  3.95547741  6.83385776 12.77022895 26.97062391]\n",
      "[46.12995013  7.1881439   3.21567944  4.60675722  8.22374053 16.12085511]\n",
      "\n",
      "\n",
      "\n",
      "88.0\n",
      "error mean by filter\n",
      "[12.22629878  3.47816816  3.12168053  4.22344337  7.11175709 15.30369957]\n",
      "[72.58410989 13.09873993  7.1733065   6.8797272   8.11985381 14.4566033 ]\n",
      "\n",
      "\n",
      "\n",
      "92.0\n",
      "error mean by filter\n",
      "[3379.15458042  143.56011487   53.022555     34.826868     29.93580659\n",
      "   40.52814509]\n",
      "[6.47255924e+04 1.15373907e+03 1.37528489e+02 6.78568749e+01\n",
      " 4.61814598e+01 5.17957704e+01]\n",
      "\n",
      "\n",
      "\n",
      "flux mean by filter\n",
      "\n",
      "6.0\n",
      "flux mean by filter\n",
      "[ 113.80634132  539.52956947 1238.00621433 1259.00873957  925.12881043\n",
      " 1212.90222749]\n",
      "[  846.6109364   5630.9248395  13073.41411306 15541.24800614\n",
      " 12421.71876664 16986.55971953]\n",
      "\n",
      "\n",
      "\n",
      "16.0\n",
      "flux mean by filter\n",
      "[20.76517469 -5.27579843  1.14834821  3.80871811 -2.45555156 -1.16057235]\n",
      "[464.26500257 360.34838006 439.96341212 325.74403133 206.33882736\n",
      "  85.83686701]\n",
      "\n",
      "\n",
      "\n",
      "53.0\n",
      "flux mean by filter\n",
      "[    7.6325356    174.06572763   254.59670662  -968.91012769\n",
      " -2767.42437066 -4533.64900568]\n",
      "[   46.04841761  1217.52437088  1819.21087811  9570.0596312\n",
      " 16073.75451097 21767.36614855]\n",
      "\n",
      "\n",
      "\n",
      "65.0\n",
      "flux mean by filter\n",
      "[-1.94925169 -2.73857283 -2.49906378 -3.27591204 -2.62300205 -2.30587654]\n",
      "[57.65540575 65.13225744 56.72295096 53.20529611 49.50642778 45.52651029]\n",
      "\n",
      "\n",
      "\n",
      "88.0\n",
      "flux mean by filter\n",
      "[11.6037075  11.08509712 10.92534309 11.12728779 12.02145037 17.23604897]\n",
      "[164.30151202 164.94692282 156.51960957 157.00996879 166.55651114\n",
      " 227.68900232]\n",
      "\n",
      "\n",
      "\n",
      "92.0\n",
      "flux mean by filter\n",
      "[435.04271819  86.27584777 -22.7475358   53.05401022  29.56968894\n",
      "  -1.59493374]\n",
      "[8247.13224054 2099.66555864 1527.33923442 1123.47660819 1016.55744085\n",
      "  986.57880369]\n",
      "\n",
      "\n",
      "\n",
      "flux Std By Filter\n",
      "\n",
      "6.0\n",
      "flux Std By Filter\n",
      "[ 407.113648    841.649801   1955.4877966  2052.73810458 2276.6443563\n",
      " 2688.60581114]\n",
      "[ 1556.80628548  5892.97048864 12679.47151375 15081.12450615\n",
      " 13701.27909379 18038.42419794]\n",
      "\n",
      "\n",
      "\n",
      "16.0\n",
      "flux Std By Filter\n",
      "[264.45739564 283.21727767 321.57370517 239.86000585 174.82638812\n",
      "  93.95446466]\n",
      "[4017.95766496  599.67434014  594.97093908  438.87930574  306.39127077\n",
      "  138.5404227 ]\n",
      "\n",
      "\n",
      "\n",
      "53.0\n",
      "flux Std By Filter\n",
      "[   39.50168143   966.98006452  1836.07058875  6182.59575144\n",
      "  9980.57994588 12778.94812181]\n",
      "[  106.78482619  1426.1924824   2220.65357498  6153.59149178\n",
      "  8937.28402854 11665.20949114]\n",
      "\n",
      "\n",
      "\n",
      "65.0\n",
      "flux Std By Filter\n",
      "[29.77616336 27.04729231 36.92546389 31.74171879 37.94658673 49.83899963]\n",
      "[156.13295237 117.07203556 105.40188798  96.8640079   94.10326064\n",
      " 100.70854384]\n",
      "\n",
      "\n",
      "\n",
      "88.0\n",
      "flux Std By Filter\n",
      "[37.85331836 34.17429221 31.81286471 35.02951714 40.07057794 50.02190126]\n",
      "[111.26089813  86.65068284  74.32411082  87.26526405 100.13501339\n",
      " 143.68739964]\n",
      "\n",
      "\n",
      "\n",
      "92.0\n",
      "flux Std By Filter\n",
      "[3192.44452535 3122.39122702 2479.49562685 1948.69015597 1667.65419134\n",
      " 1667.83112437]\n",
      "[53141.69991997  2285.41071496  1608.01507945  1266.8076921\n",
      "  1082.36791481  1097.53759812]\n",
      "\n",
      "\n",
      "\n",
      "delta Time Mean\n",
      "\n",
      "6.0\n",
      "delta Time Mean\n",
      "[89.46063836 87.81111218 39.69229882 48.29826223 33.4876935  31.29414012]\n",
      "[35.21046117 31.3258276  11.36139431 14.1478384   7.37267064 12.1332356 ]\n",
      "\n",
      "\n",
      "\n",
      "16.0\n",
      "delta Time Mean\n",
      "[78.57593727 79.528688   36.98209063 44.38691117 32.57688862 29.84720256]\n",
      "[36.30559853 35.77267717 13.20444571 16.37262988  9.32886628 10.06763763]\n",
      "\n",
      "\n",
      "\n",
      "53.0\n",
      "delta Time Mean\n",
      "[67.19332446 70.19378005 36.60087336 42.53051345 30.88574455 27.23647393]\n",
      "[35.92673131 37.36879066 14.55416641 16.92625454  9.92809476  8.15585556]\n",
      "\n",
      "\n",
      "\n",
      "65.0\n",
      "delta Time Mean\n",
      "[60.56493142 63.22502694 31.53066253 36.80970088 28.85327449 26.64988952]\n",
      "[37.16108536 38.1480312  12.39725203 17.01974809 10.04270723  9.22170638]\n",
      "\n",
      "\n",
      "\n",
      "88.0\n",
      "delta Time Mean\n",
      "[38.77305517 42.19445024 24.58443305 26.49998046 23.02862293 22.03599478]\n",
      "[35.5726938  38.27163794 11.81705049 14.91815559  9.45099795  8.57256328]\n",
      "\n",
      "\n",
      "\n",
      "92.0\n",
      "delta Time Mean\n",
      "[53.86505759 53.78836022 28.81599964 32.85921123 26.10109686 24.64499769]\n",
      "[43.50967525 43.52424053 14.30044374 18.18863902 10.75764205 12.88574516]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statsMeans, statsStd = getStats([samplesByFilter, timeLength, errorMeanByFilter, fluxMeanByFilter, fluxStdByFilter, deltaTimeMean], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.8089172  14.43312102 24.12738854 22.99363057 32.         35.50318471]\n",
      "[14.14940018 12.4600119   9.87419342 10.23948696  8.36431608 10.34908175]\n"
     ]
    }
   ],
   "source": [
    "print(statsMeans[0, 0,:])\n",
    "\n",
    "print(statsStd[0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getErrorComplicated(err, flux):\n",
    "    \n",
    "#     print(np.median(err, 0))\n",
    "#     print(np.std(flux, 0))\n",
    "    \n",
    "    stats = np.zeros(shape = (2, 6))\n",
    "    \n",
    "    stats[0, :] = np.median(err, 0)/np.std(flux, 0)\n",
    "    \n",
    "#     print(\"error normalizado promedio\")\n",
    "#     print(np.median(err, 0)/np.std(flux, 0))\n",
    "#     print(\"\\n\")\n",
    "    \n",
    "    stats[1, :] = np.std(err,0)/np.std(flux,0)\n",
    "    \n",
    "#     print(\"errores fotométricos normalizados\")\n",
    "#     print(np.std(err,0)/np.std(flux,0))\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorStats = getErrorComplicated(errorMeanByFilter, fluxMeanByFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"stats\": {\"means\": statsMeans, \"std\": statsStd}, \"errorStats\": errorStats}\n",
    "\n",
    "# # save stats\n",
    "a_file = open(\"./datasetStats.pkl\", \"wb\")\n",
    "pickle.dump(results, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load ids dictionary\n",
    "# a_file = open(\"./datasetStats.pkl\", \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "\n",
    "# get light curves ids, targets\n",
    "# ids, targets, lightCurvesIds = getLightCurvesIds(torch_dataset_lazy)\n",
    "\n",
    "\n",
    "# test array shapes\n",
    "# assert len(targets) == torch_dataset_lazy.__len__()\n",
    "# print(ids, len(ids), targets, len(targets))\n",
    "# get light curves targets\n",
    "# print(\"# light curves ids: \" + str(len(ids)))\n",
    "\n",
    "# # split training\n",
    "# trainIdx, tmpIdx = train_test_split(\n",
    "#     ids,\n",
    "#     test_size = 0.2,\n",
    "#     shuffle = True,\n",
    "#     stratify = targets,\n",
    "#     random_state = seed\n",
    "# )\n",
    "\n",
    "# # float to int\n",
    "# tmpIdx = tmpIdx.astype(int)\n",
    "\n",
    "# # split val, test\n",
    "# valIdx, testIdx = train_test_split(\n",
    "#     tmpIdx,\n",
    "# #     targets,\n",
    "#     test_size = 0.5,\n",
    "#     shuffle = True,\n",
    "#     stratify = targets[tmpIdx],\n",
    "#     random_state = seed\n",
    "# )\n",
    "\n",
    "# # float to int\n",
    "# trainIdx = trainIdx.astype(int)\n",
    "# valIdx = valIdx.astype(int)\n",
    "# testIdx = testIdx.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Exit from code, because we are in cluster or running locally. Training has finished.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Exit from code, because we are in cluster or running locally. Training has finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "sys.exit(\"Exit from code, because we are in cluster or running locally. Training has finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving ids\n",
    "# saveLightCurvesIdsBeforeBalancing(trainIdx, valIdx, testIdx, folder_path, lightCurvesIds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load ids dictionary\n",
    "# a_file = open(folder_path + \"/dataset_ids_before_balancing.pkl\", \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # analize classes distributino\n",
    "# fig, ax = plt.subplots(3, 1)\n",
    "\n",
    "# ax[0].hist(targets[trainIdx])\n",
    "# ax[1].hist(targets[valIdx])\n",
    "# ax[2].hist(targets[testIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spliting the data\n",
    "\n",
    "# # print(torch_dataset_lazy.__len__())\n",
    "\n",
    "totalSize = torch_dataset_lazy.__len__()\n",
    "\n",
    "# totalSize = totalSize\n",
    "# # print(totalSize)\n",
    "\n",
    "# selecting train splitting\n",
    "# train_size = int(0.8 * totalSize)\n",
    "train_size = trainIdx.shape[0]\n",
    "#print(train_size)\n",
    "\n",
    "# # getting test splitting\n",
    "# validation_size = math.floor((totalSize - train_size)/3)\n",
    "validation_size = valIdx.shape[0]\n",
    "# #print(validation_size)\n",
    "\n",
    "# # getting test splitting\n",
    "# test_size = totalSize - train_size - validation_size\n",
    "test_size = testIdx.shape[0]\n",
    "# #print(test_size)\n",
    "\n",
    "# # spliting the torch dataset\n",
    "# trainDataset, validationDataset,  testDataset = torch.utils.data.random_split(\n",
    "#     torch_dataset_lazy, \n",
    "#     [train_size, validation_size, test_size],\n",
    "    \n",
    "#     # set seed\n",
    "#     generator = torch.Generator().manual_seed(seed)\n",
    "# )\n",
    "\n",
    "print(\"train size:\", train_size)\n",
    "print(\"validation size: \", validation_size)\n",
    "print(\"test size:\", test_size)\n",
    "totTmp = train_size+ validation_size + test_size\n",
    "print(\"sum: \", totTmp)\n",
    "assert torch_dataset_lazy.__len__() == totTmp, \"dataset partition should be the same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"initila distribution\")\n",
    "# initialClassesDistribution = countClasses(trainDataset, only_these_labels)\n",
    "initialClassesDistribution = np.unique(targets, return_counts=True)[1]\n",
    "\n",
    "print(initialClassesDistribution)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.bar(x = np.arange(len(only_these_labels)), height = initialClassesDistribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data loader (minibatches)\n",
    "\n",
    "# training loader\n",
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    torch_dataset_lazy, \n",
    "    batch_size = batch_training_size, \n",
    "    # to balance classes\n",
    "    sampler=ImbalancedDatasetSampler(\n",
    "        torch_dataset_lazy, \n",
    "        indices = trainIdx,\n",
    "        seed = seed\n",
    "#         indices = [0, 1, 2]\n",
    "    ),\n",
    "#     sampler = torch.utils.data.SubsetRandomSampler(\n",
    "#         trainIdx,\n",
    "#         generator = torch.Generator().manual_seed(seed)\n",
    "#     ),\n",
    "    # each worker retrieve data from disk, so the data will be ready to be processed by main process. The main process should get the data from disk, so if workers > 0, the workers will get the data (not the main process)\n",
    "    num_workers = 4,\n",
    "    \n",
    "    # https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/\n",
    "    # the dataloader loads the data in pinned memory (instead of pageable memory), avoiding one process (to transfer data from pageable memory to pinned memory, work done by CUDA driver)\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "\n",
    "# validation loader\n",
    "validationLoader = torch.utils.data.DataLoader(\n",
    "#     validationDataset, \n",
    "    torch_dataset_lazy,\n",
    "    batch_size= batch_training_size,  \n",
    "    num_workers = 4,\n",
    "    pin_memory = True,\n",
    "    sampler = valIdx,\n",
    "#     sampler = torch.utils.data.SubsetRandomSampler(\n",
    "#         valIdx,\n",
    "#         generator = torch.Generator().manual_seed(seed)\n",
    "#     ),\n",
    "#     sampler=ImbalancedDatasetSampler(\n",
    "#         torch_dataset_lazy, \n",
    "#         indices = valIdx,\n",
    "#         seed = seed\n",
    "# #         indices = [0, 1, 2]\n",
    "#     ),\n",
    ")\n",
    "\n",
    "# # test loader\n",
    "# testLoader = torch.utils.data.DataLoader(testDataset)\n",
    "testLoader = torch.utils.data.DataLoader(\n",
    "#     validationDataset, \n",
    "    torch_dataset_lazy,\n",
    "#     batch_size= batch_training_size,  \n",
    "    num_workers = 4,\n",
    "    pin_memory = True,\n",
    "    sampler = testIdx,\n",
    "#     sampler = torch.utils.data.SubsetRandomSampler(\n",
    "#         testIdx,\n",
    "#         generator = torch.Generator().manual_seed(seed)\n",
    "#     ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced distribution\")\n",
    "balancedClassesDistribution = countClasses(trainLoader, only_these_labels)\n",
    "\n",
    "print(balancedClassesDistribution)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.bar(x = np.ar# return 0# return 0ange(6), height = balancedClassesDistribution)\n",
    "# ax.bar(x = only_these_labels, height = temp2, width = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save ids of dataset to use (train, test and validation)\n",
    "saveLightCurvesIdsAfterBalancing(trainLoader, train_size, testLoader, test_size, validationLoader, validation_size, path = folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load ids dictionary\n",
    "# a_file = open(folder_path + \"/dataset_ids_after_balancing.pkl\", \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# print(output[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if includeOtherFeatures:\n",
    "    \n",
    "    # save features\n",
    "    trainOtherFeaturesArray = np.zeros(shape = (train_size, otherFeaturesDim))\n",
    "    validOtherFeaturesArray = np.zeros(shape = (validation_size, otherFeaturesDim))\n",
    "\n",
    "    print(\"starting to get the other features\")\n",
    "\n",
    "    trainLastIndex = 0\n",
    "    validLastIndex = 0\n",
    "    \n",
    "    for trainData_ in trainLoader:\n",
    "        \n",
    "        # get other features by batch\n",
    "        # [batch size, features]\n",
    "        trainOtherFeatures = getOtherFeatures(trainData_[0]).to(device = cuda_device)\n",
    "\n",
    "        # indexation\n",
    "        trainLastIndex_ = trainLastIndex + trainData_[0].shape[0]\n",
    "        \n",
    "        # save features in array indexing them\n",
    "        trainOtherFeaturesArray[trainLastIndex : trainLastIndex_] = trainOtherFeatures.cpu().numpy()\n",
    "            \n",
    "        # update indexs\n",
    "        trainLastIndex = trainLastIndex_\n",
    "    \n",
    "    # test size\n",
    "    assert trainLastIndex == train_size\n",
    "    \n",
    "    for validData_ in validationLoader:\n",
    "        \n",
    "        # get other features by batch\n",
    "        # [batch size, features]\n",
    "        validOtherFeatures = getOtherFeatures(validData_[0]).to(device = cuda_device)\n",
    "\n",
    "        # indexation\n",
    "        validLastIndex_ = validLastIndex + validData_[0].shape[0]\n",
    "        \n",
    "        # save features in array indexing them\n",
    "        validOtherFeaturesArray[validLastIndex : validLastIndex_] = validOtherFeatures.cpu().numpy()\n",
    "            \n",
    "        # update indexs\n",
    "        validLastIndex = validLastIndex_\n",
    "    \n",
    "    # test size\n",
    "    assert validLastIndex == validation_size\n",
    "    \n",
    "    print(\"finish to get other features\")\n",
    "    \n",
    "    print(\"normalize features\")\n",
    "    # normalize features\n",
    "    trainNormalizedFeatures = torch.from_numpy(normalizeOtherFeatures(trainOtherFeaturesArray)).type(torch.FloatTensor)\n",
    "    validNormalizedFeatures = torch.from_numpy(normalizeOtherFeatures(validOtherFeaturesArray)).type(torch.FloatTensor)\n",
    "    \n",
    "    # check nan values\n",
    "    print(f\"nan values train: {np.any(torch.isnan(trainNormalizedFeatures).cpu().numpy())}\")\n",
    "    print(f\"nan values valid: {np.any(torch.isnan(validNormalizedFeatures).cpu().numpy())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test shape\n",
    "# assert (trainNormalizedFeatures.shape == trainOtherFeaturesArray.shape)\n",
    "# assert (validNormalizedFeatures.shape == validOtherFeaturesArray.shape)\n",
    "# print(trainNormalizedFeatures.shape)\n",
    "# print(validNormalizedFeatures.shape)\n",
    "\n",
    "# # test man values on normalized\n",
    "# print(torch.mean(torch.from_numpy(trainOtherFeaturesArray), dim = 0))\n",
    "# print(torch.mean(trainNormalizedFeatures, dim = 0))\n",
    "\n",
    "# print(torch.mean(torch.from_numpy(validOtherFeaturesArray), dim = 0))\n",
    "# print(torch.mean(validNormalizedFeatures, dim = 0))\n",
    "\n",
    "# # test max values on normalized\n",
    "# print(torch.max(torch.from_numpy(trainOtherFeaturesArray), dim = 0))\n",
    "# print(torch.max(trainNormalizedFeatures, dim = 0))\n",
    "\n",
    "# print(torch.max(torch.from_numpy(validOtherFeaturesArray), dim = 0))\n",
    "# print(torch.max(validNormalizedFeatures, dim = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create experiment parameters file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store varibales on file\n",
    "if trainingOnGuanaco or trainWithJustPython:\n",
    "    text_file = open(\"../\" + expPath + \"/experimentParameters.txt\" , \"w\")\n",
    "    text = \"N° experiment: {7}\\n General comment: {13}\\n Classes: {0}\\n train_size: {9}\\n validation_size: {10}\\n test_size: {11}\\n total dataset size: {12}\\n Epochs: {8}\\n Latent dimension: {1}\\n Hidden dimension: {2}\\n Input dimension: {3}\\n Passband: {4}\\n Learning rate: {5}\\n Batch training size: {6}\\n initial train classes distribution: {14}\\nbalanced train class distribution: {15}\".format(only_these_labels, latentDim, hiddenDim, inputDim, passband, learning_rate, batch_training_size, number_experiment, epochs, train_size, validation_size, test_size, train_size + validation_size + test_size, comment, initialClassesDistribution, balancedClassesDistribution)\n",
    "    text_file.write(text)\n",
    "    text_file.close()\n",
    "    print(\"experiment parameters file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining parameters to Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of parameters\n",
    "# latentDim = 5\n",
    "# hiddenDim = 10\n",
    "# inputDim = 72\n",
    "\n",
    "latentDim = latentDim\n",
    "hiddenDim = hiddenDim\n",
    "inputDim = inputDim\n",
    "\n",
    "# passband = passband\n",
    "\n",
    "num_classes = len(only_these_labels)\n",
    "\n",
    "if trainWithPreviousModel:\n",
    "    \n",
    "    # loadgin model\n",
    "    model = torch.load(pathToSaveModel + \".txt\").to(device = cuda_device)\n",
    "    \n",
    "    print(\"loading saved model\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # defining model\n",
    "    model = EncoderClassifier(\n",
    "        latent_dim = latentDim, \n",
    "        hidden_dim = hiddenDim, \n",
    "        input_dim = inputDim, \n",
    "        num_classes = num_classes, \n",
    "        passband = passband, \n",
    "        includeDeltaErrors = includeDeltaErrors,\n",
    "        includeOtherFeatures = includeOtherFeatures,\n",
    "        otherFeaturesDim = otherFeaturesDim,\n",
    "    )\n",
    "\n",
    "    # mdel to GPU\n",
    "    model = model.to(device = cuda_device)\n",
    "    \n",
    "    print(\"creating model with default parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# optimizeraa\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# loss function\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss\n",
    "train_loss = np.zeros((epochs,))\n",
    "test_loss = np.zeros((epochs,))\n",
    "\n",
    "# f1 scores\n",
    "f1Scores = np.zeros((epochs, ))\n",
    "\n",
    "# min global test loss \n",
    "minTestLossGlobalSoFar = float(\"inf\")\n",
    "\n",
    "# # # loss plot\n",
    "# if it is not cluster\n",
    "if (not trainingOnGuanaco) or (not trainWithJustPython):\n",
    "    \n",
    "    # add f1 and loss plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (7, 3), tight_layout = True)\n",
    "    \n",
    "    # error\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[0].set_ylabel(\"Error\")\n",
    "    \n",
    "    \n",
    "    # f1 score\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].set_ylabel(\"F1 score\")\n",
    "    \n",
    "\n",
    "# early stopping\n",
    "count_early_stop = 0\n",
    "\n",
    "\n",
    "print(\"starting the training\")\n",
    "\n",
    "\n",
    "# epoch\n",
    "for nepoch in range(epochs):\n",
    "        \n",
    "    print(\"epoch:    {0} / {1}\".format(nepoch, epochs))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    ######## Train ###########\n",
    "    \n",
    "    \n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    # this is for getting the other features\n",
    "    trainLastIndex = 0\n",
    "    \n",
    "    for data_ in trainLoader:\n",
    "        \n",
    "        # get raw data and labels\n",
    "        labels = data_[1].to(device = cuda_device)\n",
    "        \n",
    "        # optimzer to zero\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # this returns deltas\n",
    "        data = generateDeltas(data_[0], passband, includeDeltaErrors).type(torch.FloatTensor).to(device = cuda_device)\n",
    "            \n",
    "        # add other features\n",
    "        # [batch size, features dim]\n",
    "        if includeOtherFeatures:\n",
    "            \n",
    "            # index to include batch data\n",
    "            trainLastIndex_ = trainLastIndex + data_[0].shape[0]\n",
    "\n",
    "            # get only the normalized data from the batch (by indexation)\n",
    "            otherFeatures = trainNormalizedFeatures[trainLastIndex : trainLastIndex_, :].to(device = cuda_device)\n",
    "            \n",
    "            # update index \n",
    "            trainLastIndex = trainLastIndex_\n",
    "            \n",
    "            # validate data\n",
    "            if np.any(torch.isnan(otherFeatures).cpu().numpy()):\n",
    "                \n",
    "                print(f\"other features with nan values in epoch {nepoch}\")\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors, otherFeatures)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors)\n",
    "\n",
    "        # validate nana values on output model\n",
    "        if np.any(torch.isnan(outputs).cpu().numpy()):\n",
    "                \n",
    "                print(f\"outpues with nan values in epoch {nepoch}\")\n",
    "                \n",
    "\n",
    "        # loss function\n",
    "        loss = lossFunction(outputs, mapLabels(labels, only_these_labels).to(device = cuda_device))\n",
    "            \n",
    "        # validate nan values on loss\n",
    "        if np.any(torch.isnan(loss).cpu().numpy()):\n",
    "                \n",
    "                print(f\"loss with nan values in epoch {nepoch}\")\n",
    "                \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add loss value (of the currrent minibatch)\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "    # get epoch loss value\n",
    "    train_loss[nepoch] = epoch_train_loss / train_size\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### Validation ########\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    epoch_test_loss = 0\n",
    "    \n",
    "    # check f1 score in each minibatch\n",
    "    f1Score = 0\n",
    "    \n",
    "    batchCounter = 0\n",
    "    \n",
    "    # this is to get other features\n",
    "    validLastIndex = 0\n",
    "    \n",
    "    # minibatches\n",
    "    for data_ in validationLoader:\n",
    "        \n",
    "        labels = data_[1].to(device = cuda_device)\n",
    "            \n",
    "        # get deltas\n",
    "        data = generateDeltas(data_[0], passband, includeDeltaErrors).type(torch.FloatTensor).to(device = cuda_device)\n",
    "    \n",
    "        \n",
    "        # inlcude other features\n",
    "        if includeOtherFeatures:\n",
    "            \n",
    "            validLastIndex_ = validLastIndex + data_[0].shape[0]\n",
    "\n",
    "            otherFeatures = validNormalizedFeatures[validLastIndex : validLastIndex_, :].to(device = cuda_device)\n",
    "        \n",
    "            validLastIndex = validLastIndex_\n",
    "            \n",
    "            # validate data\n",
    "            if np.any(torch.isnan(otherFeatures).cpu().numpy()):\n",
    "                \n",
    "                print(f\"other features with nan values in epoch {nepoch}\")\n",
    "                \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors, otherFeatures)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors)\n",
    "        \n",
    "        if np.any(torch.isnan(outputs).cpu().numpy()):\n",
    "                \n",
    "                print(f\"outputs with nan values in epoch {nepoch}\")\n",
    "                \n",
    "        # loss function\n",
    "        loss = lossFunction(outputs, mapLabels(labels, only_these_labels).to(device = cuda_device))\n",
    "        \n",
    "        #  store minibatch loss value\n",
    "        epoch_test_loss += loss.item()\n",
    "\n",
    "        # f1 score\n",
    "        f1Score += f1_score(\n",
    "            mapLabels(labels, only_these_labels).cpu().numpy(), \n",
    "            torch.argmax(outputs, 1).cpu().numpy(), \n",
    "            average = \"weighted\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # batch counter\n",
    "        batchCounter += 1\n",
    "    \n",
    "    \n",
    "    # get epoch test loss value\n",
    "    test_loss[nepoch] = epoch_test_loss / validation_size\n",
    "    \n",
    "    # get epoch f1 score\n",
    "    f1Scores[nepoch] = f1Score / batchCounter\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # plot values\n",
    "    \n",
    "    \n",
    "    # plot loss values\n",
    "    # if it's not cluster\n",
    "    if (not trainingOnGuanaco) or (not trainWithJustPython):\n",
    "\n",
    "        # loss values\n",
    "        ax[0].plot(train_loss[0: nepoch], label = \"train\", linewidth = 3, c = \"red\") \n",
    "        ax[0].plot(test_loss[0: nepoch], label = \"test\", linestyle = \"--\", linewidth = 3, c = \"green\")\n",
    "        \n",
    "        # f1 score values\n",
    "        ax[1].plot(f1Scores[0: nepoch], linewidth = 3, c = \"green\")\n",
    "        \n",
    "        # plot\n",
    "        fig.canvas.draw()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Saving best model ####\n",
    "    \n",
    "    # if epoch test loss is smaller than global min\n",
    "    if test_loss[nepoch] < minTestLossGlobalSoFar:\n",
    "        \n",
    "        # update global min\n",
    "        minTestLossGlobalSoFar = test_loss[nepoch]\n",
    "        \n",
    "        # save model\n",
    "        saveBestModel(model, pathToSaveModel, number_experiment, nepoch, minTestLossGlobalSoFar, expPath)\n",
    "                \n",
    "   \n",
    "\n",
    "\n",
    "    #### save losses ####\n",
    "    print(\"saving losses\")\n",
    "    losses = np.asarray([train_loss, test_loss]).T\n",
    "    np.savetxt(\"../\" + expPath + \"/training_losses.csv\", losses, delimiter=\",\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ### save f1 scores ####\n",
    "    print(\"saving f1 scores\")\n",
    "    np.savetxt(\"../\" + expPath + \"/f1Scores.csv\", f1Scores, delimiter=\",\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Early stopping #####\n",
    "    # If minimum global validation error does not decrease in X epochs, so stop training\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if new test loss is greater than the min valid error\n",
    "    if test_loss[nepoch] > minTestLossGlobalSoFar:\n",
    "        count_early_stop += 1\n",
    "        print(\"early stopping counter: \", count_early_stop)\n",
    "        \n",
    "    # if it is smaller\n",
    "    else: \n",
    "        count_early_stop = 0\n",
    "    \n",
    "    # analyze early stopping\n",
    "    if count_early_stop >= threshold_early_stop:\n",
    "        \n",
    "        print(\"Early stopping in epoch: \", nepoch)\n",
    "        text_file = open(\"../\" + expPath + \"/earlyStopping.txt\", \"w\")\n",
    "        metricsText = \"Epoch: {0}\\n ES counter: {1}\\n\".format(nepoch, count_early_stop)\n",
    "        text_file.write(metricsText)\n",
    "        text_file.close()\n",
    "        break\n",
    "        \n",
    "    \n",
    "    \n",
    "# final message\n",
    "print(\"training has finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # get metrics on trainig dataset\n",
    "\n",
    "# if not includeOtherFeatures:\n",
    "#     trainNormalizedFeatures = None\n",
    "#     validNormalizedFeatures = None\n",
    "    \n",
    "# getConfusionAndClassificationReport(\n",
    "#     trainLoader, \n",
    "#     nameLabel = \"Train\", \n",
    "#     passband = passband, \n",
    "#     model = model, \n",
    "#     staticLabels = only_these_labels, \n",
    "#     number_experiment = number_experiment, \n",
    "#     expPath = expPath, \n",
    "#     includeDeltaErrors = includeDeltaErrors, \n",
    "#     includeOtherFeatures = includeOtherFeatures,\n",
    "#     normalizedFeatures = trainNormalizedFeatures,\n",
    "# )\n",
    "\n",
    "\n",
    "# # get metrics on validation dataset\n",
    "# getConfusionAndClassificationReport(\n",
    "#     validationLoader, \n",
    "#     nameLabel = \"Validation\", \n",
    "#     passband = passband, \n",
    "#     model = model, \n",
    "#     staticLabels = only_these_labels, \n",
    "#     number_experiment = number_experiment, \n",
    "#     expPath = expPath, \n",
    "#     includeDeltaErrors = includeDeltaErrors, \n",
    "#     includeOtherFeatures = includeOtherFeatures,\n",
    "#     normalizedFeatures = validNormalizedFeatures,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get own model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create new dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data loader (minibatches)\n",
    "\n",
    "# training loader\n",
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    torch_dataset_lazy, \n",
    "#     batch_size = batch_training_size, \n",
    "    # to balance classes\n",
    "    sampler=ImbalancedDatasetSampler(\n",
    "        torch_dataset_lazy, \n",
    "        indices = trainIdx,\n",
    "        seed = seed\n",
    "#         indices = [0, 1, 2]\n",
    "    ),\n",
    "#     sampler = torch.utils.data.SubsetRandomSampler(\n",
    "#         trainIdx,\n",
    "#         generator = torch.Generator().manual_seed(seed)\n",
    "#     ),\n",
    "    # each worker retrieve data from disk, so the data will be ready to be processed by main process. The main process should get the data from disk, so if workers > 0, the workers will get the data (not the main process)\n",
    "    num_workers = 4,\n",
    "    \n",
    "    # https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/\n",
    "    # the dataloader loads the data in pinned memory (instead of pageable memory), avoiding one process (to transfer data from pageable memory to pinned memory, work done by CUDA driver)\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "\n",
    "# validation loader\n",
    "validationLoader = torch.utils.data.DataLoader(\n",
    "#     validationDataset, \n",
    "    torch_dataset_lazy,\n",
    "#     batch_size= batch_training_size,  \n",
    "    num_workers = 4,\n",
    "    pin_memory = True,\n",
    "    sampler = valIdx,\n",
    "#     sampler = torch.utils.data.SubsetRandomSampler(\n",
    "#         valIdx,\n",
    "#         generator = torch.Generator().manual_seed(seed)\n",
    "#     ),\n",
    "#     sampler=ImbalancedDatasetSampler(\n",
    "#         torch_dataset_lazy, \n",
    "#         indices = valIdx,\n",
    "#         seed = seed\n",
    "# #         indices = [0, 1, 2]\n",
    "#     ),\n",
    ")\n",
    "\n",
    "# # test loader\n",
    "# testLoader = torch.utils.data.DataLoader(testDataset)\n",
    "testLoader = torch.utils.data.DataLoader(\n",
    "#     validationDataset, \n",
    "    torch_dataset_lazy,\n",
    "#     batch_size= batch_training_size,  \n",
    "    num_workers = 4,\n",
    "    pin_memory = True,\n",
    "    sampler = testIdx,\n",
    "#     sampler = torch.utils.data.SubsetRandomSampler(\n",
    "#         testIdx,\n",
    "#         generator = torch.Generator().manual_seed(seed)\n",
    "#     ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadgin model\n",
    "model = torch.load(pathToSaveModel + \".txt\").to(device = cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class predictions\n",
    "trainModelPredictions = np.zeros(shape = (train_size,))\n",
    "\n",
    "# lc ids\n",
    "trainIds = np.zeros(shape = (train_size,))\n",
    "\n",
    "# test labels\n",
    "trainLabels = np.zeros(shape = (train_size,))\n",
    "\n",
    "print(\"getting predictions on train\")\n",
    "\n",
    "# index = 0\n",
    "    \n",
    "# this is for getting the other features\n",
    "trainLastIndex = 0\n",
    "    \n",
    "# iterate on test dataset\n",
    "# for data_ in trainLoader:\n",
    "for idx, data_ in enumerate(trainLoader):\n",
    "        \n",
    "#         # index to include batch data\n",
    "#         index_ = index + data_[0].shape[0]\n",
    "\n",
    "        data = data_[0]\n",
    "\n",
    "        # this take the deltas (time and magnitude)\n",
    "        data = generateDeltas(data, passband, includeDeltaErrors).type(torch.FloatTensor).to(device = cuda_device)\n",
    "            \n",
    "#         # get model output\n",
    "#         outputs = model.forward(data, includeDeltaErrors)\n",
    "        \n",
    "        # add other features\n",
    "        # [batch size, features dim]\n",
    "        if includeOtherFeatures:\n",
    "            \n",
    "            # index to include batch data\n",
    "            trainLastIndex_ = trainLastIndex + data_[0].shape[0]\n",
    "\n",
    "            # get only the normalized data from the batch (by indexation)\n",
    "            otherFeatures = trainNormalizedFeatures[trainLastIndex : trainLastIndex_, :].to(device = cuda_device)\n",
    "            \n",
    "            # update index \n",
    "            trainLastIndex = trainLastIndex_\n",
    "            \n",
    "            # validate data\n",
    "            if np.any(torch.isnan(otherFeatures).cpu().numpy()):\n",
    "                \n",
    "                print(f\"other features with nan values in epoch {nepoch}\")\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors, otherFeatures)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors)\n",
    "        \n",
    "#         print(trainLastIndex_)\n",
    "        \n",
    "#         # get model predictions\n",
    "#         trainModelPredictions[index : index_] = only_these_labels[torch.argmax(outputs, 1).cpu().numpy()[0]]\n",
    "        \n",
    "#         # get lc ids\n",
    "#         trainIds[index : index_] = data_[2]\n",
    "        \n",
    "#         # save labels\n",
    "#         trainLabels[index : index_] = data_[1]\n",
    "        \n",
    "#         # update index \n",
    "#         index = index_\n",
    "        \n",
    "        # get model predictions\n",
    "        trainModelPredictions[idx] = only_these_labels[torch.argmax(outputs, 1).cpu().numpy()[0]]\n",
    "        \n",
    "        # get lc ids\n",
    "        trainIds[idx] = data_[2]\n",
    "        \n",
    "        # save labels\n",
    "        trainLabels[idx] = data_[1]\n",
    "        \n",
    "        \n",
    "print(\"predictions ready\")# Get own model predictions\n",
    "\n",
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class predictions\n",
    "validModelPredictions = np.zeros(shape = (validation_size,))\n",
    "\n",
    "# lc ids\n",
    "validIds = np.zeros(shape = (validation_size,))\n",
    "\n",
    "# test labels\n",
    "validLabels = np.zeros(shape = (validation_size,))\n",
    "\n",
    "print(\"getting predictions on validtion\")\n",
    "\n",
    "# index = 0\n",
    "trainLastIndex = 0\n",
    "\n",
    "# iterate on test dataset\n",
    "# for data_ in (validationLoader):\n",
    "for idx, data_ in enumerate(validationLoader):\n",
    "    \n",
    "#         # index to include batch data\n",
    "#         index_ = index + data_[0].shape[0]\n",
    "        \n",
    "        data = data_[0]\n",
    "\n",
    "        # this take the deltas (time and magnitude)\n",
    "        data = generateDeltas(data, passband, includeDeltaErrors).type(torch.FloatTensor).to(device = cuda_device)\n",
    "            \n",
    "#         # get model output\n",
    "#         outputs = model.forward(data, includeDeltaErrors)\n",
    "        \n",
    "        # add other features\n",
    "        # [batch size, features dim]\n",
    "        if includeOtherFeatures:\n",
    "            \n",
    "            # index to include batch data\n",
    "            trainLastIndex_ = trainLastIndex + data_[0].shape[0]\n",
    "\n",
    "            # get only the normalized data from the batch (by indexation)\n",
    "            otherFeatures = validNormalizedFeatures[trainLastIndex : trainLastIndex_, :].to(device = cuda_device)\n",
    "            \n",
    "            # update index \n",
    "            trainLastIndex = trainLastIndex_\n",
    "            \n",
    "            # validate data\n",
    "            if np.any(torch.isnan(otherFeatures).cpu().numpy()):\n",
    "                \n",
    "                print(f\"other features with nan values in epoch {nepoch}\")\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors, otherFeatures)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors)\n",
    "            \n",
    "#         # get model predictions\n",
    "#         validModelPredictions[index : index_] = only_these_labels[torch.argmax(outputs, 1).cpu().numpy()[0]]\n",
    "        \n",
    "#         # get lc ids\n",
    "#         validIds[index : index_] = data_[2]\n",
    "        \n",
    "#         # save labels\n",
    "#         validLabels[index : index_] = data_[1]\n",
    "        \n",
    "#         # update index \n",
    "#         index = index_\n",
    "\n",
    "        # get model predictions\n",
    "        validModelPredictions[idx] = only_these_labels[torch.argmax(outputs, 1).cpu().numpy()[0]]\n",
    "        \n",
    "        # get lc ids\n",
    "        validIds[idx] = data_[2]\n",
    "        \n",
    "        # save labels\n",
    "        validLabels[idx] = data_[1]\n",
    "        \n",
    "        \n",
    "print(\"predictions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class predictions\n",
    "testModelPredictions = np.zeros(shape = (test_size,))\n",
    "\n",
    "# lc ids\n",
    "testIds = np.zeros(shape = (test_size,))\n",
    "\n",
    "# test labels\n",
    "testLabels = np.zeros(shape = (test_size,))\n",
    "\n",
    "print(\"getting predictions on test\")\n",
    "\n",
    "trainLastIndex = 0\n",
    "\n",
    "# iterate on test dataset\n",
    "for idx, data_ in enumerate(testLoader):\n",
    "        \n",
    "        data = data_[0]\n",
    "\n",
    "        # this take the deltas (time and magnitude)\n",
    "        data = generateDeltas(data, passband, includeDeltaErrors).type(torch.FloatTensor).to(device = cuda_device)\n",
    "            \n",
    "#         # get model output\n",
    "#         outputs = model.forward(data, includeDeltaErrors)\n",
    "        \n",
    "        # add other features\n",
    "        # [batch size, features dim]\n",
    "        if includeOtherFeatures:\n",
    "            \n",
    "            # index to include batch data\n",
    "            trainLastIndex_ = trainLastIndex + data_[0].shape[0]\n",
    "\n",
    "            # get only the normalized data from the batch (by indexation)\n",
    "            otherFeatures = testNormalizedFeatures[trainLastIndex : trainLastIndex_, :].to(device = cuda_device)\n",
    "            \n",
    "            # update index \n",
    "            trainLastIndex = trainLastIndex_\n",
    "            \n",
    "            # validate data\n",
    "            if np.any(torch.isnan(otherFeatures).cpu().numpy()):\n",
    "                \n",
    "                print(f\"other features with nan values in epoch {nepoch}\")\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors, otherFeatures)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors)\n",
    "            \n",
    "        # get model predictions\n",
    "        testModelPredictions[idx] = only_these_labels[torch.argmax(outputs, 1).cpu().numpy()[0]]\n",
    "        \n",
    "        # get lc ids\n",
    "        testIds[idx] = data_[2]\n",
    "        \n",
    "        # save labels\n",
    "        testLabels[idx] = data_[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "print(\"predictions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "results = {\n",
    "    \n",
    "    # train\n",
    "    \"trainIds\": trainIds,\n",
    "    \"trainLabels\": trainLabels,\n",
    "    \"trainPredictions\": trainModelPredictions,\n",
    "    \n",
    "     # validation\n",
    "    \"validIds\": validIds,\n",
    "    \"validLabels\": validLabels,\n",
    "    \"validPredictions\": validModelPredictions,\n",
    "    \n",
    "    \n",
    "    # test\n",
    "    \"testIds\": testIds,\n",
    "    \"testLabels\": testLabels,\n",
    "    \"testPredictions\": testModelPredictions,\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# save object\n",
    "\n",
    "if trainingOnGuanaco or trainWithJustPython:\n",
    "\n",
    "    a_file = open(\"../experiments/comparingModels/seed\" + str(seed) + \"/ownModel/OwnModel\" + number_experiment + \"Predictions.pkl\", \"wb\")\n",
    "    pickle.dump(results, a_file)\n",
    "    a_file.close()\n",
    "\n",
    "    print(\"model predictions saved on a file\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(\"not save metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop execution if it's on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if  trainingOnGuanaco or trainWithJustPython:\n",
    "\n",
    "    sys.exit(\"Exit from code, because we are in cluster or running locally. Training has finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
