{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "This notebook is to train the encoder as a classifier with the idea of validate the encoder architecture first and then use this to train the VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on guanaco\n",
    "# ATENTION: if it is going to run on guanaco:\n",
    "# 1) comment the %matplotlib magic in next block and any magic (something like %code)\n",
    "# 2) Change to True the trainingOnGuanaco vairbale\n",
    "# 3) set epoch with an appropiate number\n",
    "# 4) add comment to experiemnts\n",
    "# 5) Add this file as python file \n",
    "# 6) Change launchJobOnGuanaco file to run this file but with python format\n",
    "trainingOnGuanaco = True\n",
    "\n",
    "# train without notebook\n",
    "trainWithJustPython = False\n",
    "\n",
    "# seed to generate same datasets\n",
    "seed = 0\n",
    "\n",
    "# number_experiment (this is just a name)\n",
    "# priors:\n",
    "# 1\n",
    "number_experiment = 99\n",
    "number_experiment = str(number_experiment)\n",
    "\n",
    "# training\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes to analyze\n",
    "# 42,  90,  16,  67,  62, 993,  92,  52,  88,  65, 991, 992,  15,\n",
    "#        95,   6,  53, 994,  64\n",
    "\n",
    "# periodic\n",
    "# only_these_labels = [16, 92, 53]\n",
    "\n",
    "# periodic + variable\n",
    "only_these_labels = [16, 92, 53, 88, 65, 6]\n",
    "# 53 has 24 light curves\n",
    "\n",
    "# only_these_labels = [16, 92]\n",
    "# only_these_labels = [16, 92]\n",
    "# only_these_labels = [42,  90,  16,  67,  62, 993,  92,  52,  88,  65, 991, 992,  15,\n",
    "#         95,   6,  53, 994,  64]\n",
    "\n",
    "# VAE parameters\n",
    "latentDim = 100\n",
    "hiddenDim = 100\n",
    "inputDim = 72\n",
    "\n",
    "# band\n",
    "# passband = 5\n",
    "passband = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "batch_training_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder as clasifier with periodic + variable + class balancing + 1 conv layer more + 6 channels + seed 0\n"
     ]
    }
   ],
   "source": [
    "# add general comment about experiment \n",
    "# comment = \"encoder as clasifier with periodic + variable (with class balancing) + 1 conv layer more\"\n",
    "comment = \"encoder as clasifier with periodic + variable + class balancing + 1 conv layer more + \" + str(len(passband)) + \" channels + seed \" + str(seed)\n",
    "\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# from tqdm import tqdm_notebook\n",
    "\n",
    "# %matplotlib notebook\n",
    "\n",
    "# import functions to load dataset\n",
    "import sys\n",
    "sys.path.append(\"./codesToDatasets\")\n",
    "from plasticc_dataset_torch import get_plasticc_datasets\n",
    "# from plasticc_plotting import plot_light_curve\n",
    "\n",
    "import math\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "# local imports\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "sys.path.append('../models')\n",
    "# from classifier import EncoderClassifier, \n",
    "from classifierPrototype import EncoderClassifier\n",
    "\n",
    "sys.path.append(\"./aux/\")\n",
    "from auxFunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to dataset\n",
    "pathToFile = \"/home/shared/astro/PLAsTiCC/\" if trainingOnGuanaco else \"/home/leo/Downloads/plasticc_torch-master/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset with pytorch tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have selected lazy loading. Light curves will be loaded ondemand from the harddrive\n",
      "Found 2 csv files at given path\n",
      "Loading /home/leo/Downloads/plasticc_torch-master/plasticc_train_lightcurves.csv\n",
      "samples added of class 92: 239\n",
      "max number of samples per class for class. The samples will be 239\n"
     ]
    }
   ],
   "source": [
    "# torch_dataset_lazy = get_plasticc_datasets(pathToFile)\n",
    "\n",
    "# Light curves are tensors are now [bands, [mjd, flux, err, mask],\n",
    "# lc_data, lc_label, lc_plasticc_id                              \n",
    "torch_dataset_lazy = get_plasticc_datasets(pathToFile, only_these_labels=only_these_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset test ok\n"
     ]
    }
   ],
   "source": [
    "assert torch_dataset_lazy.__len__() != 494096, \"dataset should be smaller\"\n",
    "print(\"dataset test ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting data (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2695\n",
      "train size: 2156\n",
      "validation size:  179\n",
      "test size: 360\n",
      "sum:  2695\n"
     ]
    }
   ],
   "source": [
    "# Spliting the data\n",
    "\n",
    "# print(torch_dataset_lazy.__len__())\n",
    "\n",
    "totalSize = torch_dataset_lazy.__len__()\n",
    "\n",
    "totalSize = totalSize\n",
    "# print(totalSize)\n",
    "\n",
    "# selecting train splitting\n",
    "train_size = int(0.8 * totalSize)\n",
    "#print(train_size)\n",
    "\n",
    "# getting test splitting\n",
    "validation_size = math.floor((totalSize - train_size)/3)\n",
    "#print(validation_size)\n",
    "\n",
    "# getting test splitting\n",
    "test_size = totalSize - train_size - validation_size\n",
    "#print(test_size)\n",
    "\n",
    "# spliting the torch dataset\n",
    "trainDataset, validationDataset,  testDataset = torch.utils.data.random_split(\n",
    "    torch_dataset_lazy, \n",
    "    [train_size, validation_size, test_size],\n",
    "    \n",
    "    # set seed\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    ")\n",
    "\n",
    "print(\"train size:\", train_size)\n",
    "print(\"validation size: \", validation_size)\n",
    "print(\"test size:\", test_size)\n",
    "print(\"sum: \", train_size+ validation_size + test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initila distribution\n",
      "[735. 291.  30. 588. 850. 126.]\n"
     ]
    }
   ],
   "source": [
    "print(\"initila distribution\")\n",
    "initialClassesDistribution = countClasses(trainDataset, only_these_labels)\n",
    "\n",
    "print(initialClassesDistribution)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.bar(x = np.arange(len(only_these_labels)), height = initialClassesDistribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data loader (minibatches)\n",
    "\n",
    "# training loader\n",
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    trainDataset, \n",
    "    batch_size = batch_training_size, \n",
    "    # to balance classes\n",
    "    sampler=ImbalancedDatasetSampler(trainDataset),\n",
    ")\n",
    "\n",
    "# validation loader\n",
    "validationLoader = torch.utils.data.DataLoader(\n",
    "    validationDataset, \n",
    "    batch_size= batch_training_size,  \n",
    "    num_workers = 4\n",
    ")\n",
    "\n",
    "# # test loader\n",
    "testLoader = torch.utils.data.DataLoader(testDataset)\n",
    "# trainLoader = torch.utils.data.DataLoader(torch_dataset_lazy, batch_size=256, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced distribution\n",
      "[441. 442. 446. 433. 445. 413.]\n"
     ]
    }
   ],
   "source": [
    "print(\"balanced distribution\")\n",
    "balancedClassesDistribution = countClasses(trainLoader, only_these_labels)\n",
    "\n",
    "print(balancedClassesDistribution)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.bar(x = np.arange(6), height = balancedClassesDistribution)\n",
    "# ax.bar(x = only_these_labels, height = temp2, width = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the path to save model while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory /home/leo/Desktop/thesis/work/thesis/experiments/99/seed0 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# create experiment's folder\n",
    "tmpGuanaco = \"/home/lbravo/thesis/work/thesis/\"\n",
    "tmpLocal = \"/home/leo/Desktop/thesis/work/thesis/\"\n",
    "\n",
    "expPath = \"experiments/\" + number_experiment + \"/seed\" + str(seed)\n",
    "\n",
    "folder_path = (tmpGuanaco + expPath) if trainingOnGuanaco else (tmpLocal + expPath)\n",
    "# !mkdir folder_path\n",
    "# os.makedirs(os.path.dirname(folder_path), exist_ok=True)\n",
    "\n",
    "# check if folder exists\n",
    "if not(os.path.isdir(folder_path)):\n",
    "        \n",
    "    # create folder\n",
    "    try:\n",
    "        os.makedirs(folder_path)\n",
    "        \n",
    "    except OSError as error:\n",
    "        print (\"Creation of the directory %s failed\" % folder_path)\n",
    "        print(error)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % folder_path)\n",
    "else:\n",
    "    print(\"folder already exists\")\n",
    "\n",
    "# define paht to save model while training\n",
    "pathToSaveModel = (tmpGuanaco + expPath + \"/model\") if trainingOnGuanaco else (tmpLocal + expPath + \"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment parameters file created\n"
     ]
    }
   ],
   "source": [
    "# store varibales on file\n",
    "text_file = open(\"../\" + expPath + \"/experimentParameters.txt\" , \"w\")\n",
    "text = \"N° experiment: {7}\\n General comment: {13}\\n Classes: {0}\\n train_size: {9}\\n validation_size: {10}\\n test_size: {11}\\n total dataset size: {12}\\n Epochs: {8}\\n Latent dimension: {1}\\n Hidden dimension: {2}\\n Input dimension: {3}\\n Passband: {4}\\n Learning rate: {5}\\n Batch training size: {6}\\n initial train classes distribution: {14}\\nbalanced train class distribution: {15}\".format(only_these_labels, latentDim, hiddenDim, inputDim, passband, learning_rate, batch_training_size, number_experiment, epochs, train_size, validation_size, test_size, train_size + validation_size + test_size, comment, initialClassesDistribution, balancedClassesDistribution)\n",
    "text_file.write(text)\n",
    "text_file.close()\n",
    "print(\"experiment parameters file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining parameters to Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of parameters\n",
    "# latentDim = 5\n",
    "# hiddenDim = 10\n",
    "# inputDim = 72\n",
    "\n",
    "latentDim = latentDim\n",
    "hiddenDim = hiddenDim\n",
    "inputDim = inputDim\n",
    "\n",
    "# passband = passband\n",
    "\n",
    "num_classes = len(only_these_labels)\n",
    "\n",
    "\n",
    "# defining model\n",
    "model = EncoderClassifier(latent_dim = latentDim, hidden_dim = hiddenDim, input_dim = inputDim, num_classes = num_classes, passband = passband)\n",
    "\n",
    "# mdel to GPU\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderClassifier(\n",
      "  (pconv1): PartialConv(\n",
      "    (input_conv): Conv1d(6, 64, kernel_size=(3,), stride=(2,))\n",
      "    (mask_conv): Conv1d(6, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
      "  )\n",
      "  (pconv2): PartialConv(\n",
      "    (input_conv): Conv1d(64, 32, kernel_size=(3,), stride=(2,))\n",
      "    (mask_conv): Conv1d(64, 32, kernel_size=(3,), stride=(2,), bias=False)\n",
      "  )\n",
      "  (pconv3): PartialConv(\n",
      "    (input_conv): Conv1d(32, 32, kernel_size=(3,), stride=(2,))\n",
      "    (mask_conv): Conv1d(32, 32, kernel_size=(3,), stride=(2,), bias=False)\n",
      "  )\n",
      "  (hidden1): Linear(in_features=768, out_features=100, bias=True)\n",
      "  (outputLayer): Linear(in_features=100, out_features=6, bias=True)\n",
      "  (activationConv): ReLU()\n",
      "  (activationLinear): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the training\n",
      "epoch:    0 / 3\n",
      "early stopping counter:  2\n",
      "New min test loss. Saving model\n",
      "saving losses\n",
      "saving f1 scores\n",
      "epoch:    1 / 3\n",
      "New min test loss. Saving model\n",
      "saving losses\n",
      "saving f1 scores\n",
      "epoch:    2 / 3\n",
      "New min test loss. Saving model\n",
      "saving losses\n",
      "saving f1 scores\n",
      "training has finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# optimizera\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.5)\n",
    "\n",
    "# loss function\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss\n",
    "train_loss = np.zeros((epochs,))\n",
    "test_loss = np.zeros((epochs,))\n",
    "\n",
    "# f1 scores\n",
    "f1Scores = np.zeros((epochs, ))\n",
    "\n",
    "# min global test loss \n",
    "minTestLossGlobalSoFar = float(\"inf\")\n",
    "\n",
    "# # # loss plot\n",
    "# if it is not cluster\n",
    "if (not trainingOnGuanaco) or (not trainWithJustPython):\n",
    "    \n",
    "    # add f1 and loss plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (7, 3), tight_layout = True)\n",
    "    # # fig, ax = plt.subplots()\n",
    "    \n",
    "    # error\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[0].set_ylabel(\"Error\")\n",
    "    \n",
    "    \n",
    "    # f1 score\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].set_ylabel(\"F1 score\")\n",
    "    \n",
    "\n",
    "# early stopping\n",
    "prior_test_error = 0\n",
    "count_early_stop = 0\n",
    "threshold_early_stop = 20\n",
    "\n",
    "\n",
    "print(\"starting the training\")\n",
    "\n",
    "\n",
    "# epoch\n",
    "for nepoch in range(epochs):\n",
    "        \n",
    "    print(\"epoch:    {0} / {1}\".format(nepoch, epochs))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    ######## Train ###########\n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    for data_ in trainLoader:\n",
    "        \n",
    "        data = data_[0]\n",
    "        labels = data_[1].cuda()\n",
    "#         labels = data_[1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # this take the deltas (time and magnitude)\n",
    "        data = generateDeltas(data, passband).type(torch.FloatTensor).cuda()\n",
    "#         data = generateDeltas(data, passband).type(torch.FloatTensor)\n",
    "\n",
    "#         # testing tensor size \n",
    "#         assert data.shape == torch.Size([batch_training_size, len(passband), 4, 71]), \"Shape should be [minibatch size, channels, 4, 71]\"\n",
    "#         print(\"test ok\")\n",
    "        \n",
    "        # get model output\n",
    "        outputs = model.forward(data)\n",
    "        \n",
    "#         # testing output shape size\n",
    "#         assert outputs.shape == torch.Size([batch_training_size, len(only_these_labels)]), \"Shape should be [minibatch, classes]\"\n",
    "#         print(\"test ok\")\n",
    "\n",
    "        # loss function\n",
    "        loss = lossFunction(outputs, mapLabels(labels, only_these_labels).cuda())\n",
    "        \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add loss value (of the currrent minibatch)\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "\n",
    "    # get epoch loss value\n",
    "    train_loss[nepoch] = epoch_train_loss / train_size\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### Validation ########\n",
    "    \n",
    "    epoch_test_loss = 0\n",
    "    \n",
    "    # check f1 score in each minibatch\n",
    "    f1Score = 0\n",
    "    \n",
    "    batchCounter = 0\n",
    "    \n",
    "    # minibatches\n",
    "    for data_ in validationLoader:\n",
    "        \n",
    "        data = data_[0]\n",
    "        labels = data_[1].cuda()\n",
    "        \n",
    "        data = generateDeltas(data, passband).type(torch.FloatTensor).cuda()\n",
    "        \n",
    "        outputs = model.forward(data)\n",
    "        \n",
    "#           # testing output shape size\n",
    "#         assert outputs.shape == torch.Size([batch_training_size, len(only_these_labels)]), \"Shape should be [minibatch, classes]\"\n",
    "#         print(\"test ok\")\n",
    "\n",
    "        # loss function\n",
    "        loss = lossFunction(outputs, mapLabels(labels, only_these_labels).cuda())\n",
    "    \n",
    "        #  store minibatch loss value\n",
    "        epoch_test_loss += loss.item()\n",
    "        \n",
    "        # f1 score\n",
    "        f1Score += f1_score(mapLabels(labels, only_these_labels).cpu().numpy(), torch.argmax(outputs, 1).cpu().numpy(), average = \"micro\")\n",
    "        \n",
    "        # batch counter\n",
    "        batchCounter += 1\n",
    "    \n",
    "    # get epoch test loss value\n",
    "    test_loss[nepoch] = epoch_test_loss / validation_size\n",
    "    \n",
    "    # get epoch f1 score\n",
    "    f1Scores[nepoch] = f1Score / batchCounter\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # plot loss values\n",
    "    # if it's not cluster\n",
    "    if (not trainingOnGuanaco) or (not trainWithJustPython):\n",
    "\n",
    "        # loss values\n",
    "        ax[0].plot(train_loss[0: nepoch], label = \"train\", linewidth = 3, c = \"red\") \n",
    "        ax[0].plot(test_loss[0: nepoch], label = \"test\", linestyle = \"--\", linewidth = 3, c = \"green\")\n",
    "        \n",
    "        # f1 score values\n",
    "        ax[1].plot(f1Scores[0: nepoch], linewidth = 3, c = \"green\")\n",
    "        \n",
    "        # plot\n",
    "        fig.canvas.draw()\n",
    "    \n",
    "    \n",
    "    #### Early stopping #####\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if new test loss is greater than the older one\n",
    "    count_early_stop += 1\n",
    "    if epoch_test_loss > prior_test_error:\n",
    "        count_early_stop += 1\n",
    "        print(\"early stopping counter: \", count_early_stop)\n",
    "    else: \n",
    "        count_early_stop = 0\n",
    "    \n",
    "    # update prior test error\n",
    "    prior_test_error = epoch_test_loss\n",
    "    \n",
    "    # analyze early stopping\n",
    "    if count_early_stop > threshold_early_stop:\n",
    "        \n",
    "        print(\"Early stopping in epoch: \", nepoch)\n",
    "        text_file = open(\"../\" + expPath + \"/earlyStopping.txt\", \"w\")\n",
    "        metricsText = \"Epoch: {0}\\n ES counter: {1}\\n, Reconstruction test error: {2}\".format(nepoch, count_early_stop, epoch_test_loss)\n",
    "        text_file.write(metricsText)\n",
    "        text_file.close()\n",
    "        break\n",
    "        \n",
    "        \n",
    "        \n",
    "    #### Saving best model ####\n",
    "    \n",
    "    # if epoch test loss is smaller than global min\n",
    "    if test_loss[nepoch] < minTestLossGlobalSoFar:\n",
    "        \n",
    "        # update global min\n",
    "        minTestLossGlobalSoFar = test_loss[nepoch]\n",
    "        \n",
    "        # save model\n",
    "        saveBestModel(model, pathToSaveModel, number_experiment, nepoch, minTestLossGlobalSoFar, expPath)\n",
    "                \n",
    "   \n",
    "\n",
    "\n",
    "    # save losses\n",
    "    print(\"saving losses\")\n",
    "    losses = np.asarray([train_loss, test_loss]).T\n",
    "    np.savetxt(\"../\" + expPath + \"/training_losses.csv\", losses, delimiter=\",\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # save f1 scores\n",
    "    print(\"saving f1 scores\")\n",
    "    np.savetxt(\"../\" + expPath + \"/f1Scores.csv\", f1Scores, delimiter=\",\")\n",
    "\n",
    "    \n",
    "    \n",
    "# final message\n",
    "print(\"training has finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving confusion matrix scores\n",
      "saving clasification report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving confusion matrix scores\n",
      "saving clasification report\n"
     ]
    }
   ],
   "source": [
    "# get metrics on trainig dataset\n",
    "getConfusionAndClassificationReport(trainLoader, nameLabel = \"Train\", passband = passband, model = model, staticLabels = only_these_labels, number_experiment = number_experiment, expPath = expPath)\n",
    "\n",
    "\n",
    "# get metrics on validation dataset\n",
    "getConfusionAndClassificationReport(validationLoader, nameLabel = \"Validation\", passband = passband, model = model, staticLabels = only_these_labels, number_experiment = number_experiment, expPath = expPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop execution if it's on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Exit from code, because we are in cluster or running locally. Training has finished.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Exit from code, because we are in cluster or running locally. Training has finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "if  trainingOnGuanaco or trainWithJustPython:\n",
    "\n",
    "    sys.exit(\"Exit from code, because we are in cluster or running locally. Training has finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../experiments/8/experimentParameters.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load losses array\n",
    "losses = pd.read_csv(\"/home/leo/Desktop/thesis/work/thesis/experiments/\"+ number_experiment + \"/training_losses.csv\")\n",
    "\n",
    "# f1 scores\n",
    "f1Scores = pd.read_csv(\"/home/leo/Desktop/thesis/work/thesis/experiments/\"+ number_experiment + \"/f1Scores.csv\")\n",
    "\n",
    "# plot losses\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10,4), tight_layout = True)\n",
    "\n",
    "# loss\n",
    "ax[0].set_xlabel(\"N° epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].plot(losses.iloc[:, 0], label = \"train\")\n",
    "ax[0].plot(losses.iloc[:, 1], label = \"validation\")\n",
    "ax[0].legend()\n",
    "\n",
    "# f1 scores\n",
    "ax[1].set_xlabel(\"N° epoch\")\n",
    "ax[1].set_ylabel(\"F1 score\")\n",
    "ax[1].plot(f1Scores)\n",
    "\n",
    "# best model\n",
    "# values copied from the txt file\n",
    "# bestModelEpoch = 785\n",
    "# bestModelError = 0.00434128265165168\n",
    "# ax[0].scatter(bestModelEpoch, bestModelError, c = \"r\", linewidths = 10)\n",
    "# ax[1].scatter(bestModelEpoch, f1Scores.iloc[bestModelEpoch], c = \"r\", linewidths = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../experiments/8/bestScoresModelTraining.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "# get confusion matrix\n",
    "cmTrain = pd.read_csv('../experiments/' + number_experiment + '/confusionMatrixTrain.csv', header = None) \n",
    "cmValidation = pd.read_csv('../experiments/' + number_experiment + '/confusionMatrixValidation.csv', header = None) \n",
    "\n",
    "print(\"Training\")\n",
    "sn.heatmap(cmTrain, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation\")\n",
    "sn.heatmap(cmValidation, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "!cat ../experiments/8/clasificationReportTrain.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "!cat ../experiments/8/clasificationReportValidation.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
