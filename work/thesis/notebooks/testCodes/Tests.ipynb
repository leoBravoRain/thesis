{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "domestic-replacement",
   "metadata": {},
   "source": [
    "# Test if features (mean and iq) are the spected values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.metrics import f1_score\n",
    "\n",
    "# optimizeraa\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.5)\n",
    "\n",
    "# loss function\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss\n",
    "train_loss = np.zeros((epochs,))\n",
    "test_loss = np.zeros((epochs,))\n",
    "\n",
    "# f1 scores\n",
    "f1Scores = np.zeros((epochs, ))\n",
    "\n",
    "# min global test loss \n",
    "minTestLossGlobalSoFar = float(\"inf\")\n",
    "\n",
    "# # # loss plot\n",
    "# if it is not cluster\n",
    "if (not trainingOnGuanaco) or (not trainWithJustPython):\n",
    "    \n",
    "    # add f1 and loss plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (7, 3), tight_layout = True)\n",
    "    # # fig, ax = plt.subplots()\n",
    "    \n",
    "    # error\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[0].set_ylabel(\"Error\")\n",
    "    \n",
    "    \n",
    "    # f1 score\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].set_ylabel(\"F1 score\")\n",
    "    \n",
    "\n",
    "# early stopping\n",
    "# prior_test_error = 0\n",
    "count_early_stop = 0\n",
    "threshold_early_stop = threshold_early_stop\n",
    "\n",
    "\n",
    "print(\"starting the training\")\n",
    "\n",
    "\n",
    "# epoch\n",
    "for nepoch in range(epochs):\n",
    "        \n",
    "    print(\"epoch:    {0} / {1}\".format(nepoch, epochs))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    ######## Train ###########\n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    for data_ in trainLoader:\n",
    "        \n",
    "        data = data_[0]\n",
    "        labels = data_[1].to(device = cuda_device)\n",
    "#         labels = data_[1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # this take the deltas (time and magnitude)\n",
    "        data = generateDeltas(data, passband, includeDeltaErrors).type(torch.FloatTensor).to(device = cuda_device)\n",
    "#         data = generateDeltas(data, passband).type(torch.FloatTensor)\n",
    "            \n",
    "        # add other features\n",
    "        # [batch size, features]\n",
    "#         if includeOtherFeatures:\n",
    "        if includeOtherFeatures:\n",
    "            \n",
    "            otherFeatures = getOtherFeatures(data_[0]).to(device = cuda_device)\n",
    "                \n",
    "#             print(np.any(torch.isnan(otherFeatures).cpu().numpy()))\n",
    "            \n",
    "            if np.any(torch.isnan(otherFeatures).cpu().numpy()):\n",
    "                \n",
    "                print(f\"other features with nan values in epoch {nepoch}\")\n",
    "            \n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors, otherFeatures)\n",
    "            \n",
    "            \n",
    "            # #         # testing tensor size \n",
    "# #         assert data.shape == torch.Size([batch_training_size, len(passband), 4, 71]), \"Shape should be [minibatch size, channels, 4, 71]\"\n",
    "# #         print(\"test ok\")\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # get model output\n",
    "            outputs = model.forward(data, includeDeltaErrors)\n",
    "\n",
    "        \n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing if means and iq are correct\n",
    "\n",
    "channels = [0, 1, 2, 3, 4, 5]\n",
    "indexs = np.random.choice(len(data_[2]), 2)\n",
    "\n",
    "# print(indexs)\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(indexs), \n",
    "    len(channels),\n",
    "    figsize = (12, 2*len(indexs)),\n",
    "    tight_layout = True,\n",
    ")\n",
    "\n",
    "for index in np.arange(len(indexs)):\n",
    "    \n",
    "    for channel in channels:\n",
    "    \n",
    "        mask = data_[0][indexs[index], channel, 3, :]\n",
    "\n",
    "        mask = mask.type(torch.BoolTensor)\n",
    "    \n",
    "#         print(f\"index: {indexs[index]}\")\n",
    "        \n",
    "        # print(\"data shape\")\n",
    "        # print(data_[0].shape)\n",
    "\n",
    "        # datashape: [ 128, 6, 4, 72 ]\n",
    "        ax[index][channel].scatter(\n",
    "            data_[0][indexs[index], channel, 0, mask], \n",
    "            data_[0][indexs[index], channel, 1, mask],\n",
    "        )\n",
    "\n",
    "        # print(\"values:\")\n",
    "        # print(otherFeatures[index, 0])\n",
    "\n",
    "\n",
    "        # manual mean\n",
    "        manualMean = torch.mean(data_[0][indexs[index], channel, 1, mask])\n",
    "\n",
    "#         ax[index][channel].hlines(\n",
    "#             manualMean, \n",
    "#             xmin = data_[0][indexs[index], channel, 0, mask][0], \n",
    "#             xmax = data_[0][indexs[index], channel, 0, mask][-1],\n",
    "#             color = \"r\"\n",
    "#         )\n",
    "        \n",
    "        # analyze features\n",
    "        ax[index][channel].hlines(\n",
    "            otherFeatures[indexs[index], channel].item(), \n",
    "            xmin = data_[0][indexs[index], channel, 0, mask][0], \n",
    "            xmax = data_[0][indexs[index], channel, 0, mask][-1],\n",
    "            color = \"r\"\n",
    "        )\n",
    "        \n",
    "        # test mean\n",
    "        assert manualMean.item() == otherFeatures[indexs[index], channel].item()\n",
    "        print(\"mean test ok\")\n",
    "\n",
    "        \n",
    "        iqManual =  torch.kthvalue(data_[0][indexs[index], channel, 1, mask], int(0.75*data_[0][indexs[index], channel, 1, mask].shape[0]))[0] - torch.kthvalue(data_[0][indexs[index], channel, 1, mask], int(0.25*data_[0][indexs[index], channel, 1, mask].shape[0]))[0]\n",
    "\n",
    "        assert iqManual.item() == otherFeatures[indexs[index], 6 + channel].item()\n",
    "        print(\"iq test ok\")\n",
    "        \n",
    "        \n",
    "#         print(manualMean.item())\n",
    "#         print(otherFeatures[indexs[index], channel].item())\n",
    "#         print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-harvey",
   "metadata": {},
   "source": [
    "# Test if samplers generate same data in different epochs\n",
    "This save ids of train and validation loader by epoch and then compare them (full arrays and by epoch).\n",
    "- If ids are equal by epoch, so taht means the dataloader (and the sampler in the background) is generating always the same dataset (ids and the same order).\n",
    "\n",
    "- If ids are not equals, that means that by epoch, the loader (and the sampler in the background) is generating different datasets (sampling generating different ids and orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "presidential-survival",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-33dd4fd48de1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnepochs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0midsTrain0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnepochs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0midsVal0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnepochs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "nepochs_ = 10\n",
    "\n",
    "idsTrain0 = np.zeros((nepochs_, train_size))\n",
    "idsVal0 = np.zeros((nepochs_, validation_size))\n",
    "\n",
    "# epoch\n",
    "for nepoch in range(nepochs_):\n",
    "        \n",
    "    print(\"epoch:    {0} / {1}\".format(nepoch, epochs))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    ######## Train ###########\n",
    "    \n",
    "    \n",
    "    \n",
    "    # this is for getting the other features\n",
    "    trainLastIndex = 0\n",
    "    \n",
    "    for idx, data_ in enumerate(trainLoader):\n",
    "        \n",
    "        trainLastIndex_ = trainLastIndex + data_[0].shape[0]\n",
    "        \n",
    "        idsTrain0[nepoch, trainLastIndex: trainLastIndex_] = data_[2]\n",
    "        \n",
    "#         print(trainLastIndex, trainLastIndex_)\n",
    "        \n",
    "        trainLastIndex = trainLastIndex_\n",
    "        \n",
    "     # this is for getting the other features\n",
    "    trainLastIndex = 0\n",
    "    \n",
    "    for idx, data_ in enumerate(validationLoader):\n",
    "        \n",
    "        trainLastIndex_ = trainLastIndex + data_[0].shape[0]\n",
    "        \n",
    "        idsVal0[nepoch, trainLastIndex: trainLastIndex_] = data_[2]\n",
    "        \n",
    "#         print(trainLastIndex, trainLastIndex_)\n",
    "        \n",
    "        trainLastIndex = trainLastIndex_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test  ids arrays are not equals\n",
    "assert not np.array_equal(idsTrain0, idsVal0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "nepochs_ = 10\n",
    "\n",
    "idsTrain = np.zeros((nepochs_, train_size))\n",
    "idsVal = np.zeros((nepochs_, validation_size))\n",
    "\n",
    "# epoch\n",
    "for nepoch in range(nepochs_):\n",
    "        \n",
    "    print(\"epoch:    {0} / {1}\".format(nepoch, epochs))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    ######## Train ###########\n",
    "    \n",
    "    \n",
    "    \n",
    "    # this is for getting the other features\n",
    "    trainLastIndex = 0\n",
    "    \n",
    "    for idx, data_ in enumerate(trainLoader):\n",
    "        \n",
    "        trainLastIndex_ = trainLastIndex + data_[0].shape[0]\n",
    "        \n",
    "        idsTrain[nepoch, trainLastIndex: trainLastIndex_] = data_[2]\n",
    "        \n",
    "#         print(trainLastIndex, trainLastIndex_)\n",
    "        \n",
    "        trainLastIndex = trainLastIndex_\n",
    "        \n",
    "     # this is for getting the other features\n",
    "    trainLastIndex = 0\n",
    "    \n",
    "    for idx, data_ in enumerate(validationLoader):\n",
    "        \n",
    "        trainLastIndex_ = trainLastIndex + data_[0].shape[0]\n",
    "        \n",
    "        idsVal[nepoch, trainLastIndex: trainLastIndex_] = data_[2]\n",
    "        \n",
    "#         print(trainLastIndex, trainLastIndex_)\n",
    "        \n",
    "        trainLastIndex = trainLastIndex_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test  ids arrays are equals\n",
    "assert np.array_equal(idsTrain0, idsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test  ids arrays are equals\n",
    "assert np.array_equal(idsVal0, idsVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test  ids arrays are not equals\n",
    "assert not np.array_equal(idsTrain, idsVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test ids are equals\n",
    "for i in np.arange(nepochs_):\n",
    "    \n",
    "    # test train ids\n",
    "    assert (np.array_equal(idsTrain[0, :], idsTrain[i, :]))\n",
    "    \n",
    "    # test validation ids\n",
    "    assert (np.array_equal(idsVal[0, :], idsVal[i, :])), f\"in epoch: {i}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
