{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementacion adaptada a 1D de https://github.com/naoto0804/pytorch-inpainting-with-partial-conv\n",
    "\n",
    "class PartialConv(nn.Module):\n",
    "    def __init__(self, in_channels_C,in_channels_M, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_conv = nn.Conv1d(in_channels_C, out_channels, kernel_size,\n",
    "                                    stride, padding, dilation, groups, bias)\n",
    "        self.mask_conv = nn.Conv1d(in_channels_M, out_channels, kernel_size,\n",
    "                                   stride, padding, dilation, groups, False)\n",
    "        # self.input_conv.apply(weights_init('kaiming'))\n",
    "\n",
    "        torch.nn.init.constant_(self.mask_conv.weight, 1.0)\n",
    "\n",
    "        # mask is not updated\n",
    "        for param in self.mask_conv.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self,input, mask):\n",
    "        # http://masc.cs.gmu.edu/wiki/partialconv\n",
    "        # C(X) = W^T * X + b, C(0) = b, D(M) = 1 * M + 0 = sum(M)\n",
    "        # W^T* (M .* X) / sum(M) + b = [C(M .* X) â€“ C(0)] / D(M) + C(0)\n",
    "        #print(input.shape, mask.shape)\n",
    "        output = self.input_conv(input * mask)\n",
    "        if self.input_conv.bias is not None:\n",
    "            output_bias = self.input_conv.bias.view(1, -1, 1).expand_as(output)\n",
    "        else:\n",
    "            output_bias = torch.zeros_like(output)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_mask = self.mask_conv(mask)\n",
    "\n",
    "        no_update_holes = output_mask == 0\n",
    "        mask_sum = output_mask.masked_fill_(no_update_holes, 1.0)\n",
    "\n",
    "        output_pre = (output - output_bias) / mask_sum + output_bias\n",
    "        output = output_pre.masked_fill_(no_update_holes, 0.0)\n",
    "\n",
    "        new_mask = torch.ones_like(output)\n",
    "        new_mask = new_mask.masked_fill_(no_update_holes, 0.0)\n",
    "\n",
    "        return output, new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building classifier\n",
    "\n",
    "# encoder\n",
    "class EncoderClassifier(torch.nn.Module):\n",
    "    \n",
    "\n",
    "    # init method\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim, num_classes):\n",
    "    \n",
    "    \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 1 Convolution layer\n",
    "        # Conv1d(input channel, output channel, kernel size)\n",
    "#         self.conv1 = torch.nn.Conv1d(1,64,3)\n",
    "#         self.conv1 = torch.nn.Conv1d(1,64,3, stride = 2)\n",
    "        \n",
    "        # partial convolution\n",
    "        self.pconv1 = PartialConv(in_channels_C = 1,in_channels_M = 1, out_channels = 64, kernel_size = 3, stride=2, padding=0, dilation=1, bias=True)\n",
    "        \n",
    "        # 2 Convolution layer\n",
    "        # Conv1d(input channel, output channel, kernel size)\n",
    "#         self.conv2 = torch.nn.Conv1d(64, 32, 3)\n",
    "#         self.conv2 = torch.nn.Conv1d(64, 32, 3, stride = 2)\n",
    "        \n",
    "        # partial convolution\n",
    "        self.pconv2 = PartialConv(in_channels_C = 64,in_channels_M = 64, out_channels = 32, kernel_size = 3, stride=2, padding=0, dilation=1, bias=True)\n",
    "        \n",
    "        # linear layer\n",
    "#         self.hidden1 = torch.nn.Linear(2144*2, hidden_dim)\n",
    "#         self.hidden1 = torch.nn.Linear(1088, hidden_dim)\n",
    "        self.hidden1 = torch.nn.Linear(1632, hidden_dim)\n",
    "        \n",
    "#         self.hidden2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # output layer\n",
    "        self.outputLayer = torch.nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        # activation function\n",
    "        self.activationConv = torch.nn.ReLU() #max(0, x)\n",
    "#         self.activationConv = torch.nn.Tanh()\n",
    "    \n",
    "        # this works well.(comparing with relu)\n",
    "        self.activationLinear = torch.nn.Tanh()\n",
    "\n",
    "        # this is getting nan values\n",
    "#         self.activationLinear = torch.nn.ReLU()\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # input shape: [batch_size, channels, sequence_length]\n",
    "        # print(\"input shape: {0}\".format(x.shape))\n",
    "#         print(\"input to encoder: \")\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # convolution 1\n",
    "        # x -> conv -> act -> ouput\n",
    "        # shape should be: [batch_size, number of ouput channels (64), length of output from convolution]\n",
    "        \n",
    "        #conv to time\n",
    "        # normal convolution\n",
    "#         outputTimeConv = self.activationConv(self.conv1Time(x[:, 0, :].unsqueeze(1)))\n",
    "#         outputTimeConv = self.activationConv(self.conv1(x[:, 0, :].unsqueeze(1)))\n",
    "        # partial conv\n",
    "        # output, newMask = pconv1(data, mask)\n",
    "        outputTimeConv, maskTime = self.pconv1(x[:, 0, :].unsqueeze(1), x[:, 3, :].unsqueeze(1))\n",
    "        # activation function\n",
    "        outputTimeConv = self.activationConv(outputTimeConv)\n",
    "        \n",
    "        \n",
    "        # conv to magnitude\n",
    "#         outputMagConv = self.activationConv(self.conv1Mag(x[:, 1, :].unsqueeze(1)))\n",
    "#         outputMagConv = self.activationConv(self.conv1(x[:, 1, :].unsqueeze(1)))\n",
    "        \n",
    "        # partial conv\n",
    "        # output, newMask = pconv1(data, mask)\n",
    "        outputMagConv, maskMag = self.pconv1(x[:, 1, :].unsqueeze(1), x[:, 3, :].unsqueeze(1))\n",
    "        # activation function\n",
    "        outputMagConv = self.activationConv(outputMagConv)\n",
    "        \n",
    "        \n",
    "        # conv to mag error\n",
    "#         outputMagErrorConv = self.activationConv(self.conv1(x[:, 2, :].unsqueeze(1)))\n",
    "        \n",
    "        # partial conv\n",
    "        # output, newMask = pconv1(data, mask)\n",
    "        outputMagErrorConv, maskError = self.pconv1(x[:, 2, :].unsqueeze(1), x[:, 3, :].unsqueeze(1))\n",
    "        # activation function\n",
    "        outputMagErrorConv = self.activationConv(outputMagErrorConv)\n",
    "        \n",
    "#         print(\"output conv1 shape: {0}\".format(outputMagConv.shape))\n",
    "#         print(\"output conv1 shape: {0}\".format(outputTimeConv.shape))\n",
    "        \n",
    "        # convolution 2\n",
    "#         # shape should be: [batch_size, number of ouput channels (32), length of output from convolution]\n",
    "        \n",
    "        \n",
    "        # conv to time\n",
    "#         outputTimeConv = self.activationConv(self.conv2(outputTimeConv))\n",
    "        \n",
    "        # partial conv\n",
    "        outputTimeConv, maskTime = self.pconv2(outputTimeConv, maskTime)\n",
    "        outputTimeConv = self.activationConv(outputTimeConv)\n",
    "        \n",
    "        \n",
    "        # conv to flux\n",
    "#         outputMagConv = self.activationConv(self.conv2(outputMagConv))\n",
    "        # part conv\n",
    "        outputMagConv, maskMag = self.pconv2(outputMagConv, maskMag)\n",
    "        outputMagConv = self.activationConv(outputMagConv)\n",
    "        \n",
    "        # conv to mag error\n",
    "#         outputMagErrorConv = self.activationConv(self.conv2(outputMagErrorConv))\n",
    "        # partial conv\n",
    "        outputMagErrorConv, maskError = self.pconv2(outputMagErrorConv, maskError)\n",
    "        outputMagErrorConv = self.activationConv(outputMagErrorConv)\n",
    "        \n",
    "#         print(\"output conv2 shape: {0}\".format(outputTimeConv.shape))\n",
    "#         print(\"output conv2 shape: {0}\".format(outputMagConv.shape))\n",
    "        \n",
    "        # flatten ouput\n",
    "        # shape should be: [batch_size, -1]\n",
    "        outputMagConv = outputMagConv.view(outputMagConv.shape[0], -1)\n",
    "        \n",
    "        outputTimeConv = outputTimeConv.view(outputTimeConv.shape[0], -1)\n",
    "        \n",
    "        outputMagErrorConv = outputMagErrorConv.view(outputMagErrorConv.shape[0], -1)\n",
    "        \n",
    "#         print(\"output reshape: \", outputMagConv.shape)\n",
    "#         print(\"output reshape: \", outputTimeConv.shape)\n",
    "                \n",
    "        # concatenate 3 towers\n",
    "#         output = torch.cat((outputMagConv, outputTimeConv), 1)\n",
    "        output = torch.cat((outputTimeConv, outputMagConv, outputMagErrorConv), 1)\n",
    "#         print(\"concatenate output shape: \", output.shape)\n",
    "        \n",
    "        # x -> hidden1 -> activation\n",
    "#         print(\"before linear layer: {0}\".format(output.shape))\n",
    "        output = self.activationLinear(self.hidden1(output))\n",
    "        # Should be an activiation function here?\n",
    "#         output = (self.hidden1(output))\n",
    "        \n",
    "        output = self.outputLayer(output)\n",
    "        \n",
    "        # this should return the classification\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
