{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "This notebook is to train the encoder as a classifier with the idea of validate the encoder architecture first and then use this to train the VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on guanaco\n",
    "# ATENTION: if it is going to run on guanaco, so comment the %matplotlib magic in next block\n",
    "trainingOnGuanaco = True\n",
    "\n",
    "# train without notebook\n",
    "trainWithJustPython = False\n",
    "\n",
    "# number_experiment (this is just a name)\n",
    "# priors:\n",
    "# 1\n",
    "number_experiment = 6\n",
    "number_experiment = str(number_experiment)\n",
    "\n",
    "# add general comment about experiment \n",
    "comment = \"encoder as clasifier with periodic + variable (without class balancing)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes to analyze\n",
    "# 42,  90,  16,  67,  62, 993,  92,  52,  88,  65, 991, 992,  15,\n",
    "#        95,   6,  53, 994,  64\n",
    "\n",
    "# periodic\n",
    "# only_these_labels = [16, 92, 53]\n",
    "\n",
    "# periodic + variable\n",
    "only_these_labels = [16, 92, 53, 88, 65, 6]\n",
    "# 53 has 24 light curves\n",
    "\n",
    "# only_these_labels = [16, 92]\n",
    "# only_these_labels = [16, 92]\n",
    "# only_these_labels = [42,  90,  16,  67,  62, 993,  92,  52,  88,  65, 991, 992,  15,\n",
    "#         95,   6,  53, 994,  64]\n",
    "\n",
    "# VAE parameters\n",
    "latentDim = 100\n",
    "hiddenDim = 100\n",
    "inputDim = 72\n",
    "\n",
    "# training\n",
    "epochs = 2\n",
    "\n",
    "# band\n",
    "# passband = 5\n",
    "passband = 5\n",
    "\n",
    "batch_training_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# from tqdm import tqdm_notebook\n",
    "\n",
    "# %matplotlib notebook\n",
    "\n",
    "# import functions to load dataset\n",
    "import sys\n",
    "sys.path.append(\"./codesToDatasets\")\n",
    "from plasticc_dataset_torch import get_plasticc_datasets\n",
    "# from plasticc_plotting import plot_light_curve\n",
    "\n",
    "import math\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to dataset\n",
    "pathToFile = \"/home/shared/astro/PLAsTiCC/\" if trainingOnGuanaco else \"/home/leo/Downloads/plasticc_torch-master/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset with pytorch tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have selected lazy loading. Light curves will be loaded ondemand from the harddrive\n",
      "Found 3 csv files at given path\n",
      "Loading /home/leo/Downloads/plasticc_torch-master/plasticc_train_lightcurves.csv\n",
      "Loading /home/leo/Downloads/plasticc_torch-master/plasticc_test_set_batch1.csv\n",
      "Loading /home/leo/Downloads/plasticc_torch-master/plasticc_test_set_batch2.csv\n"
     ]
    }
   ],
   "source": [
    "# torch_dataset_lazy = get_plasticc_datasets(pathToFile)\n",
    "\n",
    "# Light curves are tensors are now [bands, [mjd, flux, err, mask],\n",
    "# lc_data, lc_label, lc_plasticc_id                              \n",
    "torch_dataset_lazy = get_plasticc_datasets(pathToFile, only_these_labels=only_these_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting one light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lc_data, lc_label, lc_plasticc_id = torch_dataset_lazy.__getitem__(123)\n",
    "# display(lc_plasticc_id, lc_label)\n",
    "# 6 bands: u g r i z Y\n",
    "# 4 sequences: mjd, flux, error, mask\n",
    "# 72 samples\n",
    "# display(lc_data.shape, lc_data.dtype)\n",
    "# print(lc_data.detach().numpy()[0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_light_curve(torch_dataset_lazy, index_in_dataset=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting data (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 42307\n",
      "validation size:  5288\n",
      "test size: 5289\n",
      "sum:  52884\n"
     ]
    }
   ],
   "source": [
    "# Spliting the data\n",
    "\n",
    "# print(torch_dataset_lazy.__len__())\n",
    "\n",
    "# selecting train splitting\n",
    "train_size = int(0.8 * torch_dataset_lazy.__len__())\n",
    "#print(train_size)\n",
    "\n",
    "# getting test splitting\n",
    "validation_size = math.floor((torch_dataset_lazy.__len__() - train_size)/2)\n",
    "#print(validation_size)\n",
    "\n",
    "# getting test splitting\n",
    "test_size = torch_dataset_lazy.__len__() - train_size - validation_size\n",
    "#print(test_size)\n",
    "\n",
    "# spliting the torch dataset\n",
    "trainDataset, validationDataset,  testDataset = torch.utils.data.random_split(torch_dataset_lazy, [train_size, validation_size, test_size])\n",
    "\n",
    "print(\"train size:\", train_size)\n",
    "print(\"validation size: \", validation_size)\n",
    "print(\"test size:\", test_size)\n",
    "print(\"sum: \", train_size+ validation_size + test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data loader (minibatches)\n",
    "\n",
    "# # train loader\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size= batch_training_size, shuffle=True, num_workers = 4)\n",
    "\n",
    "# validation loader\n",
    "validationLoader = torch.utils.data.DataLoader(validationDataset, batch_size= batch_training_size, shuffle=True, num_workers = 4)\n",
    "\n",
    "# # test loader\n",
    "testLoader = torch.utils.data.DataLoader(testDataset)\n",
    "# trainLoader = torch.utils.data.DataLoader(torch_dataset_lazy, batch_size=256, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the path to save model while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory /home/leo/Desktop/thesis/work/thesis/experiments/6 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# create experiment's folder\n",
    "folder_path = (\"/home/lbravo/thesis/work/thesis/experiments/\" + number_experiment) if trainingOnGuanaco else (\"/home/leo/Desktop/thesis/work/thesis/experiments/\" + number_experiment)\n",
    "# !mkdir folder_path\n",
    "# os.makedirs(os.path.dirname(folder_path), exist_ok=True)\n",
    "\n",
    "# check if folder exists\n",
    "if not(os.path.isdir(folder_path)):\n",
    "        \n",
    "    # create folder\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % folder_path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % folder_path)\n",
    "else:\n",
    "    print(\"folder already exists\")\n",
    "\n",
    "# define paht to save model while training\n",
    "pathToSaveModel = \"/home/lbravo/thesis/thesis/work/thesis/experiments/\" + number_experiment + \"/model\" if trainingOnGuanaco else \"/home/leo/Desktop/thesis/work/thesis/experiments/\" + number_experiment + \"/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment parameters file created\n"
     ]
    }
   ],
   "source": [
    "# store varibales on file\n",
    "text_file = open(\"experiments/\" + number_experiment + \"/experimentParameters.txt\", \"w\")\n",
    "text = \"NÂ° experiment: {7}\\n General comment: {13}\\n Classes: {0}\\n train_size: {9}\\n validation_size: {10}\\n test_size: {11}\\n total dataset size: {12}\\n Epochs: {8}\\n Latent dimension: {1}\\n Hidden dimension: {2}\\n Input dimension: {3}\\n Passband: {4}\\n Learning rate: {5}\\n Batch training size: {6}\".format(only_these_labels, latentDim, hiddenDim, inputDim, passband, learning_rate, batch_training_size, number_experiment, epochs, train_size, validation_size, test_size, train_size + validation_size + test_size, comment)\n",
    "text_file.write(text)\n",
    "text_file.close()\n",
    "print(\"experiment parameters file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define autoencoder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementacion adaptada a 1D de https://github.com/naoto0804/pytorch-inpainting-with-partial-conv\n",
    "\n",
    "class PartialConv(nn.Module):\n",
    "    def __init__(self, in_channels_C,in_channels_M, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_conv = nn.Conv1d(in_channels_C, out_channels, kernel_size,\n",
    "                                    stride, padding, dilation, groups, bias)\n",
    "        self.mask_conv = nn.Conv1d(in_channels_M, out_channels, kernel_size,\n",
    "                                   stride, padding, dilation, groups, False)\n",
    "        # self.input_conv.apply(weights_init('kaiming'))\n",
    "\n",
    "        torch.nn.init.constant_(self.mask_conv.weight, 1.0)\n",
    "\n",
    "        # mask is not updated\n",
    "        for param in self.mask_conv.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self,input, mask):\n",
    "        # http://masc.cs.gmu.edu/wiki/partialconv\n",
    "        # C(X) = W^T * X + b, C(0) = b, D(M) = 1 * M + 0 = sum(M)\n",
    "        # W^T* (M .* X) / sum(M) + b = [C(M .* X) â C(0)] / D(M) + C(0)\n",
    "        #print(input.shape, mask.shape)\n",
    "        output = self.input_conv(input * mask)\n",
    "        if self.input_conv.bias is not None:\n",
    "            output_bias = self.input_conv.bias.view(1, -1, 1).expand_as(output)\n",
    "        else:\n",
    "            output_bias = torch.zeros_like(output)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_mask = self.mask_conv(mask)\n",
    "\n",
    "        no_update_holes = output_mask == 0\n",
    "        mask_sum = output_mask.masked_fill_(no_update_holes, 1.0)\n",
    "\n",
    "        output_pre = (output - output_bias) / mask_sum + output_bias\n",
    "        output = output_pre.masked_fill_(no_update_holes, 0.0)\n",
    "\n",
    "        new_mask = torch.ones_like(output)\n",
    "        new_mask = new_mask.masked_fill_(no_update_holes, 0.0)\n",
    "\n",
    "        return output, new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building classifier\n",
    "\n",
    "# encoder\n",
    "class Encoder(torch.nn.Module):\n",
    "    \n",
    "\n",
    "    # init method\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim, num_classes):\n",
    "    \n",
    "    \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 1 Convolution layer\n",
    "        # Conv1d(input channel, output channel, kernel size)\n",
    "#         self.conv1 = torch.nn.Conv1d(1,64,3)\n",
    "#         self.conv1 = torch.nn.Conv1d(1,64,3, stride = 2)\n",
    "        \n",
    "        # partial convolution\n",
    "        self.pconv1 = PartialConv(in_channels_C = 1,in_channels_M = 1, out_channels = 64, kernel_size = 3, stride=2, padding=0, dilation=1, bias=True)\n",
    "        \n",
    "        # 2 Convolution layer\n",
    "        # Conv1d(input channel, output channel, kernel size)\n",
    "#         self.conv2 = torch.nn.Conv1d(64, 32, 3)\n",
    "#         self.conv2 = torch.nn.Conv1d(64, 32, 3, stride = 2)\n",
    "        \n",
    "        # partial convolution\n",
    "        self.pconv2 = PartialConv(in_channels_C = 64,in_channels_M = 64, out_channels = 32, kernel_size = 3, stride=2, padding=0, dilation=1, bias=True)\n",
    "        \n",
    "        # linear layer\n",
    "#         self.hidden1 = torch.nn.Linear(2144*2, hidden_dim)\n",
    "#         self.hidden1 = torch.nn.Linear(1088, hidden_dim)\n",
    "        self.hidden1 = torch.nn.Linear(1632, hidden_dim)\n",
    "        \n",
    "#         self.hidden2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # output layer\n",
    "        self.outputLayer = torch.nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        # activation function\n",
    "        self.activationConv = torch.nn.ReLU() #max(0, x)\n",
    "#         self.activationConv = torch.nn.Tanh()\n",
    "    \n",
    "        # this works well.(comparing with relu)\n",
    "        self.activationLinear = torch.nn.Tanh()\n",
    "\n",
    "        # this is getting nan values\n",
    "#         self.activationLinear = torch.nn.ReLU()\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # input shape: [batch_size, channels, sequence_length]\n",
    "        # print(\"input shape: {0}\".format(x.shape))\n",
    "#         print(\"input to encoder: \")\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # convolution 1\n",
    "        # x -> conv -> act -> ouput\n",
    "        # shape should be: [batch_size, number of ouput channels (64), length of output from convolution]\n",
    "        \n",
    "        #conv to time\n",
    "        # normal convolution\n",
    "#         outputTimeConv = self.activationConv(self.conv1Time(x[:, 0, :].unsqueeze(1)))\n",
    "#         outputTimeConv = self.activationConv(self.conv1(x[:, 0, :].unsqueeze(1)))\n",
    "        # partial conv\n",
    "        # output, newMask = pconv1(data, mask)\n",
    "        outputTimeConv, maskTime = self.pconv1(x[:, 0, :].unsqueeze(1), x[:, 3, :].unsqueeze(1))\n",
    "        # activation function\n",
    "        outputTimeConv = self.activationConv(outputTimeConv)\n",
    "        \n",
    "        \n",
    "        # conv to magnitude\n",
    "#         outputMagConv = self.activationConv(self.conv1Mag(x[:, 1, :].unsqueeze(1)))\n",
    "#         outputMagConv = self.activationConv(self.conv1(x[:, 1, :].unsqueeze(1)))\n",
    "        \n",
    "        # partial conv\n",
    "        # output, newMask = pconv1(data, mask)\n",
    "        outputMagConv, maskMag = self.pconv1(x[:, 1, :].unsqueeze(1), x[:, 3, :].unsqueeze(1))\n",
    "        # activation function\n",
    "        outputMagConv = self.activationConv(outputMagConv)\n",
    "        \n",
    "        \n",
    "        # conv to mag error\n",
    "#         outputMagErrorConv = self.activationConv(self.conv1(x[:, 2, :].unsqueeze(1)))\n",
    "        \n",
    "        # partial conv\n",
    "        # output, newMask = pconv1(data, mask)\n",
    "        outputMagErrorConv, maskError = self.pconv1(x[:, 2, :].unsqueeze(1), x[:, 3, :].unsqueeze(1))\n",
    "        # activation function\n",
    "        outputMagErrorConv = self.activationConv(outputMagErrorConv)\n",
    "        \n",
    "#         print(\"output conv1 shape: {0}\".format(outputMagConv.shape))\n",
    "#         print(\"output conv1 shape: {0}\".format(outputTimeConv.shape))\n",
    "        \n",
    "        # convolution 2\n",
    "#         # shape should be: [batch_size, number of ouput channels (32), length of output from convolution]\n",
    "        \n",
    "        \n",
    "        # conv to time\n",
    "#         outputTimeConv = self.activationConv(self.conv2(outputTimeConv))\n",
    "        \n",
    "        # partial conv\n",
    "        outputTimeConv, maskTime = self.pconv2(outputTimeConv, maskTime)\n",
    "        outputTimeConv = self.activationConv(outputTimeConv)\n",
    "        \n",
    "        \n",
    "        # conv to flux\n",
    "#         outputMagConv = self.activationConv(self.conv2(outputMagConv))\n",
    "        # part conv\n",
    "        outputMagConv, maskMag = self.pconv2(outputMagConv, maskMag)\n",
    "        outputMagConv = self.activationConv(outputMagConv)\n",
    "        \n",
    "        # conv to mag error\n",
    "#         outputMagErrorConv = self.activationConv(self.conv2(outputMagErrorConv))\n",
    "        # partial conv\n",
    "        outputMagErrorConv, maskError = self.pconv2(outputMagErrorConv, maskError)\n",
    "        outputMagErrorConv = self.activationConv(outputMagErrorConv)\n",
    "        \n",
    "#         print(\"output conv2 shape: {0}\".format(outputTimeConv.shape))\n",
    "#         print(\"output conv2 shape: {0}\".format(outputMagConv.shape))\n",
    "        \n",
    "        # flatten ouput\n",
    "        # shape should be: [batch_size, -1]\n",
    "        outputMagConv = outputMagConv.view(outputMagConv.shape[0], -1)\n",
    "        \n",
    "        outputTimeConv = outputTimeConv.view(outputTimeConv.shape[0], -1)\n",
    "        \n",
    "        outputMagErrorConv = outputMagErrorConv.view(outputMagErrorConv.shape[0], -1)\n",
    "        \n",
    "#         print(\"output reshape: \", outputMagConv.shape)\n",
    "#         print(\"output reshape: \", outputTimeConv.shape)\n",
    "                \n",
    "        # concatenate 3 towers\n",
    "#         output = torch.cat((outputMagConv, outputTimeConv), 1)\n",
    "        output = torch.cat((outputTimeConv, outputMagConv, outputMagErrorConv), 1)\n",
    "#         print(\"concatenate output shape: \", output.shape)\n",
    "        \n",
    "        # x -> hidden1 -> activation\n",
    "#         print(\"before linear layer: {0}\".format(output.shape))\n",
    "        output = self.activationLinear(self.hidden1(output))\n",
    "        # Should be an activiation function here?\n",
    "#         output = (self.hidden1(output))\n",
    "        \n",
    "        output = self.outputLayer(output)\n",
    "        \n",
    "        # this should return the classification\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining parameters to Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of parameters\n",
    "# latentDim = 5\n",
    "# hiddenDim = 10\n",
    "# inputDim = 72\n",
    "\n",
    "latentDim = latentDim\n",
    "hiddenDim = hiddenDim\n",
    "inputDim = inputDim\n",
    "\n",
    "passband = passband\n",
    "\n",
    "num_classes = len(only_these_labels)\n",
    "\n",
    "\n",
    "# defining model\n",
    "model = Encoder(latent_dim = latentDim, hidden_dim = hiddenDim, input_dim = inputDim, num_classes = num_classes)\n",
    "\n",
    "# mdel to GPU\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (pconv1): PartialConv(\n",
      "    (input_conv): Conv1d(1, 64, kernel_size=(3,), stride=(2,))\n",
      "    (mask_conv): Conv1d(1, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
      "  )\n",
      "  (pconv2): PartialConv(\n",
      "    (input_conv): Conv1d(64, 32, kernel_size=(3,), stride=(2,))\n",
      "    (mask_conv): Conv1d(64, 32, kernel_size=(3,), stride=(2,), bias=False)\n",
      "  )\n",
      "  (hidden1): Linear(in_features=1632, out_features=100, bias=True)\n",
      "  (outputLayer): Linear(in_features=100, out_features=6, bias=True)\n",
      "  (activationConv): ReLU()\n",
      "  (activationLinear): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(\"input dimension: {0}\".format(len(list(trainLoader))))\n",
    "\n",
    "# # parameters number\n",
    "# count = 0\n",
    "\n",
    "# # # check model dimension\n",
    "# for name, param in model.state_dict().items():\n",
    "#     # name: str\n",
    "#     # param: Tensor\n",
    "# #     print(\"{0}: {1} \\n\".format(name, param.shape))\n",
    "# #     print(param.shape[0]*(param.shape[1] if len(param.shape)>1 else 1))\n",
    "# #     print(param.shape)\n",
    "#     count += param.shape[0]*(param.shape[1] if len(param.shape)>1 else 1)\n",
    "# # for param in model.parameters():\n",
    "    \n",
    "# print(\"number of parameters: \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it builds a mask for the deltas. It compares the next with the previous one element.\n",
    "# original mask: [1,1, 0, 0]\n",
    "# delta mask: [1, 0, 0]\n",
    "# The same results is got with original_mask[:, 1:]\n",
    "def generate_delta_mask(mask):\n",
    "    \n",
    "    # generate delta mask\n",
    "#     mask_delta = mask[:, 1:].type(torch.BoolTensor) & mask[:, :-1].type(torch.BoolTensor)\n",
    "    mask_delta = mask[:, 1:]\n",
    "    \n",
    "    return mask_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate delta time and flux\n",
    "# data = [batchSize, channels, [time, flux, err, mask], light curve]\n",
    "def generateDeltas(data, passBand):\n",
    "    \n",
    "    # work with delta time and magnitude\n",
    "    \n",
    "#     print(\"generate deltas input shape: {0}\".format(data.shape) )\n",
    "    # delta time\n",
    "    tmpDeltaTime = data[:, passBand, 0, 1:] - data[:, passBand, 0, :-1]\n",
    "\n",
    "#     print(\"generate deltas time shape: {0}\".format(tmpDeltaTime.shape) )\n",
    "\n",
    "#     # delta magnitude\n",
    "    tmpDeltaMagnitude = data[:, passBand, 1, 1:] - data[:, passBand, 1, :-1]\n",
    "#     print(\"generate deltas flux shape: {0}\".format(tmpDeltaMagnitude.shape))\n",
    "    \n",
    "    # delta errors\n",
    "    tmpDeltaMagError = data[:, passBand, 2, 1:] - data[:, passBand, 2, :-1]\n",
    "    \n",
    "    # delta mask\n",
    "    tmpMask = generate_delta_mask(data[:, passBand, 3,:])\n",
    "    \n",
    "    # concatenate tensors\n",
    "    dataToUse = torch.cat((tmpDeltaTime.unsqueeze(1), tmpDeltaMagnitude.unsqueeze(1), tmpDeltaMagError.unsqueeze(1), tmpMask.unsqueeze(1)), 1)\n",
    "#     print(\"data to use shape: {0}\".format(dataToUse.shape))\n",
    "    \n",
    "    # normalize data\n",
    "    # this was commented because it considerate that delta is already a normalization\n",
    "#     dataToUse = normalizeLightCurve(dataToUse)\n",
    "    \n",
    "    # returning data\n",
    "    return dataToUse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the labels to classes 0 to C-1\n",
    "\n",
    "def mapLabels(labels):\n",
    "\n",
    "    for i in range(len(only_these_labels)):\n",
    "        \n",
    "        labels[labels == only_these_labels[i]] = i \n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test ok\n"
     ]
    }
   ],
   "source": [
    "# test mapLabels function\n",
    "\n",
    "# input\n",
    "input_ = np.array([16, 92, 53, 16, 53])\n",
    "labels = np.array([0, 1, 2, 0, 2])\n",
    "\n",
    "# output\n",
    "output = mapLabels(input_)\n",
    "\n",
    "# # test\n",
    "assert np.array_equal(output,  labels) == True, \"Should be 0, 1, 2\"\n",
    "\n",
    "print(\"test ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the training\n",
      "epoch:    0 / 2\n",
      "early stopping counter:  2\n",
      "New min test loss. Saving model\n",
      "saving losses\n",
      "saving f1 scores\n",
      "epoch:    1 / 2\n",
      "New min test loss. Saving model\n",
      "saving losses\n",
      "saving f1 scores\n",
      "training has finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.5)\n",
    "\n",
    "# loss function\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss\n",
    "train_loss = np.zeros((epochs,))\n",
    "test_loss = np.zeros((epochs,))\n",
    "\n",
    "# f1 scores\n",
    "f1Scores = np.zeros((epochs, ))\n",
    "\n",
    "# min global test loss \n",
    "minTestLossGlobalSoFar = float(\"inf\")\n",
    "\n",
    "# # # loss plot\n",
    "# if it is not cluster\n",
    "if (not trainingOnGuanaco) or (not trainWithJustPython):\n",
    "    \n",
    "    # add f1 and loss plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (7, 3), tight_layout = True)\n",
    "    # # fig, ax = plt.subplots()\n",
    "    \n",
    "    # error\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[0].set_ylabel(\"Error\")\n",
    "    \n",
    "    \n",
    "    # f1 score\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].set_ylabel(\"F1 score\")\n",
    "    \n",
    "\n",
    "# early stopping\n",
    "prior_test_error = 0\n",
    "count_early_stop = 0\n",
    "threshold_early_stop = 20\n",
    "\n",
    "\n",
    "print(\"starting the training\")\n",
    "\n",
    "\n",
    "# epoch\n",
    "for nepoch in range(epochs):\n",
    "        \n",
    "    print(\"epoch:    {0} / {1}\".format(nepoch, epochs))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    ######## Train ###########\n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    for data_ in trainLoader:\n",
    "        \n",
    "        data = data_[0]\n",
    "        labels = data_[1].cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # this take the deltas (time and magnitude)\n",
    "        data = generateDeltas(data, passband).type(torch.FloatTensor).cuda()\n",
    "\n",
    "        # get model output\n",
    "        outputs = model.forward(data)\n",
    "        \n",
    "        # loss function\n",
    "        loss = lossFunction(outputs, mapLabels(labels).cuda())\n",
    "        \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add loss value (of the currrent minibatch)\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "\n",
    "    # get epoch loss value\n",
    "    train_loss[nepoch] = epoch_train_loss / train_size\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### Validation ########\n",
    "    \n",
    "    epoch_test_loss = 0\n",
    "    \n",
    "    # check f1 score in each minibatch\n",
    "    f1Score = 0\n",
    "    \n",
    "    batchCounter = 0\n",
    "    \n",
    "    # minibatches\n",
    "    for data_ in validationLoader:\n",
    "        \n",
    "        data = data_[0]\n",
    "        labels = data_[1].cuda()\n",
    "        \n",
    "        data = generateDeltas(data, passband).type(torch.FloatTensor).cuda()\n",
    "        \n",
    "        outputs = model.forward(data)\n",
    "        \n",
    "        # loss function\n",
    "        loss = lossFunction(outputs, mapLabels(labels).cuda())\n",
    "    \n",
    "        #  store minibatch loss value\n",
    "        epoch_test_loss += loss.item()\n",
    "        \n",
    "        # f1 score\n",
    "        f1Score += f1_score(mapLabels(labels).cpu().numpy(), torch.argmax(outputs, 1).cpu().numpy(), average = \"micro\")\n",
    "        \n",
    "        # batch counter\n",
    "        batchCounter += 1\n",
    "    \n",
    "    # get epoch test loss value\n",
    "    test_loss[nepoch] = epoch_test_loss / validation_size\n",
    "    \n",
    "    # get epoch f1 score\n",
    "    f1Scores[nepoch] = f1Score / batchCounter\n",
    "    \n",
    "    # plot loss values\n",
    "    # if it's not cluster\n",
    "    if (not trainingOnGuanaco) or (not trainWithJustPython):\n",
    "\n",
    "        # loss values\n",
    "        ax[0].plot(train_loss[0: nepoch], label = \"train\", linewidth = 3, c = \"red\") \n",
    "        ax[0].plot(test_loss[0: nepoch], label = \"test\", linestyle = \"--\", linewidth = 3, c = \"green\")\n",
    "        \n",
    "        # f1 score values\n",
    "        ax[1].plot(f1Scores[0: nepoch], linewidth = 3, c = \"green\")\n",
    "        \n",
    "        # plot\n",
    "        fig.canvas.draw()\n",
    "    \n",
    "    \n",
    "    #### Early stopping #####\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if new test loss is greater than the older one\n",
    "    count_early_stop += 1\n",
    "    if epoch_test_loss > prior_test_error:\n",
    "        count_early_stop += 1\n",
    "        print(\"early stopping counter: \", count_early_stop)\n",
    "    else: \n",
    "        count_early_stop = 0\n",
    "    \n",
    "    # update prior test error\n",
    "    prior_test_error = epoch_test_loss\n",
    "    \n",
    "    # analyze early stopping\n",
    "    if count_early_stop > threshold_early_stop:\n",
    "        \n",
    "        print(\"Early stopping in epoch: \", nepoch)\n",
    "        text_file = open(\"experiments/\" + number_experiment + \"/earlyStopping.txt\", \"w\")\n",
    "        metricsText = \"Epoch: {0}\\n ES counter: {1}\\n, Reconstruction test error: {2}\".format(nepoch, count_early_stop, epoch_test_loss)\n",
    "        text_file.write(metricsText)\n",
    "        text_file.close()\n",
    "        break\n",
    "        \n",
    "    #### Saving best model ####\n",
    "    \n",
    "    # if epoch test loss is smaller than global min\n",
    "    if (epoch_test_loss/validation_size) < minTestLossGlobalSoFar:\n",
    "        \n",
    "        print(\"New min test loss. Saving model\")\n",
    "#         print(\"old: \", minTestLossGlobalSoFar)\n",
    "#         print(\"new: \", epoch_test_loss)\n",
    "        \n",
    "        # save model\n",
    "        torch.save(model.state_dict(), pathToSaveModel)\n",
    "        \n",
    "        # update global min\n",
    "        minTestLossGlobalSoFar = epoch_test_loss\n",
    "        \n",
    "        # write metrics\n",
    "        text_file = open(\"experiments/\" + number_experiment + \"/bestScoresModelTraining.txt\", \"w\")\n",
    "        metricsText = \"Epoch: {0}\\n Reconstruction test error: {1}\".format(nepoch, minTestLossGlobalSoFar)\n",
    "        text_file.write(metricsText)\n",
    "        text_file.close()\n",
    "\n",
    "        \n",
    "        \n",
    "    # save losses\n",
    "    print(\"saving losses\")\n",
    "    losses = np.asarray([train_loss, test_loss]).T\n",
    "    np.savetxt(\"experiments/\" + number_experiment + \"/training_losses.csv\", losses, delimiter=\",\")\n",
    "    \n",
    "\n",
    "    # save f1 scores\n",
    "    print(\"saving f1 scores\")\n",
    "    np.savetxt(\"experiments/\" + number_experiment + \"/f1Scores.csv\", f1Scores, delimiter=\",\")\n",
    "\n",
    "# final message\n",
    "print(\"training has finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving confusion matrix scores\n",
      "saving clasification report\n"
     ]
    }
   ],
   "source": [
    "# get y true and labels\n",
    "\n",
    "predictions = np.zeros(shape = (0,))\n",
    "labels_ = np.zeros(shape = (0,))\n",
    "\n",
    "# minibatches\n",
    "for data_ in validationLoader:\n",
    "        \n",
    "    data = data_[0].cuda()\n",
    "    labels = data_[1].cuda()\n",
    "\n",
    "    data = generateDeltas(data, passband).type(torch.FloatTensor).cuda()\n",
    "\n",
    "    outputs = model.forward(data)\n",
    "    \n",
    "    prediction = torch.argmax(outputs, 1).cpu().numpy()\n",
    "\n",
    "    label = mapLabels(labels).cpu().numpy()\n",
    "    \n",
    "    predictions = np.append(predictions, prediction)\n",
    "    labels_ = np.append(labels_, label)\n",
    "    \n",
    "\n",
    "# getting confusion amtrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(labels_, predictions)\n",
    "\n",
    "print(\"saving confusion matrix scores\")\n",
    "np.savetxt(\"experiments/\" + number_experiment + \"/confusionMatrix.csv\", cm, delimiter=\",\")\n",
    "\n",
    "\n",
    "# classification report\n",
    "print(\"saving clasification report\")\n",
    "text_file = open(\"experiments/\" + number_experiment + \"/clasificationReport.txt\", \"w\")\n",
    "text = classification_report(labels_, predictions)\n",
    "text_file.write(text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop execution if it's on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Exit from code, because we are in cluster or running locally. Training has finished.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Exit from code, because we are in cluster or running locally. Training has finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "if  trainingOnGuanaco or trainWithJustPython:\n",
    "\n",
    "    sys.exit(\"Exit from code, because we are in cluster or running locally. Training has finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load losses array\n",
    "losses = pd.read_csv(\"/home/leo/Desktop/thesis/work/thesis/experiments/\"+ number_experiment + \"/training_losses.csv\")\n",
    "\n",
    "# f1 scores\n",
    "f1Scores = pd.read_csv(\"/home/leo/Desktop/thesis/work/thesis/experiments/\"+ number_experiment + \"/f1Scores.csv\")\n",
    "\n",
    "# plot losses\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10,4), tight_layout = True)\n",
    "\n",
    "# loss\n",
    "ax[0].set_xlabel(\"NÂ° epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].plot(losses.iloc[:, 0], label = \"train\")\n",
    "ax[0].plot(losses.iloc[:, 1], label = \"validation\")\n",
    "ax[0].scatter(429, 0.000845159766508081, c = \"r\", linewidths = 10)\n",
    "ax[0].legend()\n",
    "\n",
    "# f1 scores\n",
    "ax[1].set_xlabel(\"NÂ° epoch\")\n",
    "ax[1].set_ylabel(\"F1 score\")\n",
    "ax[1].plot(f1Scores)\n",
    "ax[1].scatter(429, f1Scores.iloc[428], c = \"r\", linewidths = 10)\n",
    "\n",
    "\n",
    "# ax[0].scatter(429, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red point is the epoch with lower value in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "cm = pd.read_csv('./experiments/' + number_experiment + '/confusionMatrix.csv', header = None) \n",
    "\n",
    "display(cm)\n",
    "\n",
    "sn.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "!cat ./experiments/6/clasificationReport.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
