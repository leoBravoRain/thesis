{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "This notebook is to train the encoder as a classifier with the idea of validate the encoder architecture first and then use this to train the VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on guanaco\n",
    "# ATENTION: if it is going to run on guanaco, so comment the %matplotlib magic in next block\n",
    "trainingOnGuanaco = False\n",
    "\n",
    "# train without notebook\n",
    "trainWithJustPython = True\n",
    "\n",
    "# number_experiment (this is just a name)\n",
    "# priors:\n",
    "# 1\n",
    "number_experiment = 6\n",
    "number_experiment = str(number_experiment)\n",
    "\n",
    "# add general comment about experiment \n",
    "comment = \"training encoder as classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes to analyze\n",
    "# 42,  90,  16,  67,  62, 993,  92,  52,  88,  65, 991, 992,  15,\n",
    "#        95,   6,  53, 994,  64\n",
    "only_these_labels = [16, 92, 53]\n",
    "# 53 has 24 light curves\n",
    "\n",
    "# only_these_labels = [16, 92]\n",
    "# only_these_labels = [16, 92]\n",
    "# only_these_labels = [42,  90,  16,  67,  62, 993,  92,  52,  88,  65, 991, 992,  15,\n",
    "#         95,   6,  53, 994,  64]\n",
    "\n",
    "# VAE parameters\n",
    "latentDim = 100\n",
    "hiddenDim = 100\n",
    "inputDim = 72\n",
    "\n",
    "# training\n",
    "epochs = 2\n",
    "\n",
    "# band\n",
    "# passband = 5\n",
    "passband = 5\n",
    "\n",
    "batch_training_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# from tqdm import tqdm_notebook\n",
    "\n",
    "# %matplotlib notebook\n",
    "\n",
    "# import functions to load dataset\n",
    "import sys\n",
    "sys.path.append(\"./codesToDatasets\")\n",
    "from plasticc_dataset_torch import get_plasticc_datasets\n",
    "# from plasticc_plotting import plot_light_curve\n",
    "\n",
    "import math\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to dataset\n",
    "pathToFile = \"/home/shared/astro/PLAsTiCC/\" if trainingOnGuanaco else \"/home/leo/Downloads/plasticc_torch-master/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset with pytorch tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have selected lazy loading. Light curves will be loaded ondemand from the harddrive\n",
      "Found 3 csv files at given path\n",
      "Loading /home/leo/Downloads/plasticc_torch-master/plasticc_train_lightcurves.csv\n",
      "Loading /home/leo/Downloads/plasticc_torch-master/plasticc_test_set_batch1.csv\n",
      "Loading /home/leo/Downloads/plasticc_torch-master/plasticc_test_set_batch2.csv\n"
     ]
    }
   ],
   "source": [
    "# torch_dataset_lazy = get_plasticc_datasets(pathToFile)\n",
    "\n",
    "# Light curves are tensors are now [bands, [mjd, flux, err, mask],\n",
    "# lc_data, lc_label, lc_plasticc_id                              \n",
    "torch_dataset_lazy = get_plasticc_datasets(pathToFile, only_these_labels=only_these_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting one light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lc_data, lc_label, lc_plasticc_id = torch_dataset_lazy.__getitem__(123)\n",
    "# display(lc_plasticc_id, lc_label)\n",
    "# 6 bands: u g r i z Y\n",
    "# 4 sequences: mjd, flux, error, mask\n",
    "# 72 samples\n",
    "# display(lc_data.shape, lc_data.dtype)\n",
    "# print(lc_data.detach().numpy()[0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_light_curve(torch_dataset_lazy, index_in_dataset=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting data (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 25030\n",
      "validation size:  3129\n",
      "test size: 3129\n",
      "sum:  31288\n"
     ]
    }
   ],
   "source": [
    "# Spliting the data\n",
    "\n",
    "# print(torch_dataset_lazy.__len__())\n",
    "\n",
    "# selecting train splitting\n",
    "train_size = int(0.8 * torch_dataset_lazy.__len__())\n",
    "#print(train_size)\n",
    "\n",
    "# getting test splitting\n",
    "validation_size = math.floor((torch_dataset_lazy.__len__() - train_size)/2)\n",
    "#print(validation_size)\n",
    "\n",
    "# getting test splitting\n",
    "test_size = torch_dataset_lazy.__len__() - train_size - validation_size\n",
    "#print(test_size)\n",
    "\n",
    "# spliting the torch dataset\n",
    "trainDataset, validationDataset,  testDataset = torch.utils.data.random_split(torch_dataset_lazy, [train_size, validation_size, test_size])\n",
    "\n",
    "print(\"train size:\", train_size)\n",
    "print(\"validation size: \", validation_size)\n",
    "print(\"test size:\", test_size)\n",
    "print(\"sum: \", train_size+ validation_size + test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data loader (minibatches)\n",
    "\n",
    "# # train loader\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size= batch_training_size, shuffle=True, num_workers = 4)\n",
    "\n",
    "# validation loader\n",
    "validationLoader = torch.utils.data.DataLoader(validationDataset, batch_size= batch_training_size, shuffle=True, num_workers = 4)\n",
    "\n",
    "# # test loader\n",
    "testLoader = torch.utils.data.DataLoader(testDataset)\n",
    "# trainLoader = torch.utils.data.DataLoader(torch_dataset_lazy, batch_size=256, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the path to save model while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory /home/leo/Desktop/thesis/work/thesis/experiments/6 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# create experiment's folder\n",
    "folder_path = (\"/home/lbravo/thesis/work/thesis/experiments/\" + number_experiment) if trainingOnGuanaco else (\"/home/leo/Desktop/thesis/work/thesis/experiments/\" + number_experiment)\n",
    "# !mkdir folder_path\n",
    "# os.makedirs(os.path.dirname(folder_path), exist_ok=True)\n",
    "\n",
    "# check if folder exists\n",
    "if not(os.path.isdir(folder_path)):\n",
    "        \n",
    "    # create folder\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % folder_path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % folder_path)\n",
    "else:\n",
    "    print(\"folder already exists\")\n",
    "\n",
    "# define paht to save model while training\n",
    "pathToSaveModel = \"/home/lbravo/thesis/thesis/work/thesis/experiments/\" + number_experiment + \"/model\" if trainingOnGuanaco else \"/home/leo/Desktop/thesis/work/thesis/experiments/\" + number_experiment + \"/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment parameters file created\n"
     ]
    }
   ],
   "source": [
    "# store varibales on file\n",
    "text_file = open(\"experiments/\" + number_experiment + \"/experimentParameters.txt\", \"w\")\n",
    "text = \"NÂ° experiment: {7}\\n General comment: {13}\\n Classes: {0}\\n train_size: {9}\\n validation_size: {10}\\n test_size: {11}\\n total dataset size: {12}\\n Epochs: {8}\\n Latent dimension: {1}\\n Hidden dimension: {2}\\n Input dimension: {3}\\n Passband: {4}\\n Learning rate: {5}\\n Batch training size: {6}\".format(only_these_labels, latentDim, hiddenDim, inputDim, passband, learning_rate, batch_training_size, number_experiment, epochs, train_size, validation_size, test_size, train_size + validation_size + test_size, comment)\n",
    "text_file.write(text)\n",
    "text_file.close()\n",
    "print(\"experiment parameters file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define autoencoder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementacion adaptada a 1D de https://github.com/naoto0804/pytorch-inpainting-with-partial-conv\n",
    "\n",
    "class PartialConv(nn.Module):\n",
    "    def __init__(self, in_channels_C,in_channels_M, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_conv = nn.Conv1d(in_channels_C, out_channels, kernel_size,\n",
    "                                    stride, padding, dilation, groups, bias)\n",
    "        self.mask_conv = nn.Conv1d(in_channels_M, out_channels, kernel_size,\n",
    "                                   stride, padding, dilation, groups, False)\n",
    "        # self.input_conv.apply(weights_init('kaiming'))\n",
    "\n",
    "        torch.nn.init.constant_(self.mask_conv.weight, 1.0)\n",
    "\n",
    "        # mask is not updated\n",
    "        for param in self.mask_conv.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self,input, mask):\n",
    "        # http://masc.cs.gmu.edu/wiki/partialconv\n",
    "        # C(X) = W^T * X + b, C(0) = b, D(M) = 1 * M + 0 = sum(M)\n",
    "        # W^T* (M .* X) / sum(M) + b = [C(M .* X) â C(0)] / D(M) + C(0)\n",
    "        #print(input.shape, mask.shape)\n",
    "        output = self.input_conv(input * mask)\n",
    "        if self.input_conv.bias is not None:\n",
    "            output_bias = self.input_conv.bias.view(1, -1, 1).expand_as(output)\n",
    "        else:\n",
    "            output_bias = torch.zeros_like(output)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_mask = self.mask_conv(mask)\n",
    "\n",
    "        no_update_holes = output_mask == 0\n",
    "        mask_sum = output_mask.masked_fill_(no_update_holes, 1.0)\n",
    "\n",
    "        output_pre = (output - output_bias) / mask_sum + output_bias\n",
    "        output = output_pre.masked_fill_(no_update_holes, 0.0)\n",
    "\n",
    "        new_mask = torch.ones_like(output)\n",
    "        new_mask = new_mask.masked_fill_(no_update_holes, 0.0)\n",
    "\n",
    "        return output, new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building classifier\n",
    "\n",
    "# encoder\n",
    "class Encoder(torch.nn.Module):\n",
    "    \n",
    "\n",
    "    # init method\n",
    "    def __init__(self, latent_dim, hidden_dim, input_dim, num_classes):\n",
    "    \n",
    "    \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 1 Convolution layer\n",
    "        # Conv1d(input channel, output channel, kernel size)\n",
    "#         self.conv1 = torch.nn.Conv1d(1,64,3)\n",
    "#         self.conv1 = torch.nn.Conv1d(1,64,3, stride = 2)\n",
    "        \n",
    "        # partial convolution\n",
    "        self.pconv1 = PartialConv(in_channels_C = 1,in_channels_M = 1, out_channels = 64, kernel_size = 3, stride=2, padding=0, dilation=1, bias=True)\n",
    "        \n",
    "        # 2 Convolution layer\n",
    "        # Conv1d(input channel, output channel, kernel size)\n",
    "#         self.conv2 = torch.nn.Conv1d(64, 32, 3)\n",
    "#         self.conv2 = torch.nn.Conv1d(64, 32, 3, stride = 2)\n",
    "        \n",
    "        # partial convolution\n",
    "        self.pconv2 = PartialConv(in_channels_C = 64,in_channels_M = 64, out_channels = 32, kernel_size = 3, stride=2, padding=0, dilation=1, bias=True)\n",
    "        \n",
    "        \n",
    "        # linear layer\n",
    "#         self.hidden1 = torch.nn.Linear(2144*2, hidden_dim)\n",
    "#         self.hidden1 = torch.nn.Linear(1088, hidden_dim)\n",
    "        self.hidden1 = torch.nn.Linear(1632, hidden_dim)\n",
    "        \n",
    "#         self.hidden2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # output layer\n",
    "        self.outputLayer = torch.nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        # activation function\n",
    "        self.activationConv = torch.nn.ReLU() #max(0, x)\n",
    "#         self.activationConv = torch.nn.Tanh()\n",
    "    \n",
    "        # this works well.(comparing with relu)\n",
    "        self.activationLinear = torch.nn.Tanh()\n",
    "\n",
    "        # this is getting nan values\n",
    "#         self.activationLinear = torch.nn.ReLU()\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # input shape: [batch_size, channels, sequence_length]\n",
    "        # print(\"input shape: {0}\".format(x.shape))\n",
    "#         print(\"input to encoder: \")\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # convolution 1\n",
    "        # x -> conv -> act -> ouput\n",
    "        # shape should be: [batch_size, number of ouput channels (64), length of output from convolution]\n",
    "        \n",
    "        #conv to time\n",
    "        # normal convolution\n",
    "#         outputTimeConv = self.activationConv(self.conv1Time(x[:, 0, :].unsqueeze(1)))\n",
    "#         outputTimeConv = self.activationConv(self.conv1(x[:, 0, :].unsqueeze(1)))\n",
    "        # partial conv\n",
    "        # output, newMask = pconv1(data, mask)\n",
    "        outputTimeConv, maskTime = self.pconv1(x[:, 0, :].unsqueeze(1), x[:, 3, :].unsqueeze(1))\n",
    "        # activation function\n",
    "        outputTimeConv = self.activationConv(outputTimeConv)\n",
    "        \n",
    "        \n",
    "        # conv to magnitude\n",
    "#         outputMagConv = self.activationConv(self.conv1Mag(x[:, 1, :].unsqueeze(1)))\n",
    "#         outputMagConv = self.activationConv(self.conv1(x[:, 1, :].unsqueeze(1)))\n",
    "        \n",
    "        # partial conv\n",
    "        # output, newMask = pconv1(data, mask)\n",
    "        outputMagConv, maskMag = self.pconv1(x[:, 1, :].unsqueeze(1), x[:, 3, :].unsqueeze(1))\n",
    "        # activation function\n",
    "        outputMagConv = self.activationConv(outputMagConv)\n",
    "        \n",
    "        \n",
    "        # conv to mag error\n",
    "#         outputMagErrorConv = self.activationConv(self.conv1(x[:, 2, :].unsqueeze(1)))\n",
    "        \n",
    "        # partial conv\n",
    "        # output, newMask = pconv1(data, mask)\n",
    "        outputMagErrorConv, maskError = self.pconv1(x[:, 2, :].unsqueeze(1), x[:, 3, :].unsqueeze(1))\n",
    "        # activation function\n",
    "        outputMagErrorConv = self.activationConv(outputMagErrorConv)\n",
    "        \n",
    "#         print(\"output conv1 shape: {0}\".format(outputMagConv.shape))\n",
    "#         print(\"output conv1 shape: {0}\".format(outputTimeConv.shape))\n",
    "        \n",
    "        # convolution 2\n",
    "#         # shape should be: [batch_size, number of ouput channels (32), length of output from convolution]\n",
    "        \n",
    "        \n",
    "        # conv to time\n",
    "#         outputTimeConv = self.activationConv(self.conv2(outputTimeConv))\n",
    "        \n",
    "        # partial conv\n",
    "        outputTimeConv, maskTime = self.pconv2(outputTimeConv, maskTime)\n",
    "        outputTimeConv = self.activationConv(outputTimeConv)\n",
    "        \n",
    "        \n",
    "        # conv to flux\n",
    "#         outputMagConv = self.activationConv(self.conv2(outputMagConv))\n",
    "        # part conv\n",
    "        outputMagConv, maskMag = self.pconv2(outputMagConv, maskMag)\n",
    "        outputMagConv = self.activationConv(outputMagConv)\n",
    "        \n",
    "        # conv to mag error\n",
    "#         outputMagErrorConv = self.activationConv(self.conv2(outputMagErrorConv))\n",
    "        # partial conv\n",
    "        outputMagErrorConv, maskError = self.pconv2(outputMagErrorConv, maskError)\n",
    "        outputMagErrorConv = self.activationConv(outputMagErrorConv)\n",
    "        \n",
    "#         print(\"output conv2 shape: {0}\".format(outputTimeConv.shape))\n",
    "#         print(\"output conv2 shape: {0}\".format(outputMagConv.shape))\n",
    "        \n",
    "        # flatten ouput\n",
    "        # shape should be: [batch_size, -1]\n",
    "        outputMagConv = outputMagConv.view(outputMagConv.shape[0], -1)\n",
    "        \n",
    "        outputTimeConv = outputTimeConv.view(outputTimeConv.shape[0], -1)\n",
    "        \n",
    "        outputMagErrorConv = outputMagErrorConv.view(outputMagErrorConv.shape[0], -1)\n",
    "        \n",
    "#         print(\"output reshape: \", outputMagConv.shape)\n",
    "#         print(\"output reshape: \", outputTimeConv.shape)\n",
    "                \n",
    "        # concatenate 3 towers\n",
    "#         output = torch.cat((outputMagConv, outputTimeConv), 1)\n",
    "        output = torch.cat((outputTimeConv, outputMagConv, outputMagErrorConv), 1)\n",
    "#         print(\"concatenate output shape: \", output.shape)\n",
    "        \n",
    "        # x -> hidden1 -> activation\n",
    "#         print(\"before linear layer: {0}\".format(output.shape))\n",
    "        output = self.activationLinear(self.hidden1(output))\n",
    "        # Should be an activiation function here?\n",
    "#         output = (self.hidden1(output))\n",
    "        \n",
    "        output = self.outputLayer(output)\n",
    "        \n",
    "        # this should return the classification\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining parameters to Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of parameters\n",
    "# latentDim = 5\n",
    "# hiddenDim = 10\n",
    "# inputDim = 72\n",
    "\n",
    "latentDim = latentDim\n",
    "hiddenDim = hiddenDim\n",
    "inputDim = inputDim\n",
    "\n",
    "passband = passband\n",
    "\n",
    "num_classes = len(only_these_labels)\n",
    "\n",
    "\n",
    "# defining model\n",
    "model = Encoder(latent_dim = latentDim, hidden_dim = hiddenDim, input_dim = inputDim, num_classes = num_classes)\n",
    "\n",
    "# mdel to GPU\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (pconv1): PartialConv(\n",
      "    (input_conv): Conv1d(1, 64, kernel_size=(3,), stride=(2,))\n",
      "    (mask_conv): Conv1d(1, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
      "  )\n",
      "  (pconv2): PartialConv(\n",
      "    (input_conv): Conv1d(64, 32, kernel_size=(3,), stride=(2,))\n",
      "    (mask_conv): Conv1d(64, 32, kernel_size=(3,), stride=(2,), bias=False)\n",
      "  )\n",
      "  (hidden1): Linear(in_features=1632, out_features=100, bias=True)\n",
      "  (outputLayer): Linear(in_features=100, out_features=3, bias=True)\n",
      "  (activationConv): ReLU()\n",
      "  (activationLinear): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(\"input dimension: {0}\".format(len(list(trainLoader))))\n",
    "\n",
    "# # parameters number\n",
    "# count = 0\n",
    "\n",
    "# # # check model dimension\n",
    "# for name, param in model.state_dict().items():\n",
    "#     # name: str\n",
    "#     # param: Tensor\n",
    "# #     print(\"{0}: {1} \\n\".format(name, param.shape))\n",
    "# #     print(param.shape[0]*(param.shape[1] if len(param.shape)>1 else 1))\n",
    "# #     print(param.shape)\n",
    "#     count += param.shape[0]*(param.shape[1] if len(param.shape)>1 else 1)\n",
    "# # for param in model.parameters():\n",
    "    \n",
    "# print(\"number of parameters: \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it builds a mask for the deltas. It compares the next with the previous one element.\n",
    "# original mask: [1,1, 0, 0]\n",
    "# delta mask: [1, 0, 0]\n",
    "# The same results is got with original_mask[:, 1:]\n",
    "def generate_delta_mask(mask):\n",
    "    \n",
    "    # generate delta mask\n",
    "#     mask_delta = mask[:, 1:].type(torch.BoolTensor) & mask[:, :-1].type(torch.BoolTensor)\n",
    "    mask_delta = mask[:, 1:]\n",
    "    \n",
    "    return mask_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate delta time and flux\n",
    "# data = [batchSize, channels, [time, flux, err, mask], light curve]\n",
    "def generateDeltas(data, passBand):\n",
    "    \n",
    "    # work with delta time and magnitude\n",
    "    \n",
    "#     print(\"generate deltas input shape: {0}\".format(data.shape) )\n",
    "    # delta time\n",
    "    tmpDeltaTime = data[:, passBand, 0, 1:] - data[:, passBand, 0, :-1]\n",
    "\n",
    "#     print(\"generate deltas time shape: {0}\".format(tmpDeltaTime.shape) )\n",
    "\n",
    "#     # delta magnitude\n",
    "    tmpDeltaMagnitude = data[:, passBand, 1, 1:] - data[:, passBand, 1, :-1]\n",
    "#     print(\"generate deltas flux shape: {0}\".format(tmpDeltaMagnitude.shape))\n",
    "    \n",
    "    # delta errors\n",
    "    tmpDeltaMagError = data[:, passBand, 2, 1:] - data[:, passBand, 2, :-1]\n",
    "    \n",
    "    # delta mask\n",
    "    tmpMask = generate_delta_mask(data[:, passBand, 3,:])\n",
    "    \n",
    "    # concatenate tensors\n",
    "    dataToUse = torch.cat((tmpDeltaTime.unsqueeze(1), tmpDeltaMagnitude.unsqueeze(1), tmpDeltaMagError.unsqueeze(1), tmpMask.unsqueeze(1)), 1)\n",
    "#     print(\"data to use shape: {0}\".format(dataToUse.shape))\n",
    "    \n",
    "    # normalize data\n",
    "    # this was commented because it considerate that delta is already a normalization\n",
    "#     dataToUse = normalizeLightCurve(dataToUse)\n",
    "    \n",
    "    # returning data\n",
    "    return dataToUse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the labels to classes 0 to C-1\n",
    "\n",
    "def mapLabels(labels):\n",
    "\n",
    "    for i in range(len(only_these_labels)):\n",
    "        \n",
    "        labels[labels == only_these_labels[i]] = i \n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the training\n",
      "epoch:    0 / 2\n",
      "early stopping counter:  2\n",
      "New min test loss. Saving model\n",
      "saving losses\n",
      "saving f1 scores\n",
      "epoch:    1 / 2\n",
      "New min test loss. Saving model\n",
      "saving losses\n",
      "saving f1 scores\n",
      "training has finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.5)\n",
    "\n",
    "# loss function\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "\n",
    "# number of epochs\n",
    "epochs = epochs\n",
    "\n",
    "# loss\n",
    "train_loss = np.zeros((epochs,))\n",
    "test_loss = np.zeros((epochs,))\n",
    "\n",
    "# f1 scores\n",
    "f1Scores = np.zeros((epochs, ))\n",
    "\n",
    "# min global test loss \n",
    "minTestLossGlobalSoFar = float(\"inf\")\n",
    "\n",
    "# # # loss plot\n",
    "# if it is not cluster\n",
    "if (not trainingOnGuanaco) or (not trainWithJustPython):\n",
    "    \n",
    "    # add f1 and loss plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (7, 3), tight_layout = True)\n",
    "    # # fig, ax = plt.subplots()\n",
    "    \n",
    "    # error\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[0].set_ylabel(\"Error\")\n",
    "    \n",
    "    \n",
    "    # f1 score\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].set_ylabel(\"F1 score\")\n",
    "    \n",
    "\n",
    "# early stopping\n",
    "prior_test_error = 0\n",
    "count_early_stop = 0\n",
    "threshold_early_stop = 20\n",
    "\n",
    "\n",
    "print(\"starting the training\")\n",
    "\n",
    "\n",
    "# epoch\n",
    "for nepoch in range(epochs):\n",
    "        \n",
    "    print(\"epoch:    {0} / {1}\".format(nepoch, epochs))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    ######## Train ###########\n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    for data_ in trainLoader:\n",
    "        \n",
    "        data = data_[0]\n",
    "        labels = data_[1].cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # this take the deltas (time and magnitude)\n",
    "        data = generateDeltas(data, passband).type(torch.FloatTensor).cuda()\n",
    "\n",
    "        # get model output\n",
    "        outputs = model.forward(data)\n",
    "        \n",
    "        # loss function\n",
    "        loss = lossFunction(outputs, mapLabels(labels).cuda())\n",
    "        \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add loss value (of the currrent minibatch)\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "\n",
    "    # get epoch loss value\n",
    "    train_loss[nepoch] = epoch_train_loss / train_size\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### Validation ########\n",
    "    \n",
    "    epoch_test_loss = 0\n",
    "    f1Score = 0\n",
    "    batchCounter = 0\n",
    "    \n",
    "    # minibatches\n",
    "    for data_ in validationLoader:\n",
    "        \n",
    "        data = data_[0]\n",
    "        labels = data_[1].cuda()\n",
    "        \n",
    "        data = generateDeltas(data, passband).type(torch.FloatTensor).cuda()\n",
    "        \n",
    "        outputs = model.forward(data)\n",
    "        \n",
    "        # loss function\n",
    "        loss = lossFunction(outputs, mapLabels(labels).cuda())\n",
    "    \n",
    "        #  store minibatch loss value\n",
    "        epoch_test_loss += loss.item()\n",
    "        \n",
    "        # f1 score\n",
    "        f1Score += f1_score(mapLabels(labels).cpu().numpy(), torch.argmax(outputs, 1).cpu().numpy(), average = \"micro\")\n",
    "        \n",
    "        # batch counter\n",
    "        batchCounter += 1\n",
    "    \n",
    "    # get epoch test loss value\n",
    "    test_loss[nepoch] = epoch_test_loss / validation_size\n",
    "    \n",
    "    # get epoch f1 score\n",
    "    f1Scores[nepoch] = f1Score / batchCounter\n",
    "    \n",
    "    # plot loss values\n",
    "    # if it's not cluster\n",
    "    if (not trainingOnGuanaco) or (not trainWithJustPython):\n",
    "\n",
    "        # loss values\n",
    "        ax[0].plot(train_loss[0: nepoch], label = \"train\", linewidth = 3, c = \"red\") \n",
    "        ax[0].plot(test_loss[0: nepoch], label = \"test\", linestyle = \"--\", linewidth = 3, c = \"green\")\n",
    "        \n",
    "        # f1 score values\n",
    "        ax[1].plot(f1Scores[0: nepoch], linewidth = 3, c = \"green\")\n",
    "        \n",
    "        # plot\n",
    "        fig.canvas.draw()\n",
    "    \n",
    "    \n",
    "    #### Early stopping #####\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if new test loss is greater than the older one\n",
    "    count_early_stop += 1\n",
    "    if epoch_test_loss > prior_test_error:\n",
    "        count_early_stop += 1\n",
    "        print(\"early stopping counter: \", count_early_stop)\n",
    "    else: \n",
    "        count_early_stop = 0\n",
    "    \n",
    "    # update prior test error\n",
    "    prior_test_error = epoch_test_loss\n",
    "    \n",
    "    # analyze early stopping\n",
    "    if count_early_stop > threshold_early_stop:\n",
    "        \n",
    "        print(\"Early stopping in epoch: \", nepoch)\n",
    "        text_file = open(\"experiments/\" + number_experiment + \"/earlyStopping.txt\", \"w\")\n",
    "        metricsText = \"Epoch: {0}\\n ES counter: {1}\\n, Reconstruction test error: {2}\".format(nepoch, count_early_stop, epoch_test_loss)\n",
    "        text_file.write(metricsText)\n",
    "        text_file.close()\n",
    "        break\n",
    "        \n",
    "    #### Saving best model ####\n",
    "    \n",
    "    # if epoch test loss is smaller than global min\n",
    "    if (epoch_test_loss/validation_size) < minTestLossGlobalSoFar:\n",
    "        \n",
    "        print(\"New min test loss. Saving model\")\n",
    "#         print(\"old: \", minTestLossGlobalSoFar)\n",
    "#         print(\"new: \", epoch_test_loss)\n",
    "        \n",
    "        # save model\n",
    "        torch.save(model.state_dict(), pathToSaveModel)\n",
    "        \n",
    "        # update global min\n",
    "        minTestLossGlobalSoFar = epoch_test_loss\n",
    "        \n",
    "        # write metrics\n",
    "        text_file = open(\"experiments/\" + number_experiment + \"/bestScoresModelTraining.txt\", \"w\")\n",
    "        metricsText = \"Epoch: {0}\\n Reconstruction test error: {1}\".format(nepoch, minTestLossGlobalSoFar)\n",
    "        text_file.write(metricsText)\n",
    "        text_file.close()\n",
    "\n",
    "        \n",
    "        \n",
    "    # save losses\n",
    "    print(\"saving losses\")\n",
    "    losses = np.asarray([train_loss, test_loss]).T\n",
    "    np.savetxt(\"experiments/\" + number_experiment + \"/training_losses.csv\", losses, delimiter=\",\")\n",
    "    \n",
    "\n",
    "    # save f1 scores\n",
    "    print(\"saving f1 scores\")\n",
    "    np.savetxt(\"experiments/\" + number_experiment + \"/f1Scores.csv\", f1Scores, delimiter=\",\")\n",
    "\n",
    "# final message\n",
    "print(\"training has finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving confusion matrix scores\n",
      "saving clasification report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# get y true and labels\n",
    "\n",
    "predictions = np.zeros(shape = (0,))\n",
    "labels_ = np.zeros(shape = (0,))\n",
    "\n",
    "# minibatches\n",
    "for data_ in validationLoader:\n",
    "        \n",
    "    data = data_[0].cuda()\n",
    "    labels = data_[1].cuda()\n",
    "\n",
    "    data = generateDeltas(data, passband).type(torch.FloatTensor).cuda()\n",
    "\n",
    "    outputs = model.forward(data)\n",
    "    \n",
    "    prediction = torch.argmax(outputs, 1).cpu().numpy()\n",
    "\n",
    "    label = mapLabels(labels).cpu().numpy()\n",
    "    \n",
    "    predictions = np.append(predictions, prediction)\n",
    "    labels_ = np.append(labels_, label)\n",
    "    \n",
    "\n",
    "# getting confusion amtrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(labels_, predictions)\n",
    "\n",
    "print(\"saving confusion matrix scores\")\n",
    "np.savetxt(\"experiments/\" + number_experiment + \"/confusionMatrix.csv\", cm, delimiter=\",\")\n",
    "\n",
    "\n",
    "# classification report\n",
    "print(\"saving clasification report\")\n",
    "text_file = open(\"experiments/\" + number_experiment + \"/clasificationReport.txt\", \"w\")\n",
    "text = classification_report(labels_, predictions)\n",
    "text_file.write(text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop execution if it's on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Exit from code, because we are in cluster or running locally. Training has finished.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Exit from code, because we are in cluster or running locally. Training has finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "if  trainingOnGuanaco or trainWithJustPython:\n",
    "\n",
    "    sys.exit(\"Exit from code, because we are in cluster or running locally. Training has finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load losses array\n",
    "losses = pd.read_csv(\"/home/leo/Desktop/thesis/work/thesis/experiments/\"+ number_experiment + \"/training_losses.csv\")\n",
    "\n",
    "# f1 scores\n",
    "f1Scores = pd.read_csv(\"/home/leo/Desktop/thesis/work/thesis/experiments/\"+ number_experiment + \"/f1Scores.csv\")\n",
    "\n",
    "# plot losses\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10,4), tight_layout = True)\n",
    "\n",
    "# loss\n",
    "ax[0].set_xlabel(\"NÂ° epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].plot(losses.iloc[:, 0], label = \"train\")\n",
    "ax[0].plot(losses.iloc[:, 1], label = \"validation\")\n",
    "ax[0].scatter(429, 0.000845159766508081, c = \"r\", linewidths = 10)\n",
    "ax[0].legend()\n",
    "\n",
    "# f1 scores\n",
    "ax[1].set_xlabel(\"NÂ° epoch\")\n",
    "ax[1].set_ylabel(\"F1 score\")\n",
    "ax[1].plot(f1Scores)\n",
    "ax[1].scatter(429, f1Scores.iloc[428], c = \"r\", linewidths = 10)\n",
    "\n",
    "\n",
    "# ax[0].scatter(429, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red point is the epoch with lower value in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "cm = pd.read_csv('./experiments/' + number_experiment + '/confusionMatrix.csv', header = None) \n",
    "\n",
    "display(cm)\n",
    "\n",
    "sn.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "!cat ./experiments/6/clasificationReport.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
