{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be building a model that will complete a sentence based on a word or a few characters passed into it\n",
    "\n",
    "The model will be fed with a word and will predict what the next character in the sentence will be. This process will repeat itself until we generate a sentence of our desired length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hello     \n",
      "Target Sequence: ello      \n",
      "Input Sequence: how are yo\n",
      "Target Sequence: ow are you\n",
      "(2, 10, 10)\n",
      "[[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# create all sentences i want to predict\n",
    "text = [\"hello\", \"how are you\"]\n",
    "# text = ['hey how are you','good i am fine','have a nice day']\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "# print(int2char)\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "# print(char2int)\n",
    "\n",
    "\n",
    "# Finding the length of the longest string in our data\n",
    "maxlen = len(max(text, key=len))\n",
    "\n",
    "# Padding\n",
    "\n",
    "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of\n",
    "# the sentence matches the length of the longest sentence\n",
    "for i in range(len(text)):\n",
    "  while len(text[i])<maxlen:\n",
    "      text[i] += ' '\n",
    "        \n",
    "# Creating lists that will hold our input and target sequences\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # Remove last character for input sequence\n",
    "  input_seq.append(text[i][:-1])\n",
    "    \n",
    "    # Remove first character for target sequence\n",
    "  target_seq.append(text[i][1:])\n",
    "  print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))\n",
    "    \n",
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",
    "    \n",
    "# print(input_seq)\n",
    "\n",
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features\n",
    "\n",
    "# Input shape --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n",
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "\n",
    "print(input_seq.shape)\n",
    "print(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "# model.to(\"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100............. Loss: 0.7788\n",
      "Epoch: 20/100............. Loss: 0.2903\n",
      "Epoch: 30/100............. Loss: 0.0884\n",
      "Epoch: 40/100............. Loss: 0.0368\n",
      "Epoch: 50/100............. Loss: 0.0206\n",
      "Epoch: 60/100............. Loss: 0.0141\n",
      "Epoch: 70/100............. Loss: 0.0109\n",
      "Epoch: 80/100............. Loss: 0.0090\n",
      "Epoch: 90/100............. Loss: 0.0076\n",
      "Epoch: 100/100............. Loss: 0.0067\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "#     input_seq.to(device)\n",
    "    output, hidden = model(input_seq)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in the model and character as arguments and returns the next character prediction and hidden state\n",
    "def predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "#     character.to(device)\n",
    "    \n",
    "    out, hidden = model(character)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the desired output length and input characters as arguments, returning the produced sentence\n",
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helloollooolooo'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model, 15, 'hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN with sinus function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f354010e668>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x_ = np.arange(0, 4)\n",
    "# print(x)\n",
    "\n",
    "inputs = x_[:-1]\n",
    "# print(data)\n",
    "\n",
    "y = np.sin(x_)\n",
    "# print(y)\n",
    "\n",
    "targets = y[1:]\n",
    "# print(targets)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# ax.scatter(x, y, color = \"blue\", label = \"target\", alpha = 0.5)\n",
    "ax.plot(x_, y, color = \"blue\", alpha = 0.4, label = \"original\")\n",
    "ax.plot(inputs, targets, label =\"targets\", color = \"red\", alpha = 0.2)\n",
    "ax.scatter(inputs, targets, color = \"red\")\n",
    "# ax.plot(data, targets, color = \"red\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to tensor\n",
    "inputs = torch.Tensor(inputs)\n",
    "targets = torch.Tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters of model\n",
    "input_size = 1\n",
    "hidden_size = 1\n",
    "batch_size = 1 \n",
    "sequence_length = 1\n",
    "output_size = 1\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "# create model\n",
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "#         self.rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size, batch_first = True)\n",
    "        self.rnn = nn.LSTM(input_size = input_size, hidden_size = hidden_size, batch_first = True)\n",
    "    \n",
    "                         \n",
    "    def forward(self, hidden, x):\n",
    "        \n",
    "        # reshape input\n",
    "        x = x.view(batch_size, sequence_length, input_size)\n",
    "        \n",
    "#         print(\"x input: \", x.shape)\n",
    "        \n",
    "        # forward\n",
    "        out, hidden =  self.rnn(x, hidden)\n",
    "        \n",
    "#         print(\"output shape: \", out.view(-1, output_size).shape)\n",
    "        return hidden, out.view(-1, output_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "#         return Variable(torch.zeros(1, batch_size, hidden_size))\n",
    "        return (torch.zeros(1, batch_size, hidden_size))\n",
    "#         return (torch.zeros(batch_size, hidden_size).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "\n",
    "criterion = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 1, 1), got (1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-da0ab46dc79a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# forward with input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-c6e34629a8a4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#         print(\"output shape: \", out.view(-1, output_size).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0;32m--> 494\u001b[0;31m                                'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[1;32m    495\u001b[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n\u001b[1;32m    496\u001b[0m                                'Expected hidden[1] size {}, got {}')\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# type: (Tensor, Tuple[int, int, int], str) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 1, 1), got (1, 1)"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "# iterate by epochs\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # hidden state to zero (nothing before)\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    # present each input\n",
    "    for input_, label in zip(inputs, targets):\n",
    "    \n",
    "#         print(\"hidden shape: \", hidden.shape)\n",
    "#         print(\"input shape: \", input_.shape)\n",
    "        \n",
    "        # forward with input\n",
    "        hidden, output = model(hidden, input_) \n",
    "\n",
    "        # loss \n",
    "        loss += criterion(output, label)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"epoch: \", epoch, \" - loss: \" + str(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 1, 1), got (1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-4095641f2069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#     print(outputs.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-c6e34629a8a4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#         print(\"output shape: \", out.view(-1, output_size).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0;32m--> 494\u001b[0;31m                                'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[1;32m    495\u001b[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n\u001b[1;32m    496\u001b[0m                                'Expected hidden[1] size {}, got {}')\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# type: (Tensor, Tuple[int, int, int], str) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 1, 1), got (1, 1)"
     ]
    }
   ],
   "source": [
    "# do predictions\n",
    "hidden = model.init_hidden()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# print(len(x_))\n",
    "for x in range(len(x_)):\n",
    "    \n",
    "    hidden, outputs = model.forward(hidden, torch.Tensor([x]).squeeze())\n",
    "    \n",
    "#     print(outputs.item())\n",
    "    predictions.append(outputs.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f354018d048>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot predicionts\n",
    "fig, ax = plt.subplots()\n",
    "# print(x_)\n",
    "ax.plot(x_, predictions, label = \"predictions\")\n",
    "ax.plot(x_[1:]-1, y[1:], label = \"original\")\n",
    "ax.scatter(x_[:-1], y[1:], label = \"targets\", color = \"red\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN with hello "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# unique characters\n",
    "idx2char = ['h', 'e', 'l', 'o', 'o', 'o']\n",
    "# h: 0\n",
    "# e: 1\n",
    "# l: 2\n",
    "# o: 3\n",
    "\n",
    "# hell\n",
    "x_data = [0, 1, 2, 2];\n",
    "\n",
    "# length 4 because there is 4 differents characters\n",
    "one_hot_look = [\n",
    "    [1, 0, 0, 0], # h\n",
    "    [0, 1, 0, 0], # e\n",
    "    [0, 0, 1, 0], # l\n",
    "    [0, 0, 0, 1], # o\n",
    "]\n",
    "\n",
    "# print(one_hot_look[3])\n",
    "\n",
    "# ello \n",
    "# (because text is like a time serie and I want to predict the nex character, so the target will be the next character of each char in x_data)\n",
    "y_data = [1, 2, 2, 3]\n",
    "\n",
    "# transform x-data to one-hot encoding\n",
    "x_one_hot = [ one_hot_look[x] for x in x_data]\n",
    "# y_one_hot = [ one_hot_look[x] for x in y_data]\n",
    "\n",
    "# print(y_one_hot)\n",
    "# data to tensor\n",
    "inputs = Variable(torch.Tensor(x_one_hot))\n",
    "targets = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "input_size = len(one_hot_look[0])\n",
    "hidden_size = input_size\n",
    "batch_size = 1\n",
    "sequence_length = 4\n",
    "num_classes = input_size\n",
    "num_layers = 1\n",
    "\n",
    "# create model\n",
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size, batch_first = True)\n",
    "                         \n",
    "    def forward(self, hidden, x):\n",
    "        \n",
    "#         print(x.shape)\n",
    "        \n",
    "        # reshape input\n",
    "        x = x.view(batch_size, sequence_length, input_size)\n",
    "        \n",
    "#         print(\"input to RNN shape: \", x.shape)\n",
    "        \n",
    "        # forward\n",
    "        out, hidden =  self.rnn(x, hidden)\n",
    "        \n",
    "        return hidden, out.view(-1, num_classes)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        return Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
    "\n",
    "# instantiate model\n",
    "model = RNN()\n",
    "\n",
    "# define loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  | loss:  1.5757358074188232\n",
      "epoch:  1  | loss:  1.3357949256896973\n",
      "epoch:  2  | loss:  1.1334294080734253\n",
      "epoch:  3  | loss:  0.9961792230606079\n",
      "epoch:  4  | loss:  0.9124128818511963\n",
      "epoch:  5  | loss:  0.8545787334442139\n",
      "epoch:  6  | loss:  0.8063748478889465\n",
      "epoch:  7  | loss:  0.7623152732849121\n",
      "epoch:  8  | loss:  0.723191499710083\n",
      "epoch:  9  | loss:  0.6885234117507935\n",
      "epoch:  10  | loss:  0.6555707454681396\n",
      "epoch:  11  | loss:  0.6261085867881775\n",
      "epoch:  12  | loss:  0.6030715703964233\n",
      "epoch:  13  | loss:  0.5893162488937378\n",
      "epoch:  14  | loss:  0.5797090530395508\n",
      "epoch:  15  | loss:  0.5693175792694092\n",
      "epoch:  16  | loss:  0.5605154633522034\n",
      "epoch:  17  | loss:  0.5522721409797668\n",
      "epoch:  18  | loss:  0.5439419150352478\n",
      "epoch:  19  | loss:  0.5368401408195496\n",
      "epoch:  20  | loss:  0.5318097472190857\n",
      "epoch:  21  | loss:  0.5271846652030945\n",
      "epoch:  22  | loss:  0.5231980085372925\n",
      "epoch:  23  | loss:  0.5205021500587463\n",
      "epoch:  24  | loss:  0.5178208351135254\n",
      "epoch:  25  | loss:  0.5150336623191833\n",
      "epoch:  26  | loss:  0.5130512118339539\n",
      "epoch:  27  | loss:  0.5112521648406982\n",
      "epoch:  28  | loss:  0.5092743039131165\n",
      "epoch:  29  | loss:  0.5078673362731934\n",
      "epoch:  30  | loss:  0.5065380334854126\n",
      "epoch:  31  | loss:  0.5050885677337646\n",
      "epoch:  32  | loss:  0.5040152072906494\n",
      "epoch:  33  | loss:  0.503104567527771\n",
      "epoch:  34  | loss:  0.5020298361778259\n",
      "epoch:  35  | loss:  0.5011850595474243\n",
      "epoch:  36  | loss:  0.5004876255989075\n",
      "epoch:  37  | loss:  0.4997151792049408\n",
      "epoch:  38  | loss:  0.4990379810333252\n",
      "epoch:  39  | loss:  0.4985092878341675\n",
      "epoch:  40  | loss:  0.49790942668914795\n",
      "epoch:  41  | loss:  0.4973229169845581\n",
      "epoch:  42  | loss:  0.4968602657318115\n",
      "epoch:  43  | loss:  0.49638745188713074\n",
      "epoch:  44  | loss:  0.49589306116104126\n",
      "epoch:  45  | loss:  0.49548202753067017\n",
      "epoch:  46  | loss:  0.49509039521217346\n",
      "epoch:  47  | loss:  0.4946604371070862\n",
      "epoch:  48  | loss:  0.49428391456604004\n",
      "epoch:  49  | loss:  0.49394094944000244\n",
      "epoch:  50  | loss:  0.49357205629348755\n",
      "epoch:  51  | loss:  0.4932286739349365\n",
      "epoch:  52  | loss:  0.4929233193397522\n",
      "epoch:  53  | loss:  0.4925984740257263\n",
      "epoch:  54  | loss:  0.49228596687316895\n",
      "epoch:  55  | loss:  0.49200674891471863\n",
      "epoch:  56  | loss:  0.4917182922363281\n",
      "epoch:  57  | loss:  0.4914346933364868\n",
      "epoch:  58  | loss:  0.4911787509918213\n",
      "epoch:  59  | loss:  0.4909183979034424\n",
      "epoch:  60  | loss:  0.49066001176834106\n",
      "epoch:  61  | loss:  0.49042364954948425\n",
      "epoch:  62  | loss:  0.4901861250400543\n",
      "epoch:  63  | loss:  0.4899502992630005\n",
      "epoch:  64  | loss:  0.4897317886352539\n",
      "epoch:  65  | loss:  0.48951321840286255\n",
      "epoch:  66  | loss:  0.48929649591445923\n",
      "epoch:  67  | loss:  0.4890935719013214\n",
      "epoch:  68  | loss:  0.4888911247253418\n",
      "epoch:  69  | loss:  0.48869121074676514\n",
      "epoch:  70  | loss:  0.48850229382514954\n",
      "epoch:  71  | loss:  0.4883137047290802\n",
      "epoch:  72  | loss:  0.4881284236907959\n",
      "epoch:  73  | loss:  0.4879515767097473\n",
      "epoch:  74  | loss:  0.4877752363681793\n",
      "epoch:  75  | loss:  0.48760277032852173\n",
      "epoch:  76  | loss:  0.48743683099746704\n",
      "epoch:  77  | loss:  0.48727139830589294\n",
      "epoch:  78  | loss:  0.48711007833480835\n",
      "epoch:  79  | loss:  0.48695361614227295\n",
      "epoch:  80  | loss:  0.48679792881011963\n",
      "epoch:  81  | loss:  0.4866466224193573\n",
      "epoch:  82  | loss:  0.4864986538887024\n",
      "epoch:  83  | loss:  0.4863518476486206\n",
      "epoch:  84  | loss:  0.4862091541290283\n",
      "epoch:  85  | loss:  0.48606884479522705\n",
      "epoch:  86  | loss:  0.4859302341938019\n",
      "epoch:  87  | loss:  0.48579519987106323\n",
      "epoch:  88  | loss:  0.48566192388534546\n",
      "epoch:  89  | loss:  0.4855305850505829\n",
      "epoch:  90  | loss:  0.48540231585502625\n",
      "epoch:  91  | loss:  0.4852755069732666\n",
      "epoch:  92  | loss:  0.4851510226726532\n",
      "epoch:  93  | loss:  0.4850287437438965\n",
      "epoch:  94  | loss:  0.48490792512893677\n",
      "epoch:  95  | loss:  0.4847894012928009\n",
      "epoch:  96  | loss:  0.4846726655960083\n",
      "epoch:  97  | loss:  0.48455744981765747\n",
      "epoch:  98  | loss:  0.48444443941116333\n",
      "epoch:  99  | loss:  0.484332799911499\n",
      "epoch:  100  | loss:  0.4842228889465332\n",
      "epoch:  101  | loss:  0.4841146469116211\n",
      "epoch:  102  | loss:  0.4840078353881836\n",
      "epoch:  103  | loss:  0.4839026927947998\n",
      "epoch:  104  | loss:  0.4837989807128906\n",
      "epoch:  105  | loss:  0.4836965799331665\n",
      "epoch:  106  | loss:  0.4835958182811737\n",
      "epoch:  107  | loss:  0.4834962487220764\n",
      "epoch:  108  | loss:  0.4833981394767761\n",
      "epoch:  109  | loss:  0.4833012819290161\n",
      "epoch:  110  | loss:  0.4832056760787964\n",
      "epoch:  111  | loss:  0.4831114411354065\n",
      "epoch:  112  | loss:  0.4830183982849121\n",
      "epoch:  113  | loss:  0.4829264283180237\n",
      "epoch:  114  | loss:  0.4828357994556427\n",
      "epoch:  115  | loss:  0.4827461540699005\n",
      "epoch:  116  | loss:  0.4826577603816986\n",
      "epoch:  117  | loss:  0.4825704097747803\n",
      "epoch:  118  | loss:  0.4824841022491455\n",
      "epoch:  119  | loss:  0.4823988974094391\n",
      "epoch:  120  | loss:  0.4823145866394043\n",
      "epoch:  121  | loss:  0.48223140835762024\n",
      "epoch:  122  | loss:  0.4821491539478302\n",
      "epoch:  123  | loss:  0.48206794261932373\n",
      "epoch:  124  | loss:  0.4819876253604889\n",
      "epoch:  125  | loss:  0.4819082021713257\n",
      "epoch:  126  | loss:  0.48182976245880127\n",
      "epoch:  127  | loss:  0.4817521870136261\n",
      "epoch:  128  | loss:  0.481675386428833\n",
      "epoch:  129  | loss:  0.48159950971603394\n",
      "epoch:  130  | loss:  0.4815244674682617\n",
      "epoch:  131  | loss:  0.48145031929016113\n",
      "epoch:  132  | loss:  0.4813769459724426\n",
      "epoch:  133  | loss:  0.4813042879104614\n",
      "epoch:  134  | loss:  0.48123249411582947\n",
      "epoch:  135  | loss:  0.4811614155769348\n",
      "epoch:  136  | loss:  0.48109114170074463\n",
      "epoch:  137  | loss:  0.48102158308029175\n",
      "epoch:  138  | loss:  0.4809526801109314\n",
      "epoch:  139  | loss:  0.48088452219963074\n",
      "epoch:  140  | loss:  0.4808170795440674\n",
      "epoch:  141  | loss:  0.4807504117488861\n",
      "epoch:  142  | loss:  0.4806843400001526\n",
      "epoch:  143  | loss:  0.4806188941001892\n",
      "epoch:  144  | loss:  0.48055410385131836\n",
      "epoch:  145  | loss:  0.4804900288581848\n",
      "epoch:  146  | loss:  0.480426549911499\n",
      "epoch:  147  | loss:  0.480363667011261\n",
      "epoch:  148  | loss:  0.4803014397621155\n",
      "epoch:  149  | loss:  0.4802398085594177\n",
      "epoch:  150  | loss:  0.4801786541938782\n",
      "epoch:  151  | loss:  0.4801182150840759\n",
      "epoch:  152  | loss:  0.4800582826137543\n",
      "epoch:  153  | loss:  0.4799988865852356\n",
      "epoch:  154  | loss:  0.47994017601013184\n",
      "epoch:  155  | loss:  0.4798819422721863\n",
      "epoch:  156  | loss:  0.47982415556907654\n",
      "epoch:  157  | loss:  0.4797670543193817\n",
      "epoch:  158  | loss:  0.47971031069755554\n",
      "epoch:  159  | loss:  0.4796541631221771\n",
      "epoch:  160  | loss:  0.4795985519886017\n",
      "epoch:  161  | loss:  0.47954338788986206\n",
      "epoch:  162  | loss:  0.47948867082595825\n",
      "epoch:  163  | loss:  0.4794345498085022\n",
      "epoch:  164  | loss:  0.4793807566165924\n",
      "epoch:  165  | loss:  0.47932755947113037\n",
      "epoch:  166  | loss:  0.47927480936050415\n",
      "epoch:  167  | loss:  0.47922247648239136\n",
      "epoch:  168  | loss:  0.4791705906391144\n",
      "epoch:  169  | loss:  0.47911912202835083\n",
      "epoch:  170  | loss:  0.47906816005706787\n",
      "epoch:  171  | loss:  0.47901755571365356\n",
      "epoch:  172  | loss:  0.4789673686027527\n",
      "epoch:  173  | loss:  0.4789176881313324\n",
      "epoch:  174  | loss:  0.47886836528778076\n",
      "epoch:  175  | loss:  0.47881948947906494\n",
      "epoch:  176  | loss:  0.4787709712982178\n",
      "epoch:  177  | loss:  0.47872281074523926\n",
      "epoch:  178  | loss:  0.47867506742477417\n",
      "epoch:  179  | loss:  0.4786277115345001\n",
      "epoch:  180  | loss:  0.4785807728767395\n",
      "epoch:  181  | loss:  0.47853416204452515\n",
      "epoch:  182  | loss:  0.47848793864250183\n",
      "epoch:  183  | loss:  0.47844207286834717\n",
      "epoch:  184  | loss:  0.47839659452438354\n",
      "epoch:  185  | loss:  0.4783514738082886\n",
      "epoch:  186  | loss:  0.4783066213130951\n",
      "epoch:  187  | loss:  0.47826212644577026\n",
      "epoch:  188  | loss:  0.47821804881095886\n",
      "epoch:  189  | loss:  0.47817426919937134\n",
      "epoch:  190  | loss:  0.4781308174133301\n",
      "epoch:  191  | loss:  0.47808772325515747\n",
      "epoch:  192  | loss:  0.47804492712020874\n",
      "epoch:  193  | loss:  0.4780023992061615\n",
      "epoch:  194  | loss:  0.4779602289199829\n",
      "epoch:  195  | loss:  0.4779183864593506\n",
      "epoch:  196  | loss:  0.4778769016265869\n",
      "epoch:  197  | loss:  0.47783559560775757\n",
      "epoch:  198  | loss:  0.47779467701911926\n",
      "epoch:  199  | loss:  0.47775405645370483\n",
      "epoch:  200  | loss:  0.4777136743068695\n",
      "epoch:  201  | loss:  0.47767359018325806\n",
      "epoch:  202  | loss:  0.47763383388519287\n",
      "epoch:  203  | loss:  0.4775943160057068\n",
      "epoch:  204  | loss:  0.47755515575408936\n",
      "epoch:  205  | loss:  0.47751617431640625\n",
      "epoch:  206  | loss:  0.4774775207042694\n",
      "epoch:  207  | loss:  0.47743913531303406\n",
      "epoch:  208  | loss:  0.4774010181427002\n",
      "epoch:  209  | loss:  0.47736310958862305\n",
      "epoch:  210  | loss:  0.4773254692554474\n",
      "epoch:  211  | loss:  0.4772881269454956\n",
      "epoch:  212  | loss:  0.47725099325180054\n",
      "epoch:  213  | loss:  0.4772142171859741\n",
      "epoch:  214  | loss:  0.47717759013175964\n",
      "epoch:  215  | loss:  0.47714123129844666\n",
      "epoch:  216  | loss:  0.47710514068603516\n",
      "epoch:  217  | loss:  0.4770691990852356\n",
      "epoch:  218  | loss:  0.4770335853099823\n",
      "epoch:  219  | loss:  0.4769982099533081\n",
      "epoch:  220  | loss:  0.476963073015213\n",
      "epoch:  221  | loss:  0.47692811489105225\n",
      "epoch:  222  | loss:  0.4768933653831482\n",
      "epoch:  223  | loss:  0.47685888409614563\n",
      "epoch:  224  | loss:  0.4768245816230774\n",
      "epoch:  225  | loss:  0.47679051756858826\n",
      "epoch:  226  | loss:  0.4767566919326782\n",
      "epoch:  227  | loss:  0.47672316431999207\n",
      "epoch:  228  | loss:  0.47668975591659546\n",
      "epoch:  229  | loss:  0.4766564965248108\n",
      "epoch:  230  | loss:  0.4766234755516052\n",
      "epoch:  231  | loss:  0.47659075260162354\n",
      "epoch:  232  | loss:  0.4765581488609314\n",
      "epoch:  233  | loss:  0.47652578353881836\n",
      "epoch:  234  | loss:  0.4764936566352844\n",
      "epoch:  235  | loss:  0.47646164894104004\n",
      "epoch:  236  | loss:  0.47642993927001953\n",
      "epoch:  237  | loss:  0.476398229598999\n",
      "epoch:  238  | loss:  0.47636687755584717\n",
      "epoch:  239  | loss:  0.47633564472198486\n",
      "epoch:  240  | loss:  0.4763046205043793\n",
      "epoch:  241  | loss:  0.4762738347053528\n",
      "epoch:  242  | loss:  0.47624310851097107\n",
      "epoch:  243  | loss:  0.47621268033981323\n",
      "epoch:  244  | loss:  0.47618240118026733\n",
      "epoch:  245  | loss:  0.4761521816253662\n",
      "epoch:  246  | loss:  0.4761222302913666\n",
      "epoch:  247  | loss:  0.47609251737594604\n",
      "epoch:  248  | loss:  0.4760628938674927\n",
      "epoch:  249  | loss:  0.47603344917297363\n",
      "epoch:  250  | loss:  0.4760042428970337\n",
      "epoch:  251  | loss:  0.4759751558303833\n",
      "epoch:  252  | loss:  0.47594621777534485\n",
      "epoch:  253  | loss:  0.4759174585342407\n",
      "epoch:  254  | loss:  0.4758888781070709\n",
      "epoch:  255  | loss:  0.4758604168891907\n",
      "epoch:  256  | loss:  0.47583216428756714\n",
      "epoch:  257  | loss:  0.47580406069755554\n",
      "epoch:  258  | loss:  0.4757760763168335\n",
      "epoch:  259  | loss:  0.47574830055236816\n",
      "epoch:  260  | loss:  0.47572070360183716\n",
      "epoch:  261  | loss:  0.47569310665130615\n",
      "epoch:  262  | loss:  0.47566577792167664\n",
      "epoch:  263  | loss:  0.47563862800598145\n",
      "epoch:  264  | loss:  0.4756115674972534\n",
      "epoch:  265  | loss:  0.47558459639549255\n",
      "epoch:  266  | loss:  0.4755578637123108\n",
      "epoch:  267  | loss:  0.47553128004074097\n",
      "epoch:  268  | loss:  0.4755048155784607\n",
      "epoch:  269  | loss:  0.4754784405231476\n",
      "epoch:  270  | loss:  0.4754522144794464\n",
      "epoch:  271  | loss:  0.47542619705200195\n",
      "epoch:  272  | loss:  0.47540032863616943\n",
      "epoch:  273  | loss:  0.4753745198249817\n",
      "epoch:  274  | loss:  0.4753488600254059\n",
      "epoch:  275  | loss:  0.47532325983047485\n",
      "epoch:  276  | loss:  0.4752979576587677\n",
      "epoch:  277  | loss:  0.47527265548706055\n",
      "epoch:  278  | loss:  0.4752475917339325\n",
      "epoch:  279  | loss:  0.47522255778312683\n",
      "epoch:  280  | loss:  0.4751976728439331\n",
      "epoch:  281  | loss:  0.4751729369163513\n",
      "epoch:  282  | loss:  0.4751483201980591\n",
      "epoch:  283  | loss:  0.47512388229370117\n",
      "epoch:  284  | loss:  0.47509947419166565\n",
      "epoch:  285  | loss:  0.47507524490356445\n",
      "epoch:  286  | loss:  0.4750511050224304\n",
      "epoch:  287  | loss:  0.47502708435058594\n",
      "epoch:  288  | loss:  0.475003182888031\n",
      "epoch:  289  | loss:  0.474979430437088\n",
      "epoch:  290  | loss:  0.4749557375907898\n",
      "epoch:  291  | loss:  0.4749322533607483\n",
      "epoch:  292  | loss:  0.4749087691307068\n",
      "epoch:  293  | loss:  0.4748854637145996\n",
      "epoch:  294  | loss:  0.47486230731010437\n",
      "epoch:  295  | loss:  0.4748392105102539\n",
      "epoch:  296  | loss:  0.474816232919693\n",
      "epoch:  297  | loss:  0.47479337453842163\n",
      "epoch:  298  | loss:  0.47477057576179504\n",
      "epoch:  299  | loss:  0.474747896194458\n",
      "epoch:  300  | loss:  0.4747254252433777\n",
      "epoch:  301  | loss:  0.47470295429229736\n",
      "epoch:  302  | loss:  0.474680632352829\n",
      "epoch:  303  | loss:  0.47465837001800537\n",
      "epoch:  304  | loss:  0.4746363162994385\n",
      "epoch:  305  | loss:  0.4746142029762268\n",
      "epoch:  306  | loss:  0.47459232807159424\n",
      "epoch:  307  | loss:  0.4745704233646393\n",
      "epoch:  308  | loss:  0.4745487570762634\n",
      "epoch:  309  | loss:  0.47452718019485474\n",
      "epoch:  310  | loss:  0.47450560331344604\n",
      "epoch:  311  | loss:  0.47448423504829407\n",
      "epoch:  312  | loss:  0.4744628965854645\n",
      "epoch:  313  | loss:  0.4744415879249573\n",
      "epoch:  314  | loss:  0.4744203984737396\n",
      "epoch:  315  | loss:  0.4743994474411011\n",
      "epoch:  316  | loss:  0.47437840700149536\n",
      "epoch:  317  | loss:  0.474357545375824\n",
      "epoch:  318  | loss:  0.47433674335479736\n",
      "epoch:  319  | loss:  0.4743160605430603\n",
      "epoch:  320  | loss:  0.474295437335968\n",
      "epoch:  321  | loss:  0.4742749333381653\n",
      "epoch:  322  | loss:  0.4742545187473297\n",
      "epoch:  323  | loss:  0.4742342233657837\n",
      "epoch:  324  | loss:  0.47421392798423767\n",
      "epoch:  325  | loss:  0.474193811416626\n",
      "epoch:  326  | loss:  0.4741736650466919\n",
      "epoch:  327  | loss:  0.47415363788604736\n",
      "epoch:  328  | loss:  0.47413384914398193\n",
      "epoch:  329  | loss:  0.47411391139030457\n",
      "epoch:  330  | loss:  0.4740942120552063\n",
      "epoch:  331  | loss:  0.4740745425224304\n",
      "epoch:  332  | loss:  0.4740549325942993\n",
      "epoch:  333  | loss:  0.474035382270813\n",
      "epoch:  334  | loss:  0.4740159511566162\n",
      "epoch:  335  | loss:  0.4739966094493866\n",
      "epoch:  336  | loss:  0.47397738695144653\n",
      "epoch:  337  | loss:  0.47395819425582886\n",
      "epoch:  338  | loss:  0.4739390015602112\n",
      "epoch:  339  | loss:  0.47391998767852783\n",
      "epoch:  340  | loss:  0.4739009737968445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  341  | loss:  0.4738820791244507\n",
      "epoch:  342  | loss:  0.4738633334636688\n",
      "epoch:  343  | loss:  0.4738445580005646\n",
      "epoch:  344  | loss:  0.4738258123397827\n",
      "epoch:  345  | loss:  0.4738072156906128\n",
      "epoch:  346  | loss:  0.47378861904144287\n",
      "epoch:  347  | loss:  0.4737701714038849\n",
      "epoch:  348  | loss:  0.4737517833709717\n",
      "epoch:  349  | loss:  0.47373348474502563\n",
      "epoch:  350  | loss:  0.4737151861190796\n",
      "epoch:  351  | loss:  0.4736969769001007\n",
      "epoch:  352  | loss:  0.4736787974834442\n",
      "epoch:  353  | loss:  0.47366076707839966\n",
      "epoch:  354  | loss:  0.4736426770687103\n",
      "epoch:  355  | loss:  0.4736247658729553\n",
      "epoch:  356  | loss:  0.47360682487487793\n",
      "epoch:  357  | loss:  0.4735890030860901\n",
      "epoch:  358  | loss:  0.47357118129730225\n",
      "epoch:  359  | loss:  0.47355347871780396\n",
      "epoch:  360  | loss:  0.47353577613830566\n",
      "epoch:  361  | loss:  0.4735181927680969\n",
      "epoch:  362  | loss:  0.47350066900253296\n",
      "epoch:  363  | loss:  0.473483145236969\n",
      "epoch:  364  | loss:  0.4734656810760498\n",
      "epoch:  365  | loss:  0.47344833612442017\n",
      "epoch:  366  | loss:  0.47343093156814575\n",
      "epoch:  367  | loss:  0.47341370582580566\n",
      "epoch:  368  | loss:  0.4733964502811432\n",
      "epoch:  369  | loss:  0.4733792543411255\n",
      "epoch:  370  | loss:  0.47336214780807495\n",
      "epoch:  371  | loss:  0.47334498167037964\n",
      "epoch:  372  | loss:  0.4733279347419739\n",
      "epoch:  373  | loss:  0.4733109474182129\n",
      "epoch:  374  | loss:  0.4732939600944519\n",
      "epoch:  375  | loss:  0.4732770323753357\n",
      "epoch:  376  | loss:  0.47326016426086426\n",
      "epoch:  377  | loss:  0.4732432961463928\n",
      "epoch:  378  | loss:  0.47322651743888855\n",
      "epoch:  379  | loss:  0.4732097387313843\n",
      "epoch:  380  | loss:  0.4731929898262024\n",
      "epoch:  381  | loss:  0.4731762409210205\n",
      "epoch:  382  | loss:  0.4731596112251282\n",
      "epoch:  383  | loss:  0.47314292192459106\n",
      "epoch:  384  | loss:  0.47312623262405396\n",
      "epoch:  385  | loss:  0.47310972213745117\n",
      "epoch:  386  | loss:  0.47309303283691406\n",
      "epoch:  387  | loss:  0.4730764925479889\n",
      "epoch:  388  | loss:  0.47305989265441895\n",
      "epoch:  389  | loss:  0.47304338216781616\n",
      "epoch:  390  | loss:  0.4730268120765686\n",
      "epoch:  391  | loss:  0.4730103015899658\n",
      "epoch:  392  | loss:  0.47299373149871826\n",
      "epoch:  393  | loss:  0.4729771614074707\n",
      "epoch:  394  | loss:  0.47296059131622314\n",
      "epoch:  395  | loss:  0.47294414043426514\n",
      "epoch:  396  | loss:  0.4729275405406952\n",
      "epoch:  397  | loss:  0.47291088104248047\n",
      "epoch:  398  | loss:  0.4728942811489105\n",
      "epoch:  399  | loss:  0.4728776216506958\n",
      "epoch:  400  | loss:  0.4728608727455139\n",
      "epoch:  401  | loss:  0.4728442132472992\n",
      "epoch:  402  | loss:  0.47282734513282776\n",
      "epoch:  403  | loss:  0.4728105664253235\n",
      "epoch:  404  | loss:  0.4727935791015625\n",
      "epoch:  405  | loss:  0.47277653217315674\n",
      "epoch:  406  | loss:  0.4727594256401062\n",
      "epoch:  407  | loss:  0.4727422297000885\n",
      "epoch:  408  | loss:  0.4727248549461365\n",
      "epoch:  409  | loss:  0.47270745038986206\n",
      "epoch:  410  | loss:  0.47268974781036377\n",
      "epoch:  411  | loss:  0.4726719856262207\n",
      "epoch:  412  | loss:  0.47265389561653137\n",
      "epoch:  413  | loss:  0.4726356267929077\n",
      "epoch:  414  | loss:  0.47261714935302734\n",
      "epoch:  415  | loss:  0.4725984036922455\n",
      "epoch:  416  | loss:  0.47257930040359497\n",
      "epoch:  417  | loss:  0.4725598096847534\n",
      "epoch:  418  | loss:  0.47253990173339844\n",
      "epoch:  419  | loss:  0.4725196361541748\n",
      "epoch:  420  | loss:  0.47249871492385864\n",
      "epoch:  421  | loss:  0.4724772572517395\n",
      "epoch:  422  | loss:  0.47245508432388306\n",
      "epoch:  423  | loss:  0.4724321961402893\n",
      "epoch:  424  | loss:  0.47240835428237915\n",
      "epoch:  425  | loss:  0.47238361835479736\n",
      "epoch:  426  | loss:  0.4723575711250305\n",
      "epoch:  427  | loss:  0.4723302125930786\n",
      "epoch:  428  | loss:  0.4723013937473297\n",
      "epoch:  429  | loss:  0.4722707271575928\n",
      "epoch:  430  | loss:  0.4722379446029663\n",
      "epoch:  431  | loss:  0.47220271825790405\n",
      "epoch:  432  | loss:  0.47216475009918213\n",
      "epoch:  433  | loss:  0.47212350368499756\n",
      "epoch:  434  | loss:  0.4720783829689026\n",
      "epoch:  435  | loss:  0.47202903032302856\n",
      "epoch:  436  | loss:  0.4719747304916382\n",
      "epoch:  437  | loss:  0.4719148576259613\n",
      "epoch:  438  | loss:  0.4718490540981293\n",
      "epoch:  439  | loss:  0.47177690267562866\n",
      "epoch:  440  | loss:  0.4716984033584595\n",
      "epoch:  441  | loss:  0.4716140627861023\n",
      "epoch:  442  | loss:  0.4715246558189392\n",
      "epoch:  443  | loss:  0.47143059968948364\n",
      "epoch:  444  | loss:  0.4713311493396759\n",
      "epoch:  445  | loss:  0.4712223410606384\n",
      "epoch:  446  | loss:  0.47109684348106384\n",
      "epoch:  447  | loss:  0.47094589471817017\n",
      "epoch:  448  | loss:  0.4707641303539276\n",
      "epoch:  449  | loss:  0.4705519676208496\n",
      "epoch:  450  | loss:  0.47031283378601074\n",
      "epoch:  451  | loss:  0.4700469970703125\n",
      "epoch:  452  | loss:  0.46974682807922363\n",
      "epoch:  453  | loss:  0.4693978428840637\n",
      "epoch:  454  | loss:  0.4689815044403076\n",
      "epoch:  455  | loss:  0.46848106384277344\n",
      "epoch:  456  | loss:  0.4678838849067688\n",
      "epoch:  457  | loss:  0.46717244386672974\n",
      "epoch:  458  | loss:  0.4662967920303345\n",
      "epoch:  459  | loss:  0.4651535749435425\n",
      "epoch:  460  | loss:  0.4635801911354065\n",
      "epoch:  461  | loss:  0.4611626863479614\n",
      "epoch:  462  | loss:  0.4561513066291809\n",
      "epoch:  463  | loss:  0.4407401382923126\n",
      "epoch:  464  | loss:  0.43038538098335266\n",
      "epoch:  465  | loss:  0.42471954226493835\n",
      "epoch:  466  | loss:  0.4137820303440094\n",
      "epoch:  467  | loss:  0.41799986362457275\n",
      "epoch:  468  | loss:  0.4038236141204834\n",
      "epoch:  469  | loss:  0.40057089924812317\n",
      "epoch:  470  | loss:  0.38613826036453247\n",
      "epoch:  471  | loss:  0.39939451217651367\n",
      "epoch:  472  | loss:  0.38748225569725037\n",
      "epoch:  473  | loss:  0.3901614546775818\n",
      "epoch:  474  | loss:  0.37746527791023254\n",
      "epoch:  475  | loss:  0.387777179479599\n",
      "epoch:  476  | loss:  0.37802714109420776\n",
      "epoch:  477  | loss:  0.3845559060573578\n",
      "epoch:  478  | loss:  0.37693673372268677\n",
      "epoch:  479  | loss:  0.3768611550331116\n",
      "epoch:  480  | loss:  0.3701278865337372\n",
      "epoch:  481  | loss:  0.3713473081588745\n",
      "epoch:  482  | loss:  0.36985331773757935\n",
      "epoch:  483  | loss:  0.36774885654449463\n",
      "epoch:  484  | loss:  0.36947476863861084\n",
      "epoch:  485  | loss:  0.36737266182899475\n",
      "epoch:  486  | loss:  0.36864808201789856\n",
      "epoch:  487  | loss:  0.3665013015270233\n",
      "epoch:  488  | loss:  0.36657488346099854\n",
      "epoch:  489  | loss:  0.3653329610824585\n",
      "epoch:  490  | loss:  0.3651507496833801\n",
      "epoch:  491  | loss:  0.3653181791305542\n",
      "epoch:  492  | loss:  0.3640574812889099\n",
      "epoch:  493  | loss:  0.36512917280197144\n",
      "epoch:  494  | loss:  0.36370015144348145\n",
      "epoch:  495  | loss:  0.36425670981407166\n",
      "epoch:  496  | loss:  0.3633275628089905\n",
      "epoch:  497  | loss:  0.36342233419418335\n",
      "epoch:  498  | loss:  0.36281347274780273\n",
      "epoch:  499  | loss:  0.36272132396698\n",
      "epoch:  500  | loss:  0.3626744747161865\n",
      "epoch:  501  | loss:  0.3621683716773987\n",
      "epoch:  502  | loss:  0.3624606132507324\n",
      "epoch:  503  | loss:  0.361913800239563\n",
      "epoch:  504  | loss:  0.3620346784591675\n",
      "epoch:  505  | loss:  0.36173439025878906\n",
      "epoch:  506  | loss:  0.36155086755752563\n",
      "epoch:  507  | loss:  0.36149588227272034\n",
      "epoch:  508  | loss:  0.3612249493598938\n",
      "epoch:  509  | loss:  0.36126935482025146\n",
      "epoch:  510  | loss:  0.3609924018383026\n",
      "epoch:  511  | loss:  0.3610360622406006\n",
      "epoch:  512  | loss:  0.36081570386886597\n",
      "epoch:  513  | loss:  0.3607756495475769\n",
      "epoch:  514  | loss:  0.3606656789779663\n",
      "epoch:  515  | loss:  0.3605138659477234\n",
      "epoch:  516  | loss:  0.36048364639282227\n",
      "epoch:  517  | loss:  0.36030882596969604\n",
      "epoch:  518  | loss:  0.36029088497161865\n",
      "epoch:  519  | loss:  0.36014634370803833\n",
      "epoch:  520  | loss:  0.36010637879371643\n",
      "epoch:  521  | loss:  0.36000701785087585\n",
      "epoch:  522  | loss:  0.3599298596382141\n",
      "epoch:  523  | loss:  0.3598712086677551\n",
      "epoch:  524  | loss:  0.3597608208656311\n",
      "epoch:  525  | loss:  0.359720915555954\n",
      "epoch:  526  | loss:  0.35961267352104187\n",
      "epoch:  527  | loss:  0.3595656156539917\n",
      "epoch:  528  | loss:  0.35948294401168823\n",
      "epoch:  529  | loss:  0.3594188690185547\n",
      "epoch:  530  | loss:  0.3593594431877136\n",
      "epoch:  531  | loss:  0.3592846989631653\n",
      "epoch:  532  | loss:  0.3592359125614166\n",
      "epoch:  533  | loss:  0.35915783047676086\n",
      "epoch:  534  | loss:  0.35910993814468384\n",
      "epoch:  535  | loss:  0.3590378761291504\n",
      "epoch:  536  | loss:  0.3589845299720764\n",
      "epoch:  537  | loss:  0.35892409086227417\n",
      "epoch:  538  | loss:  0.35886460542678833\n",
      "epoch:  539  | loss:  0.35881367325782776\n",
      "epoch:  540  | loss:  0.3587524890899658\n",
      "epoch:  541  | loss:  0.3587050437927246\n",
      "epoch:  542  | loss:  0.3586457669734955\n",
      "epoch:  543  | loss:  0.3585973381996155\n",
      "epoch:  544  | loss:  0.3585425019264221\n",
      "epoch:  545  | loss:  0.3584916591644287\n",
      "epoch:  546  | loss:  0.35844188928604126\n",
      "epoch:  547  | loss:  0.358389675617218\n",
      "epoch:  548  | loss:  0.3583434820175171\n",
      "epoch:  549  | loss:  0.3582921624183655\n",
      "epoch:  550  | loss:  0.35824716091156006\n",
      "epoch:  551  | loss:  0.35819822549819946\n",
      "epoch:  552  | loss:  0.35815298557281494\n",
      "epoch:  553  | loss:  0.35810667276382446\n",
      "epoch:  554  | loss:  0.3580610156059265\n",
      "epoch:  555  | loss:  0.3580169975757599\n",
      "epoch:  556  | loss:  0.3579714894294739\n",
      "epoch:  557  | loss:  0.3579291105270386\n",
      "epoch:  558  | loss:  0.35788482427597046\n",
      "epoch:  559  | loss:  0.3578432500362396\n",
      "epoch:  560  | loss:  0.3578006327152252\n",
      "epoch:  561  | loss:  0.35775941610336304\n",
      "epoch:  562  | loss:  0.3577183485031128\n",
      "epoch:  563  | loss:  0.35767751932144165\n",
      "epoch:  564  | loss:  0.35763782262802124\n",
      "epoch:  565  | loss:  0.35759758949279785\n",
      "epoch:  566  | loss:  0.3575589060783386\n",
      "epoch:  567  | loss:  0.35751962661743164\n",
      "epoch:  568  | loss:  0.35748156905174255\n",
      "epoch:  569  | loss:  0.3574434518814087\n",
      "epoch:  570  | loss:  0.35740602016448975\n",
      "epoch:  571  | loss:  0.35736900568008423\n",
      "epoch:  572  | loss:  0.35733217000961304\n",
      "epoch:  573  | loss:  0.35729607939720154\n",
      "epoch:  574  | loss:  0.35725992918014526\n",
      "epoch:  575  | loss:  0.35722455382347107\n",
      "epoch:  576  | loss:  0.3571891784667969\n",
      "epoch:  577  | loss:  0.3571544587612152\n",
      "epoch:  578  | loss:  0.35711991786956787\n",
      "epoch:  579  | loss:  0.35708585381507874\n",
      "epoch:  580  | loss:  0.35705214738845825\n",
      "epoch:  581  | loss:  0.3570185899734497\n",
      "epoch:  582  | loss:  0.3569856286048889\n",
      "epoch:  583  | loss:  0.35695284605026245\n",
      "epoch:  584  | loss:  0.35692042112350464\n",
      "epoch:  585  | loss:  0.3568882346153259\n",
      "epoch:  586  | loss:  0.35685649514198303\n",
      "epoch:  587  | loss:  0.35682493448257446\n",
      "epoch:  588  | loss:  0.35679376125335693\n",
      "epoch:  589  | loss:  0.3567628860473633\n",
      "epoch:  590  | loss:  0.3567323088645935\n",
      "epoch:  591  | loss:  0.35670197010040283\n",
      "epoch:  592  | loss:  0.35667192935943604\n",
      "epoch:  593  | loss:  0.3566421866416931\n",
      "epoch:  594  | loss:  0.3566127419471741\n",
      "epoch:  595  | loss:  0.35658353567123413\n",
      "epoch:  596  | loss:  0.3565545082092285\n",
      "epoch:  597  | loss:  0.3565259575843811\n",
      "epoch:  598  | loss:  0.35649752616882324\n",
      "epoch:  599  | loss:  0.35646936297416687\n",
      "epoch:  600  | loss:  0.356441468000412\n",
      "epoch:  601  | loss:  0.3564137816429138\n",
      "epoch:  602  | loss:  0.35638630390167236\n",
      "epoch:  603  | loss:  0.35635918378829956\n",
      "epoch:  604  | loss:  0.3563322424888611\n",
      "epoch:  605  | loss:  0.3563055694103241\n",
      "epoch:  606  | loss:  0.3562791347503662\n",
      "epoch:  607  | loss:  0.3562527894973755\n",
      "epoch:  608  | loss:  0.3562268316745758\n",
      "epoch:  609  | loss:  0.35620102286338806\n",
      "epoch:  610  | loss:  0.35617542266845703\n",
      "epoch:  611  | loss:  0.3561501204967499\n",
      "epoch:  612  | loss:  0.35612496733665466\n",
      "epoch:  613  | loss:  0.35610002279281616\n",
      "epoch:  614  | loss:  0.3560752272605896\n",
      "epoch:  615  | loss:  0.3560507595539093\n",
      "epoch:  616  | loss:  0.35602641105651855\n",
      "epoch:  617  | loss:  0.3560023307800293\n",
      "epoch:  618  | loss:  0.3559783697128296\n",
      "epoch:  619  | loss:  0.35595470666885376\n",
      "epoch:  620  | loss:  0.3559311628341675\n",
      "epoch:  621  | loss:  0.355907678604126\n",
      "epoch:  622  | loss:  0.3558845520019531\n",
      "epoch:  623  | loss:  0.3558616042137146\n",
      "epoch:  624  | loss:  0.3558387756347656\n",
      "epoch:  625  | loss:  0.35581618547439575\n",
      "epoch:  626  | loss:  0.35579371452331543\n",
      "epoch:  627  | loss:  0.35577136278152466\n",
      "epoch:  628  | loss:  0.35574930906295776\n",
      "epoch:  629  | loss:  0.3557273745536804\n",
      "epoch:  630  | loss:  0.3557056188583374\n",
      "epoch:  631  | loss:  0.35568398237228394\n",
      "epoch:  632  | loss:  0.35566258430480957\n",
      "epoch:  633  | loss:  0.35564130544662476\n",
      "epoch:  634  | loss:  0.35562023520469666\n",
      "epoch:  635  | loss:  0.35559922456741333\n",
      "epoch:  636  | loss:  0.3555784225463867\n",
      "epoch:  637  | loss:  0.35555779933929443\n",
      "epoch:  638  | loss:  0.3555372953414917\n",
      "epoch:  639  | loss:  0.3555169105529785\n",
      "epoch:  640  | loss:  0.35549676418304443\n",
      "epoch:  641  | loss:  0.3554766774177551\n",
      "epoch:  642  | loss:  0.3554568290710449\n",
      "epoch:  643  | loss:  0.3554370403289795\n",
      "epoch:  644  | loss:  0.3554174304008484\n",
      "epoch:  645  | loss:  0.35539793968200684\n",
      "epoch:  646  | loss:  0.35537856817245483\n",
      "epoch:  647  | loss:  0.35535940527915955\n",
      "epoch:  648  | loss:  0.3553403615951538\n",
      "epoch:  649  | loss:  0.35532140731811523\n",
      "epoch:  650  | loss:  0.3553025722503662\n",
      "epoch:  651  | loss:  0.3552839756011963\n",
      "epoch:  652  | loss:  0.355265349149704\n",
      "epoch:  653  | loss:  0.35524699091911316\n",
      "epoch:  654  | loss:  0.3552286624908447\n",
      "epoch:  655  | loss:  0.355210542678833\n",
      "epoch:  656  | loss:  0.35519248247146606\n",
      "epoch:  657  | loss:  0.35517454147338867\n",
      "epoch:  658  | loss:  0.3551567792892456\n",
      "epoch:  659  | loss:  0.3551390767097473\n",
      "epoch:  660  | loss:  0.35512155294418335\n",
      "epoch:  661  | loss:  0.35510408878326416\n",
      "epoch:  662  | loss:  0.3550867438316345\n",
      "epoch:  663  | loss:  0.3550695478916168\n",
      "epoch:  664  | loss:  0.3550524413585663\n",
      "epoch:  665  | loss:  0.3550354838371277\n",
      "epoch:  666  | loss:  0.35501861572265625\n",
      "epoch:  667  | loss:  0.3550018072128296\n",
      "epoch:  668  | loss:  0.3549850881099701\n",
      "epoch:  669  | loss:  0.3549685776233673\n",
      "epoch:  670  | loss:  0.3549521565437317\n",
      "epoch:  671  | loss:  0.35493576526641846\n",
      "epoch:  672  | loss:  0.35491952300071716\n",
      "epoch:  673  | loss:  0.3549034297466278\n",
      "epoch:  674  | loss:  0.35488733649253845\n",
      "epoch:  675  | loss:  0.3548714220523834\n",
      "epoch:  676  | loss:  0.35485559701919556\n",
      "epoch:  677  | loss:  0.35483986139297485\n",
      "epoch:  678  | loss:  0.3548242151737213\n",
      "epoch:  679  | loss:  0.35480865836143494\n",
      "epoch:  680  | loss:  0.3547931909561157\n",
      "epoch:  681  | loss:  0.35477787256240845\n",
      "epoch:  682  | loss:  0.35476261377334595\n",
      "epoch:  683  | loss:  0.35474738478660583\n",
      "epoch:  684  | loss:  0.35473233461380005\n",
      "epoch:  685  | loss:  0.3547173738479614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  686  | loss:  0.3547024428844452\n",
      "epoch:  687  | loss:  0.3546876311302185\n",
      "epoch:  688  | loss:  0.35467293858528137\n",
      "epoch:  689  | loss:  0.35465824604034424\n",
      "epoch:  690  | loss:  0.35464370250701904\n",
      "epoch:  691  | loss:  0.3546292185783386\n",
      "epoch:  692  | loss:  0.35461491346359253\n",
      "epoch:  693  | loss:  0.3546005189418793\n",
      "epoch:  694  | loss:  0.3545863628387451\n",
      "epoch:  695  | loss:  0.3545721769332886\n",
      "epoch:  696  | loss:  0.3545581102371216\n",
      "epoch:  697  | loss:  0.35454416275024414\n",
      "epoch:  698  | loss:  0.3545302152633667\n",
      "epoch:  699  | loss:  0.3545163869857788\n",
      "epoch:  700  | loss:  0.35450270771980286\n",
      "epoch:  701  | loss:  0.3544890284538269\n",
      "epoch:  702  | loss:  0.3544754087924957\n",
      "epoch:  703  | loss:  0.3544619083404541\n",
      "epoch:  704  | loss:  0.35444849729537964\n",
      "epoch:  705  | loss:  0.3544350862503052\n",
      "epoch:  706  | loss:  0.35442182421684265\n",
      "epoch:  707  | loss:  0.3544086217880249\n",
      "epoch:  708  | loss:  0.35439544916152954\n",
      "epoch:  709  | loss:  0.3543824255466461\n",
      "epoch:  710  | loss:  0.3543693423271179\n",
      "epoch:  711  | loss:  0.35435646772384644\n",
      "epoch:  712  | loss:  0.3543435335159302\n",
      "epoch:  713  | loss:  0.35433077812194824\n",
      "epoch:  714  | loss:  0.3543180227279663\n",
      "epoch:  715  | loss:  0.3543053865432739\n",
      "epoch:  716  | loss:  0.35429278016090393\n",
      "epoch:  717  | loss:  0.3542802333831787\n",
      "epoch:  718  | loss:  0.3542677164077759\n",
      "epoch:  719  | loss:  0.3542553782463074\n",
      "epoch:  720  | loss:  0.35424306988716125\n",
      "epoch:  721  | loss:  0.35423076152801514\n",
      "epoch:  722  | loss:  0.3542185425758362\n",
      "epoch:  723  | loss:  0.354206383228302\n",
      "epoch:  724  | loss:  0.354194313287735\n",
      "epoch:  725  | loss:  0.35418227314949036\n",
      "epoch:  726  | loss:  0.3541702926158905\n",
      "epoch:  727  | loss:  0.3541584014892578\n",
      "epoch:  728  | loss:  0.35414648056030273\n",
      "epoch:  729  | loss:  0.35413479804992676\n",
      "epoch:  730  | loss:  0.35412299633026123\n",
      "epoch:  731  | loss:  0.35411131381988525\n",
      "epoch:  732  | loss:  0.3540997803211212\n",
      "epoch:  733  | loss:  0.3540881872177124\n",
      "epoch:  734  | loss:  0.35407668352127075\n",
      "epoch:  735  | loss:  0.35406529903411865\n",
      "epoch:  736  | loss:  0.35405388474464417\n",
      "epoch:  737  | loss:  0.35404253005981445\n",
      "epoch:  738  | loss:  0.3540312647819519\n",
      "epoch:  739  | loss:  0.35402002930641174\n",
      "epoch:  740  | loss:  0.35400888323783875\n",
      "epoch:  741  | loss:  0.35399773716926575\n",
      "epoch:  742  | loss:  0.35398662090301514\n",
      "epoch:  743  | loss:  0.35397565364837646\n",
      "epoch:  744  | loss:  0.353964626789093\n",
      "epoch:  745  | loss:  0.3539537787437439\n",
      "epoch:  746  | loss:  0.35394287109375\n",
      "epoch:  747  | loss:  0.35393205285072327\n",
      "epoch:  748  | loss:  0.3539212942123413\n",
      "epoch:  749  | loss:  0.3539106249809265\n",
      "epoch:  750  | loss:  0.35389989614486694\n",
      "epoch:  751  | loss:  0.3538893461227417\n",
      "epoch:  752  | loss:  0.3538787364959717\n",
      "epoch:  753  | loss:  0.35386818647384644\n",
      "epoch:  754  | loss:  0.35385772585868835\n",
      "epoch:  755  | loss:  0.35384732484817505\n",
      "epoch:  756  | loss:  0.35383695363998413\n",
      "epoch:  757  | loss:  0.3538266122341156\n",
      "epoch:  758  | loss:  0.35381633043289185\n",
      "epoch:  759  | loss:  0.3538060784339905\n",
      "epoch:  760  | loss:  0.3537958264350891\n",
      "epoch:  761  | loss:  0.3537857234477997\n",
      "epoch:  762  | loss:  0.3537755608558655\n",
      "epoch:  763  | loss:  0.3537655472755432\n",
      "epoch:  764  | loss:  0.35375553369522095\n",
      "epoch:  765  | loss:  0.3537454903125763\n",
      "epoch:  766  | loss:  0.35373562574386597\n",
      "epoch:  767  | loss:  0.3537256121635437\n",
      "epoch:  768  | loss:  0.35371583700180054\n",
      "epoch:  769  | loss:  0.3537060618400574\n",
      "epoch:  770  | loss:  0.35369622707366943\n",
      "epoch:  771  | loss:  0.35368651151657104\n",
      "epoch:  772  | loss:  0.35367679595947266\n",
      "epoch:  773  | loss:  0.35366714000701904\n",
      "epoch:  774  | loss:  0.3536575436592102\n",
      "epoch:  775  | loss:  0.35364797711372375\n",
      "epoch:  776  | loss:  0.3536384105682373\n",
      "epoch:  777  | loss:  0.3536289930343628\n",
      "epoch:  778  | loss:  0.35361945629119873\n",
      "epoch:  779  | loss:  0.353610098361969\n",
      "epoch:  780  | loss:  0.3536006808280945\n",
      "epoch:  781  | loss:  0.35359132289886475\n",
      "epoch:  782  | loss:  0.35358208417892456\n",
      "epoch:  783  | loss:  0.3535728454589844\n",
      "epoch:  784  | loss:  0.353563517332077\n",
      "epoch:  785  | loss:  0.353554368019104\n",
      "epoch:  786  | loss:  0.3535451889038086\n",
      "epoch:  787  | loss:  0.35353606939315796\n",
      "epoch:  788  | loss:  0.3535269796848297\n",
      "epoch:  789  | loss:  0.35351794958114624\n",
      "epoch:  790  | loss:  0.35350891947746277\n",
      "epoch:  791  | loss:  0.3534999191761017\n",
      "epoch:  792  | loss:  0.35349100828170776\n",
      "epoch:  793  | loss:  0.35348206758499146\n",
      "epoch:  794  | loss:  0.3534732162952423\n",
      "epoch:  795  | loss:  0.35346436500549316\n",
      "epoch:  796  | loss:  0.353455513715744\n",
      "epoch:  797  | loss:  0.35344675183296204\n",
      "epoch:  798  | loss:  0.35343801975250244\n",
      "epoch:  799  | loss:  0.35342928767204285\n",
      "epoch:  800  | loss:  0.3534206449985504\n",
      "epoch:  801  | loss:  0.3534119725227356\n",
      "epoch:  802  | loss:  0.35340332984924316\n",
      "epoch:  803  | loss:  0.3533947765827179\n",
      "epoch:  804  | loss:  0.3533862233161926\n",
      "epoch:  805  | loss:  0.35337769985198975\n",
      "epoch:  806  | loss:  0.35336917638778687\n",
      "epoch:  807  | loss:  0.35336071252822876\n",
      "epoch:  808  | loss:  0.35335227847099304\n",
      "epoch:  809  | loss:  0.3533439040184021\n",
      "epoch:  810  | loss:  0.35333549976348877\n",
      "epoch:  811  | loss:  0.353327214717865\n",
      "epoch:  812  | loss:  0.35331881046295166\n",
      "epoch:  813  | loss:  0.35331058502197266\n",
      "epoch:  814  | loss:  0.3533022999763489\n",
      "epoch:  815  | loss:  0.35329413414001465\n",
      "epoch:  816  | loss:  0.35328584909439087\n",
      "epoch:  817  | loss:  0.3532777428627014\n",
      "epoch:  818  | loss:  0.3532696068286896\n",
      "epoch:  819  | loss:  0.35326147079467773\n",
      "epoch:  820  | loss:  0.35325339436531067\n",
      "epoch:  821  | loss:  0.3532453775405884\n",
      "epoch:  822  | loss:  0.3532373309135437\n",
      "epoch:  823  | loss:  0.3532293438911438\n",
      "epoch:  824  | loss:  0.3532212972640991\n",
      "epoch:  825  | loss:  0.353213369846344\n",
      "epoch:  826  | loss:  0.35320544242858887\n",
      "epoch:  827  | loss:  0.35319751501083374\n",
      "epoch:  828  | loss:  0.35318970680236816\n",
      "epoch:  829  | loss:  0.3531818389892578\n",
      "epoch:  830  | loss:  0.35317403078079224\n",
      "epoch:  831  | loss:  0.35316622257232666\n",
      "epoch:  832  | loss:  0.35315847396850586\n",
      "epoch:  833  | loss:  0.35315075516700745\n",
      "epoch:  834  | loss:  0.35314300656318665\n",
      "epoch:  835  | loss:  0.35313528776168823\n",
      "epoch:  836  | loss:  0.353127658367157\n",
      "epoch:  837  | loss:  0.35311999917030334\n",
      "epoch:  838  | loss:  0.3531123995780945\n",
      "epoch:  839  | loss:  0.3531047999858856\n",
      "epoch:  840  | loss:  0.35309720039367676\n",
      "epoch:  841  | loss:  0.35308966040611267\n",
      "epoch:  842  | loss:  0.3530821204185486\n",
      "epoch:  843  | loss:  0.35307466983795166\n",
      "epoch:  844  | loss:  0.35306715965270996\n",
      "epoch:  845  | loss:  0.35305970907211304\n",
      "epoch:  846  | loss:  0.3530523180961609\n",
      "epoch:  847  | loss:  0.35304495692253113\n",
      "epoch:  848  | loss:  0.3530374765396118\n",
      "epoch:  849  | loss:  0.35303011536598206\n",
      "epoch:  850  | loss:  0.35302287340164185\n",
      "epoch:  851  | loss:  0.3530155122280121\n",
      "epoch:  852  | loss:  0.3530081808567047\n",
      "epoch:  853  | loss:  0.3530009090900421\n",
      "epoch:  854  | loss:  0.35299360752105713\n",
      "epoch:  855  | loss:  0.3529864549636841\n",
      "epoch:  856  | loss:  0.35297924280166626\n",
      "epoch:  857  | loss:  0.35297197103500366\n",
      "epoch:  858  | loss:  0.3529648184776306\n",
      "epoch:  859  | loss:  0.35295772552490234\n",
      "epoch:  860  | loss:  0.35295066237449646\n",
      "epoch:  861  | loss:  0.35294345021247864\n",
      "epoch:  862  | loss:  0.35293638706207275\n",
      "epoch:  863  | loss:  0.35292935371398926\n",
      "epoch:  864  | loss:  0.35292232036590576\n",
      "epoch:  865  | loss:  0.35291528701782227\n",
      "epoch:  866  | loss:  0.35290831327438354\n",
      "epoch:  867  | loss:  0.35290125012397766\n",
      "epoch:  868  | loss:  0.35289430618286133\n",
      "epoch:  869  | loss:  0.3528873920440674\n",
      "epoch:  870  | loss:  0.35288047790527344\n",
      "epoch:  871  | loss:  0.3528735637664795\n",
      "epoch:  872  | loss:  0.35286664962768555\n",
      "epoch:  873  | loss:  0.3528597950935364\n",
      "epoch:  874  | loss:  0.3528529405593872\n",
      "epoch:  875  | loss:  0.3528461158275604\n",
      "epoch:  876  | loss:  0.35283929109573364\n",
      "epoch:  877  | loss:  0.352832555770874\n",
      "epoch:  878  | loss:  0.35282573103904724\n",
      "epoch:  879  | loss:  0.35281896591186523\n",
      "epoch:  880  | loss:  0.3528122007846832\n",
      "epoch:  881  | loss:  0.352805495262146\n",
      "epoch:  882  | loss:  0.3527988791465759\n",
      "epoch:  883  | loss:  0.3527921140193939\n",
      "epoch:  884  | loss:  0.35278546810150146\n",
      "epoch:  885  | loss:  0.3527787923812866\n",
      "epoch:  886  | loss:  0.35277223587036133\n",
      "epoch:  887  | loss:  0.35276564955711365\n",
      "epoch:  888  | loss:  0.3527589738368988\n",
      "epoch:  889  | loss:  0.3527524471282959\n",
      "epoch:  890  | loss:  0.3527458906173706\n",
      "epoch:  891  | loss:  0.3527393341064453\n",
      "epoch:  892  | loss:  0.3527328372001648\n",
      "epoch:  893  | loss:  0.3527263104915619\n",
      "epoch:  894  | loss:  0.352719783782959\n",
      "epoch:  895  | loss:  0.35271334648132324\n",
      "epoch:  896  | loss:  0.3527068793773651\n",
      "epoch:  897  | loss:  0.35270047187805176\n",
      "epoch:  898  | loss:  0.352694034576416\n",
      "epoch:  899  | loss:  0.3526875674724579\n",
      "epoch:  900  | loss:  0.35268115997314453\n",
      "epoch:  901  | loss:  0.35267478227615356\n",
      "epoch:  902  | loss:  0.3526684641838074\n",
      "epoch:  903  | loss:  0.3526621460914612\n",
      "epoch:  904  | loss:  0.3526557683944702\n",
      "epoch:  905  | loss:  0.3526495397090912\n",
      "epoch:  906  | loss:  0.35264313220977783\n",
      "epoch:  907  | loss:  0.3526368737220764\n",
      "epoch:  908  | loss:  0.3526305556297302\n",
      "epoch:  909  | loss:  0.3526242971420288\n",
      "epoch:  910  | loss:  0.35261809825897217\n",
      "epoch:  911  | loss:  0.3526118993759155\n",
      "epoch:  912  | loss:  0.35260558128356934\n",
      "epoch:  913  | loss:  0.35259944200515747\n",
      "epoch:  914  | loss:  0.3525933027267456\n",
      "epoch:  915  | loss:  0.35258716344833374\n",
      "epoch:  916  | loss:  0.3525809645652771\n",
      "epoch:  917  | loss:  0.35257482528686523\n",
      "epoch:  918  | loss:  0.35256868600845337\n",
      "epoch:  919  | loss:  0.3525626063346863\n",
      "epoch:  920  | loss:  0.3525565266609192\n",
      "epoch:  921  | loss:  0.3525503873825073\n",
      "epoch:  922  | loss:  0.352544367313385\n",
      "epoch:  923  | loss:  0.3525383174419403\n",
      "epoch:  924  | loss:  0.3525322675704956\n",
      "epoch:  925  | loss:  0.3525262475013733\n",
      "epoch:  926  | loss:  0.35252025723457336\n",
      "epoch:  927  | loss:  0.35251420736312866\n",
      "epoch:  928  | loss:  0.3525082468986511\n",
      "epoch:  929  | loss:  0.35250231623649597\n",
      "epoch:  930  | loss:  0.35249632596969604\n",
      "epoch:  931  | loss:  0.3524903953075409\n",
      "epoch:  932  | loss:  0.35248440504074097\n",
      "epoch:  933  | loss:  0.352478563785553\n",
      "epoch:  934  | loss:  0.3524726629257202\n",
      "epoch:  935  | loss:  0.35246679186820984\n",
      "epoch:  936  | loss:  0.3524608612060547\n",
      "epoch:  937  | loss:  0.3524549603462219\n",
      "epoch:  938  | loss:  0.3524491786956787\n",
      "epoch:  939  | loss:  0.3524433374404907\n",
      "epoch:  940  | loss:  0.35243749618530273\n",
      "epoch:  941  | loss:  0.35243168473243713\n",
      "epoch:  942  | loss:  0.35242587327957153\n",
      "epoch:  943  | loss:  0.3524200916290283\n",
      "epoch:  944  | loss:  0.3524143099784851\n",
      "epoch:  945  | loss:  0.35240858793258667\n",
      "epoch:  946  | loss:  0.35240280628204346\n",
      "epoch:  947  | loss:  0.352397084236145\n",
      "epoch:  948  | loss:  0.3523913025856018\n",
      "epoch:  949  | loss:  0.35238558053970337\n",
      "epoch:  950  | loss:  0.3523798882961273\n",
      "epoch:  951  | loss:  0.35237419605255127\n",
      "epoch:  952  | loss:  0.35236856341362\n",
      "epoch:  953  | loss:  0.35236287117004395\n",
      "epoch:  954  | loss:  0.35235723853111267\n",
      "epoch:  955  | loss:  0.3523515462875366\n",
      "epoch:  956  | loss:  0.35234594345092773\n",
      "epoch:  957  | loss:  0.3523402810096741\n",
      "epoch:  958  | loss:  0.3523346781730652\n",
      "epoch:  959  | loss:  0.3523291349411011\n",
      "epoch:  960  | loss:  0.35232359170913696\n",
      "epoch:  961  | loss:  0.3523179292678833\n",
      "epoch:  962  | loss:  0.3523123264312744\n",
      "epoch:  963  | loss:  0.3523068428039551\n",
      "epoch:  964  | loss:  0.35230129957199097\n",
      "epoch:  965  | loss:  0.35229572653770447\n",
      "epoch:  966  | loss:  0.35229021310806274\n",
      "epoch:  967  | loss:  0.352284699678421\n",
      "epoch:  968  | loss:  0.3522792458534241\n",
      "epoch:  969  | loss:  0.3522736430168152\n",
      "epoch:  970  | loss:  0.3522682189941406\n",
      "epoch:  971  | loss:  0.3522627055644989\n",
      "epoch:  972  | loss:  0.35225731134414673\n",
      "epoch:  973  | loss:  0.35225188732147217\n",
      "epoch:  974  | loss:  0.35224640369415283\n",
      "epoch:  975  | loss:  0.35224097967147827\n",
      "epoch:  976  | loss:  0.3522355556488037\n",
      "epoch:  977  | loss:  0.3522301912307739\n",
      "epoch:  978  | loss:  0.3522247076034546\n",
      "epoch:  979  | loss:  0.3522194027900696\n",
      "epoch:  980  | loss:  0.3522140383720398\n",
      "epoch:  981  | loss:  0.35220867395401\n",
      "epoch:  982  | loss:  0.3522033095359802\n",
      "epoch:  983  | loss:  0.35219794511795044\n",
      "epoch:  984  | loss:  0.35219264030456543\n",
      "epoch:  985  | loss:  0.35218727588653564\n",
      "epoch:  986  | loss:  0.35218197107315063\n",
      "epoch:  987  | loss:  0.35217660665512085\n",
      "epoch:  988  | loss:  0.3521713614463806\n",
      "epoch:  989  | loss:  0.3521661162376404\n",
      "epoch:  990  | loss:  0.35216084122657776\n",
      "epoch:  991  | loss:  0.35215556621551514\n",
      "epoch:  992  | loss:  0.3521502912044525\n",
      "epoch:  993  | loss:  0.3521450161933899\n",
      "epoch:  994  | loss:  0.35213983058929443\n",
      "epoch:  995  | loss:  0.3521345853805542\n",
      "epoch:  996  | loss:  0.35212934017181396\n",
      "epoch:  997  | loss:  0.35212409496307373\n",
      "epoch:  998  | loss:  0.35211899876594543\n",
      "epoch:  999  | loss:  0.3521137535572052\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "\n",
    "# train by epochs\n",
    "for epoch in range(0, 1000):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    # train feeding a sequence\n",
    "    hidden, output = model(hidden, inputs)\n",
    "    loss += criterion(output, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"epoch: \", epoch, \" | loss: \", loss.item())\n",
    "    # finish train feeding a sequence\n",
    "    \n",
    "    # feed one char by one char\n",
    "    \n",
    "#     for input_, label in zip(inputs, targets):\n",
    "        \n",
    "# #         print(input_.shape)\n",
    "#     #     print(label.unsqueeze(0).shape)\n",
    "#         # evaluate model\n",
    "#         hidden, output = model(hidden, input_)\n",
    "#         # get predictions\n",
    "#         val, idx = output.max(1)\n",
    "#         loss += criterion(output, label.unsqueeze(0))\n",
    "# #         print(idx2char[idx], \"label: \", idx2char[label])\n",
    "    \n",
    "#     print(\"epoch: \", epoch, \" | loss: \", loss.item())\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_ih_l0 tensor([[-1.7786, -2.3806, -1.4605, -0.1670],\n",
      "        [ 3.3692, -3.8771, -1.7714,  0.2418],\n",
      "        [-3.7073,  2.4700, -1.3504,  0.2012],\n",
      "        [-4.2258, -1.2781,  1.4582, -0.4340]])\n",
      "rnn.weight_hh_l0 tensor([[  1.4087,   0.4948,   0.0374,  -0.7896],\n",
      "        [  2.8944,   0.6467,  -0.6588,   0.0979],\n",
      "        [  0.3689,   1.9626,  -2.6739, -14.6571],\n",
      "        [ -1.1021,  -4.3824,   2.6738,  10.6441]])\n",
      "rnn.bias_ih_l0 tensor([-1.5459,  0.3350, -2.4582, -0.0663])\n",
      "rnn.bias_hh_l0 tensor([-1.4088,  0.8661, -3.0453,  0.3004])\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "Input: \n",
      "\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      "\n",
      "Prediction\n",
      "o\n",
      "l\n",
      "l\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "# idx2char = ['h', 'e', 'l', 'o']\n",
    "# h: 0\n",
    "# e: 1\n",
    "# l: 2\n",
    "# o: 3\n",
    "\n",
    "# evaluate model\n",
    "# print(torch.Tensor(one_hot_look[0]))\n",
    "# print(model.init_hidden())\n",
    "letters = [1, 2, 2, 3]\n",
    "hidden, output = model(model.init_hidden(), torch.Tensor([ one_hot_look[x] for x in letters]))\n",
    "\n",
    "print(output.shape)\n",
    "\n",
    "print(\"Input: \\n\")\n",
    "for i in range(4):\n",
    "    print(idx2char[letters[i]])\n",
    "\n",
    "print(\"\\nPrediction\")\n",
    "for i in range(4):\n",
    "    print(idx2char[output[i].argmax()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
